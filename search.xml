<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[leaning-to-rank-for-IR]]></title>
    <url>%2F2019%2F01%2F23%2Fleaning-to-rank-for-IR%2F</url>
    <content type="text"><![CDATA[Learning to Rank for Information Retrievalhttps://www.cda.cn/uploadfile/image/20151220/20151220115436_46293.pdf 目标：本周内读完前11页的内容Chapter 1 Overiew本章主要是介绍l2r，首先以文档搜索为例介绍排名问题，然后，回顾传统方法，提出广泛使用的评估方法，之后给出机器学习解决排名问题的冬季，并对现有的方法进行分类和简化描述 介绍搜索引擎包括：爬虫，解析器，索引器，链接分析器，查询处理器和排名起。爬虫复杂爬去网页信息；解析器复杂分析文档并生成对应的索引和超链接图。ranking在搜索引擎中是处于核心作用，在其他信息检索任务中也是同样的呃核心作用，如协同过滤，问答，多媒体检索，文本摘要等。 1.2 Ranking in IR传统的方法相关性排名模型相关性排名模型根据文档和查询之间的相关性产生排序的结果。通常将每个单独的文档作为输入，并计算匹配的分之，然后按照这些分值进行降序排序。早期相关性的排名模型基于文档中查询项的出现来检索文档，例子包括布尔模型。基本上，这些模型可以预测文档中是否与查询相关，但是无法预测相关的程度。为了进一步建立相关度模型，提出了向量空间模型，文档和查询都表示为欧几里得空间中的向量，向量的内积可以表示为它们的相关性。其中TF-IDF常用语表示查询和文档的向量。 虽然VSM暗示了查询单词之间的独立性，隐藏语义索引LSI避免了这种假设，尤其是使用SVD作为讲原始线性空间分解为“潜在语义空间”。然后使用这个新空间中的相似性来定义查询和文档之间的相似性。 另外，基于概率排序模型也取得很大成功。BM2和语言模型等著名的排序模型都可以看作是概率排序模型。BM25是按照文档相关性的对数概率对文档进行排序。给定一个query q，包含了很多的单词t1,t2,…,tM, 计算一个文档的BM25值的公式：其中TF表示文档d中单词t的出现频率，LEN(d)表示文档d的长度（字数），avdl表示文档集合的平均文档长度。$k_1$和$b$表示自由参数，控制每个项的权重。LMIR是将统计LM应用到IR上。统计LM会对一个序列的单词分配一个改了。当应用到IR上，LM会与文档相关联。利用查询query q，基于文档会产生query的可能性$P(q|d)=\prod_{i=1}^{M}P(t_i|d)$.$\lambda$表示平滑系数。 重要的排序模型PageRank模型PageRank使用随机点击链接的浏览者到达特定网页对网页进行排名的概率。 在一般情况下，任何页面的PageRank值都可以表示为网页$d_u$的PageRank值依赖于所有指向du的网页的rank值处于该网页的外链数。 Query-Level Position-Based Evaluations 基于query位置的评估方法有了模型，我们就需要一个可以用来衡量模型好坏的评估方法。 Cranfield 实验方法： 收集很多随机选择的query 对于每个query 收集与这个query相关的文档集${d_j}_{J=1}^M$/ 人工对这些文档进行相关度评价 使用模型对文档集进行排序 使用评估方法衡量模型排序结果和真实结果的差异 使用所有query评估结果的平均值作为模型的性能结果 相关度判断方法： 相关程度：人工标注判断一个文档是否相关。假设对同一个qeury q的两个文档，我们分配对应的相关度量。如果两个文档$d_u$和$d_v$，对应的相关度$l_u&gt;L_V$那么，u就要比v更相关。 成对偏好：指定一个文档比其他文档更相关。如果一个文档u要比文档v更相关，我们标定$l_{u,v}=1$,否则就标定为-1. 整体顺序：对于给定的query标定文档集的整体排序。对于文档集合${d_j}_{j=1}^M$，将其表示为一个特定的序列，表示$\pi_l$ 大多数评估测量是首先针对每个查询定义的，作为排名模型和相关性判断给出的排序列表π的函数。然后对测试集中的所有查询求平均度量。一下介绍的方法都是通过最大化值进行优化。如MRR，MAP，NDCG等，我们可以通过考虑使用1减去这些值来最小化值作为优化目标。 Mean Reciprocal Rank（MRR) 平均倒数排名对于query q，排序结果中第一个相关文档的位置表示为$r_1$, MRR表示为r1的倒数。 Mean Average PrecisionMAP首先，我们定义位置k(P@K)的精确度。假设我们有两个标签相关和不想关。1表示相关，0表示不想关。然后，就可以定义AP：m表示和query q相关联的所有文档的数目。$m_1$表示为相关文档的树木。对于上图1.4，我们可以计算得到：P@1=1,p@2=1/2,p@3=2/3,然后，AP=5/6 Discounted Cumulative Gain (DCG)DCG可以根据多个有序列类别来利用相关性判断，并在定义中具有明确的折扣因子。假设查询q的排序列表是pi，那么位置k的DCG表示为：在这里，G(.)表示文档的评级，通常设置为$G(z)=(2^z-1)$ $\eta(.)$表示为位置的折扣因子,通常设置为$\eta(j)=1/log(j+1)$。 对其进行归一化，除去DCG@k的最大值。得到归一化的NDCG: 以下摘自链接：http://sofasofa.io/forum_main_post.php?postid=1002561 Rank Correlation (RC) 总结下： 所有评价方法都是计算在query level的。 都是基于位置进行度量的。这些方法通常是不可微的，所以优化起来很麻烦。 Learning to rankMachine Learning Framework许多机器学习研究中，一下几个关键组成部分： 输入空间。通常，对象是由特征向量组成，根据不同的应用进行提取。 输出空间，包含了针对输入对象的学习目标。机器学习的输出空间有两个相关但是不同的定义。首先是任务的输出空间，她高度依赖于应用的场景。第二个是让学习过程更容易的输出空间，这个可能和任务的输出不同。例如，使用回归方法解决分类任务的时候。 假设空间。定义了输入空间到输出空间的函数类。 为了优化参数，通常使用训练集。损失函数测量模型输出和真实标签的一致程度。利用损失函数，可以在训练集上定义经验损失，通过经验损失最小化学习最优假设。Learning to rank定义我们定义使用机器学习完成排序任务的方法为learning-to-rank方法。多数的算法都是通过监督学习优化query-documnet对的特征。我们称具有以下两种属性的方法为leanring-to-rank方法 Feature Based。基于特征意味着所有的文档都是用向量表示。学习排名的典型特征包括文档中查询术语的频率，BM25模型和PageRank模型的输出，甚至本文档与其他文档之间的关系。 判别训练判别训练是指leanring to rank有自己的输入空间，输出空间，假设空间和损失函数。判别式训练事是从训练数据中自动学习的过程，这个对于真实搜索引擎是十分重要的，因为搜索引擎每天都会接受大量用户反馈和使用log，自动学习并不断改进排名机制是非常重要的。 Learning-to-Rank Framework 排名算法分为三种：PointWise，pairwise，listwise不同的方法，定义不同的输入和输出空间，使用不同的假设函数以及不同的损失函数。 Pointwise方法输入空间是包含单个文档的特征向量。输出空间是每个文档的相关度，可以根据相关程度，转换为不同的标签。ground truth label表示正确的标注。 如果判断是直接通过相关程度$l_j$给出，那么文档的标签就可以定义为$y_j=l_j$ 如果判断是给定了一个成对的偏好$l_{uv}$，则可以通过计算击败其他文档的频率来得到文档的正确标签。 如果判断结果是给定了一个总顺序$\pi_l$。则可以通过使用映射函数获得真是标签。例如，文档在$\pi_l$的位置可以当作是真实标签。 假设空间包含了将文档的特征向量作为输入并预测文档的相关度的函数，我们称之为评分函数。基于评分函数，我们可以对文档进行排序生产最终的排序列表。 损失函数检查了每个文档的真实标签预测准确度，在不同的pointwise算法中，排序是作为一个回归，分类模型来做的，因此相应的损失函数也可以当作该算法的损失函数 需要注意的是，逐点方法不考虑文档之间的相互依赖性，因此文档在最终的排序结果的位置对于其损失函数是不可见的，该方法没有利用某些文档实际上与同一个查询相关联的事实。这种发发有其局限性。 Pairwise方法输入空间是成对的文档，都表示为特征向量。输出空间也是包含了成对的属性（我们从{+1,-1}中取值）。根据成对偏好，我们可以将不同类型的判断转化为标签。 如果相关程度定义为$l_j$，那么$(x_u,x_v)$成对标签就可以定义为$y_{u,v}=2*I_{l_u&gt;l_v}-1$ 如果相关程度是直接给定的偏好，就直接设定$y_{u,v}=l_{u,v}$ 如果给定的是总的顺序$\pi_l$，定义$y_{u,v}=2*I_{\pi_l(u)&lt;\pi_l(v)}-1$ 假设空间包含了双变量函数h，他将一对文档作为输入并输出它们的相对顺序。为了简单，我们使用评分函数f定义，即：$h（x_u，x_v）= 2·I {f（x_u）&gt; f（x_v）} - 1$。 损失函数是衡量$h(x_u,x_v)$和真实标签$y_{u,v}$。 成对方法使用的损失函数仅仅考虑了两个文档之间的相对顺序。然而，仅仅查看一对文档，很难得出最终的列表的位置。考虑到信息检索中都是使用query级别和位置，我们可以看到这种方法和信息检索的排序之间的差别。 Listwise方法输入空间是一个queyr对应的相关联的文档集合,如$x={x_j}_{j=1}^m$输出空间是文档的排序列表。不同的判断发过誓可以转化为排序列表的标签。 对于给定了相关度的判别，按照相关度的大小排序得到一个列表作为真实标签。 如果给定的是pairwise的偏好，同样定义一个满足所有pairwise偏好的列表座位真实标签。 给定一个总的列表，直接定义标签为$\pi_y=\pi_l$ 对于列表方法，学习过程是为了让输出空间与任务的输出完全相同。优化过程更直接。 假设空间包含了多变量的函数h，会对一个文档集进行处理并预测他们的序列。假设函数h通常由评分函数f来实现，例如$h(x) = sort \odot f(x)$ Listwise方法的损失函数通常有两种，一种是和评价方法有关（度量特定损失函数），另外一种无关（非度量特定损失函数）。我们主要参考一下标准来将listwise方法和其他两种方法区分： 列表损失函数是根据一个query以及对应的所有训练文档定义的 liswise损失函数不能完全分解为单个文档或者文档对的求和 强调排序列表的概念，并且最终的排序结果的文档位置在损失函数中是可见的。 总结表： 文章使用的符号： Chapter2 The Pointwise方法本章讨论了基于回归算法，基于分类的算法和基于序数的肃反啊，然后讨论他们的优缺点。对于基于回归的算法，输出空间包括实数值相关分数。对于分类的算法=，输出空间包含了无序的类别，对于基于序数的算法，输出空间输出有序的类别。 Regression-Based Algorithms排序问题被当作一个回归问题。 Subset Ranking with Regression给定了 query q对应的文档集$x={x_j}_{j=1}^m$,真实标签 $y={y_j}_{j=1}^m$. 打分函数f用来排序这些文档。损失函数定义为： 只有当评分函数 能够准确输出时候，没有损失，否则就会有二次的损失。对于相关文档，只有当打分函数准确输出为1时候，损失为0，否则如果打分为2，看起来是对这个文档具有更强的相关预测，也会有损失。所以，某种意义上，这样的损失函数不合理。 作为一种拓展，有提出一种加权回归模型。权重有助于模型更集中雨相关文档的回归错误。另外，还对回归损失增加了正则化项，减少过拟合风险。 平方损失的理论是基于NDCG的排名误差的理论上限，然而根据以上的讨论，即使有较大的损失，对应的排序结果可能也是最优的。我们可以认为平方损失是基于NDCG排名错误的松散约束。 Classiﬁcation-Based Algorithms二元分类基于SVM的方法假设文档对应的标签y为1表示相关，-1表示不想关 其实就是1以文档的特征向量作为输入，使用SVM做二元分类判断该文档是否和query匹配。 Logistic Regression-Based Method Multi-class Classiﬁcation for RankingBoosting Tree-Based Method给定了一个文档集合 $x={x_j}^m_{j=1}$.对应的相关度为 $y={y_j}_{j=1}^m$. 假设又一个多元分类器，预测文档对应的相关度。 然后，用于学习分类器的损失函数是定义为0-1分类错误的替代函数： 在增强树算法，我们使用以下代理损失函数： Association Rule Mining-Based Method 基于关联规则挖掘的方法从训练数据中发现文档归属于某一类的特征。 基于序数回归的算法此时，输出空间包含的是有序类别。 通常是找到一个打分函数，然后用一系列阈值对得分进行分割，得到有序类别。采用 PRanking、基于 margin 的方法都可以。 Perceptron-Based Ranking (PRanking)Pranking的目的是在投影文档之后找到参数向量定义的方向，在文档上可以容易使用阈值将文档区分为不同的有序类别。 Ranking with Large Margin Principles主要是使用SVM技术学习模型参数和阈值b 讨论与相关反馈的关系学习排名的逐点方法，特别是基于分类的算法，与相关反馈算法有很强的相关性 相关反馈算法在信息检索中有重要作用。基本思想是从显示，隐式或者盲目的反馈中学习以更新原始的query，新的query用于检索得到一组新文档，通过迭代式执行这个操作，我们可以使query更接近于最优query，从而提高检索性能。 如Rocchio算法： 所以，我们相当于将query vector看作是一个模型的参数。 这种方法和learning 2 rank方法的区别在于； 该算法空间是VSM算法空间，query和document都表示为向量，他们之间的内积当作是相关程度。与此不同的是，l2r算法中，特征空间是从每一个query-document对中抽取得到的。 该算法从反馈中学习模型的参数，然后去对和相同query的文档进行新的排序。l2r则是会对未知的query进行排序验证 该算法的模型参数w实际上具有物理意义，即是更新的查询向量。而l2r则没有这个含义，并且仅对应于每个特征对于排名任务的重要性。 算法的目的是更新query以获得更好的结果，但是不学习最佳的排名。 Pointwise算法问题 pointwise 类方法并没有考虑同一个 query 对应的 docs 间的内部依赖性。一方面，导致输入空间内的样本不是 IID 的，违反了 ML 的基本假设，另一方面，没有充分利用这种样本间的结构性。其次，当不同 query 对应不同数量的 docs 时，整体 loss 将会被对应 docs 数量大的 query 组所支配，前面说过应该每组 query 都是等价的。 ranking 追求的是排序结果，并不要求精确打分，只要有相对打分即可。 损失函数也没有 model 到预测排序中的位置信息。因此，损失函数可能无意的过多强调那些不重要的 docs，即那些排序在后面对用户体验影响小的 doc。 改进方法RankCosine介绍了一种query-level normalization factor，定义的损失函数是基于query q的得分函数f和真实标签的得分向量的cosine相似度。 Chapter 3 The PairWise 方法Pairwise方法不关注与每个文档的相关程度，它只关心与两个文档之间的相对顺序，从这个意义讲，它更接近于“排名”的概念。 在成对方法汇总，排名通常被简化为文档对的分类，即，确定一对文档中哪个文档是优选的。也就是，学习的目标是最小化错误分类文档的数量。 示例算法]]></content>
      <tags>
        <tag>Learning to rank</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The BellKor 2008 Solution to the Netflix Prize]]></title>
    <url>%2F2019%2F01%2F22%2Fthe-bellkor-2008-solution-to-the-betflix-prize%2F</url>
    <content type="text"><![CDATA[介绍主要介绍论文结构 新方法因子模型SVD Asymmetric-SVD SVD++ 这些模型的学习方法是使用SGD，参数$\mu$是指训练集中的平均打分值。 时间效应发现了三种强时间效应 电影偏差：电影会随着时间发生流行度变化 用户偏差：用户会随着时间改变他们的打分基线 用户偏好：用户的偏好会随着时间发生变化 之后，开始介绍下如何在模型中使用这些时间效应以SVD++为例：Here，我们预测随着时间t的打分值r。请注意，相关的参数现在构造为和时间相关的函数。这些偏差是同时学习的。时间偏差效应更容易捕捉到，因为我们不需要最精细的细粒度。对于item的时间效应，比较容易捕获到：作者将训练集时间切分了30份，每一份作为一个bin。同一个电影在不同bin具有不同的结果。对于用户层面的时间效应：]]></content>
      <tags>
        <tag>Recommender System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[the-wistom-of-the-few]]></title>
    <url>%2F2019%2F01%2F22%2Fthe-wistom-of-the-few%2F</url>
    <content type="text"><![CDATA[The Wisdom of the FewA Collaborative Filtering Approach Based on Expert Opinions from the Web]]></content>
      <tags>
        <tag>Recommender System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matirx-factorization-for-RS]]></title>
    <url>%2F2019%2F01%2F21%2Fmatirx-factorization-for-RS%2F</url>
    <content type="text"><![CDATA[矩阵分解技术在推荐系统中的应用潜在因子分解最成功的案例是基于矩阵分解的，在其基本形式中，矩阵分解对商品评价来推断出用户和item的隐藏因子向量。item和user之间的高匹配会导致推荐。推荐系统通常依赖的数据防止在矩阵中，其中一个维度是用户，一个维度是感兴趣的项目。通常这个矩阵是稀疏的，因为单个用户可能仅仅评估了项目中一小部分。矩阵分解的优势在于，它允许结合其他的信息。当显示反馈不可用时候，推荐系统可以根据隐式的反馈意见判断用户偏好。隐式反馈通过观察用户行为间接反映意见，包括购买历史，浏览历史，搜索模式，甚至鼠标移动。 隐式反馈通常表示事件的存在或不存在，因此它通常由密集填充的矩阵表示。 基础的MF模型item i对应的向量为$q_i$,用户u对应的隐向量为$p_u$，两者之间的感兴趣程度可以表示为$$ r_{ui}=q_i^T p_u $$ 然后，可以计算得到一个用户最感兴趣的商品。 这个模型的方法最相关是SVD。但是，在使用SVD时候，是需要被分解的矩阵是完整的，但是实际中的用户-项目矩阵是稀疏且有大量missing value。所以，不能直接使用SVD。 有一些方法是直接建模去拟合已经有的用户打分数据。 使用惩罚系数$\lambda$来控制隐藏因子矩阵，防止过拟合。 学习算法SGD alternating least squares因为两个向量都是位置的，所以公式2是非凸的问题，所以，如果我们固定其中一个向量，优化另外一个，就可以进行优化。 ALS方法就是交替性的固定某个向量，优化另外一个向量，直到收敛为止。 Add Bias观察到的评级大小的变化是由于用户和项目相关的影响，称为偏差，与任何交互无关。在实际中，某些用户比其他用户更倾向打高分，某些项目更容易获得更高的评分等。我们可以通过增加bias来解决：其中，$\mu$表示整体的平均打分值。$b_u$ $b_i$分别表示用户u的偏差和项目i的偏差。 额外的输入通常，一个系统必须要解决冷启动问题。解决这个问题的方法是合并有关用户的其他信息源。 随时间动态变化矩阵分解方法非常适合对时间效应建模，这可以显著提高准确性。将评级分解为： 可变置信度的输入并非所有的评级都是具有相同的权重和置信度。 使用$c_{iu}$表示评价$r_{iu}$的置信度]]></content>
      <tags>
        <tag>Recommender System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filtering]]></title>
    <url>%2F2019%2F01%2F21%2FFiltering%2F</url>
    <content type="text"><![CDATA[Recommendations Item-to-Item Collaborative Filtering推荐系统是使用消费者的兴趣作为输入生产一个推荐项目的列表。很多的应用只有使用顾客购买的item以及明确的评价，但是他们可能也会有其他的属性，包括查看的项目，人口统计数据，主题兴趣和喜好的艺术家等。 点击量：广告被点击的次数点击率：点击率：广告被点击的比例，点击量/展现量转化次数：访问这到达转换目标页的次数转化率（VR）：转换次数/访问次数 零售商拥有大量数据，数千万的用户和商品 许多场景需要在半秒内返回结果，同时要生产高质量的建议 新客户冷启动问题 老客户可以获取稻粱信息 数据不稳定：每次交互都能提供有价值的客户数据，算法必须要即使响应新的信息 Recommendation Algorithms多数的推荐系统都是首先找到一组和当前客户的购买和评价的商品有重叠的用户，算法聚集这些相类似的顾客，消除该用户已经购买的商品，然后推荐给这个用户剩余的商品。 传统的协同过滤算法传统方法将客户表示为项目的N维度向量，其中N是不同的项目目录。向量的组成部分对于购买或者评级为正的项目为正，对于负评级项目为负。为了抵消流行商品的影响，算法会通过乘以一个反频率（购买或者评价商品顾客数量的倒数），使不知名的商品更具有相关性。对于所有用户来说，这个向量是十分稀疏的。该算法基于与用户最相似的少数客户生成推荐，他可以以各种方式衡量两个顾客A和B的相似度，可以通过余弦距离测量。通过对一个商品计算有多少相似顾客会购买它来进行排序。 使用协同过滤的方法复杂度很高，最差会有O(MN)的复杂度。M是顾客数据，而N是每个顾客对应的N个商品。 聚类模型使用一个相似度衡量方法，聚类算法可以将最相似的几个顾客聚到一起组成一个族。然后根据同一个Cluster中的顾客作为相似顾客做推荐 Search-Based方法基于搜索，也叫做基于内容的推荐方法，是通过搜索相关商品来做推荐。给定一个用户的购买记录，算法能够构造出一个搜素的query，去找到其他相关的相似商品。 Item-to-item协同过滤算法amazon会提供购物车建议，基于他们的购物车推荐给用户商品。amazon使用推荐算法为每个客户的兴趣进行个性化，item-to-item协同过滤，可以实时生成高质量的建议。 如何工作？这种方法并不是将客户和类似的客户做匹配，而是将客户购买的商品的每一个和类似的商品做匹配，然后将这些商品组合到推荐列表中。为了确定给定商品的最相似匹配，该算法通过查找客户倾向于一起购买的商品构建项目表。 这个方法的优势在于，可以在离线状态下计算similar-items，然后线上能够实时推荐用户相似的商品。而传统的方法无法保证这一点。]]></content>
      <tags>
        <tag>recommend system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度文本匹配模型]]></title>
    <url>%2F2018%2F09%2F02%2F%E6%B7%B1%E5%BA%A6%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[DSSM(Deep Semantic Structrured Model)结构可以在检索的场景下，使用点击数据来训练语义层次的匹配 输入为一个query和这个query相关的doc，这里的特征可以使最简单的one-hot形式，我们要计算query和doc的相似度。 one-hot编码会导致维度过大以及OOV问题 所以采用word-hashing的方法 之后就是传统的神经网络$$ l_i = f(W_il_{i-1} + b_i) $$ 得到y之后，就可以用cos函数计算query和doc之间的相似度了。 Word HashingWord Hashing是paper非常重要的一个trick，以英文单词来说，比如good，他可以写成#good#，然后按tri-grams来进行分解为#go goo ood od#，再将这个tri-grams灌入到bag-of-word中，这种方式可以非常有效的解决vocabulary太大的问题(因为在真实的web search中vocabulary就是异常的大)，另外也不会出现oov问题，因此英文单词才26个，3个字母的组合都是有限的，很容易枚举光。那么问题就来了，这样两个不同的单词会不会产出相同的tri-grams，paper里面做了统计，说了这个冲突的概率非常的低，500K个word可以降到30k维，冲突的概率为0.0044% 但是在中文场景下，这个Word Hashing估计没有这么有效了因为直接使用了word hashing，因为无法记录上下文信息 训练过程得到了query和每个doc的相似度之后，就可以得到后验概率 $ p(D|Q) $ 最终他需要优化的损失函数为:$$ L(Λ)=−log∏(Q,D+)P(D+|Q) $$D+表示被点击的文档，这里就是最大化点击文档的相关性的最大似然]]></content>
      <tags>
        <tag>NLP, Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[semantic-textual-similarity]]></title>
    <url>%2F2018%2F07%2F31%2Fsemantic-textual-similarity%2F</url>
    <content type="text"><![CDATA[Semantic textual similarityModel：ESIM包含的部分： 编码模块：使用bi-lstms编码文本，取最后一层的输出。 局部推理模块 使用soft-attention的方法，得到整个句子的分布式表达。每个time step对应的权值大小是对应编码的向量的乘积。作者并没有使用额外的参数去求这个softmax的权值。 推理合成部分 得到经过softmax的每个word的表达，然后通过concat原始word编码，attention编码，二者的差，二者的点乘。concat到一起 输入到下一层的inference Bi-LSTMs 输出模块 然后，对Bi-LSTM的输出，在time维度上取max pooling，average pooling.将结果concat到一起，输入到MLP中。 BIMPM word representation Layers: 同时使用预训练的词向量，和cnn+lstm的character embedding Context Representation Layer 使用bi-lstm编码得到双向的每个time-step的context embedding Matching Layers 使用了四种匹配策略，可以看做是不同的Attention方法。 Full-Matching P中每一个前向(反向)文法向量与Q前向(反向)的最后一个时间步的输出进行匹配。 Maxpooling-Matching P中每一个前向(反向)文法向量与Q前向(反向)每一个时间步的输出进行匹配，最后仅保留匹配最大的结果向量。 Attentive-Matching 先计算P中每一个前向(反向)文法向量与Q中每一个前向(反向)文法向量的余弦相似度，然后利用余弦相似度作为权重对Q各个文法向量进行加权求平均作为Q的整体表示，最后P中每一个前向(后向)文法向量与Q对应的整体表示进行匹配。 Max-Attentive-Matching 与Attentive-Matching类似，不同的是不进行加权求和，而是直接取Q中余弦相似度最高的单词文法向量作为Q整体向量表示，与P中每一个前向(反向)文法向量进行匹配。 输出层： Matching Layer输出的匹配向量经Aggregation Layer双向LSTM处理后作为最后预测层的输入，预测层利用softmax函数输出预测结果。 MPCNN（15年的论文，但是试验效果非常好） 完全使用CNN来做文本匹配。CNN虽然对时序信息处理的不好，但是能够捕获到关键的信息。CNN 用作 MT 模型介绍 Sentence Model： 将文本转化成一个表达，使用多种类型的卷积核和pooling方法做 多种卷积方式： 对整个sequence 的word embedding使用卷积（对于time step这个维度进行卷积运算） 对word vectors维度使用卷积，得到不同粒度下的信息 多种pooling方式 max, min, mean Multiple Window Sizes 多种卷积核大小 Similarity measure Layer 同样使用多种相似度评测的方法 得到两个text的分布式表达，通过计算余弦距离，欧几里得距离，绝对距离（element-wise） concat到一起 输出层 全连接层和softmax层 在天池的CIKM文本匹配比赛中，这个模型的效果非常好。]]></content>
      <tags>
        <tag>NLP, deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batchnorm如何发挥作用的]]></title>
    <url>%2F2018%2F07%2F16%2FBatchnorm%E5%A6%82%E4%BD%95%E5%8F%91%E6%8C%A5%E4%BD%9C%E7%94%A8%E7%9A%84%2F</url>
    <content type="text"><![CDATA[How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)MIT的研究人员从优化过程对应的空间平滑性这一角度进行分析BN在深度学习中的作用 BatchNormalizatio是一种通过固定层输入的分布来改善神经网络训练的技术，通过引入一个附件的网络控制这些分布的均值和方差。 目前，对BatchNorm的成功以及其最初动机的最广泛接受的解释是，这种有效性源于在训练过程中控制每层输入数据分布的变化以减少所谓的“Internal Covariate Shift”。那什么是Internal Covariate Shift呢，可以理解为在神经网络的训练过程中，由于参数改变，而引起层输入分布的变化。研究人员们推测，这种持续的变化会对训练造成负面影响，而BatchNorm恰好减少了Internal Covariate Shift，从而弥补这种影响。 作者发现了BatchNorm对训练过程有着更根本的影响：它能使优化问题的解空间更加平滑，而这种平滑性确保了梯度更具预测性和稳定性，因此可以使用更大范围的学习速率并获得更快的网络收敛。 BN究竟发挥了什么作用?作者提出，BN是对训练过程的影响：通过对底层优化问题的再参数化，使得解空间更加平滑。损失函数的额平滑性得到改进，即损失函数能够以较小的速率进行变化，梯度的幅度也变小，然而效果更强。这种平滑性对训练算法的性能起到主要的影响。改进梯度的平滑性，可以在计算梯度方向上采用更大的步长。 它能使任何基于梯度的训练算法采取更大的步长之后，防止损失函数的解空间突变，既不会掉入梯度消失的平坦区域，也不会掉入梯度爆炸的尖锐局部最小值。这也就使得我们能够用更大的学习速率，并且通常会使得训练速度更快而对超参数的选择更不敏感。因此，是BatchNorm的平滑效果提高了训练性能。]]></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mix-of-softmax]]></title>
    <url>%2F2018%2F07%2F12%2Fmix-of-softmax%2F</url>
    <content type="text"><![CDATA[BREAKING THE SOFTMAX BOTTLENECK: A HIGH-RANK RNN LANGUAGE MODELICLR2018 Abstract将LM看做是一个matrix分解的问题，现有的softmax-based models会受限于Softmax bottleneck。作者证明了，使用对于分布式的word embedding 使用softmax没有能力model language。作者提出了MoS的方法，改进了softmax。 IntroductionLM的问题可以归结为通过某个word的上下文context，预测next-token.这是一个conditional probability问题。 通常的方法，使用rnn将context编码成一个vector，然后通过softmax求得条件概率，这种方法会很大程度的改变了context。 基于softmax的方法是没有足够能力去model language的。这就是softmax bottleneck。 作者提出Mixture of Softmaxes 作者的github有开源MoS的代码，但是是pytorch版本的，项目需要使用tf，所以就自己改写了，发现，运行速度上要比NCE慢了许多，对比来看，NCE对于训练的加速还是很明显的。训练结果来看，MoS会更好点。 tf实现的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# coding: utf-8# Author: gjweiimport tensorflow as tffrom model.BiLM import ELMOimport ipdbclass ELMoMoS(ELMO): """reference: https://github.com/zihangdai/mos/blob/master/model.py paper: https://arxiv.org/abs/1711.03953.pdf """ def __init__(self, config): super(ELMoMoS, self).__init__(config) def add_loss_op(self): # use mix of softmax with tf.variable_scope("loss"): mask = tf.sequence_mask(self.label_lengths) rnn_output = tf.concat((self.rnn_output[0][:, :-2, :], self.rnn_output[1][:, 2:, :]), axis=-1) # reshape it to [-1, self.rnn_output_dim] inputs = tf.reshape(rnn_output, shape=[-1, self.rnn_output_dim * 2]) # shape is [bs x step, nhidlast] # bs * step, nhidlast ——&gt; bs * step, n_expers * ninp latent = tf.layers.dense(inputs, units=self.config.n_experts * self.config.ninp, activation=tf.nn.tanh, kernel_initializer=tf.glorot_uniform_initializer(), name='latent') # bs * step, n_experts * nipn --&gt; bs * step * n_experts, ninp --&gt; bs * step * n_experts, ntoken latent = tf.reshape(latent, shape=[-1, self.config.ninp]) logit = tf.layers.dense(latent, units=self.config.ntokens, kernel_initializer=tf.glorot_uniform_initializer(), name='logit') # bs * step, nhidlast --&gt; bs * step, n_experts prior_logit = tf.layers.dense(inputs, units=self.config.n_experts, use_bias=False, kernel_initializer=tf.glorot_uniform_initializer(), name='prior_logit') # prior_logit = tf.reshape(prior_logit, shape=[-1, self.config.n_experts]) prior = tf.nn.softmax(prior_logit, axis=-1) # bs * step, n_experts prop = tf.reshape(tf.nn.softmax(tf.reshape(logit, shape=[-1, self.config.ntokens])), shape=[-1, self.config.n_experts, self.config.ntokens]) # [bs * step, n_experts, ntokens] prior = tf.expand_dims(prior, axis=2) # bs * step, n_experts, 1 prop = tf.reduce_sum(tf.multiply(prop, prior), axis=1) # bs * step * n_tokens self.logits = tf.reshape(prop, [-1, self.config.nwords]) # bs * step, n_tokens # labels = tf.reshape(self.labels, shape=[-1, 1]) labels = tf.reshape(tf.one_hot(self.labels, depth=self.config.ntokens), shape=[-1, self.config.ntokens]) losses = 1 * tf.losses.log_loss(labels=labels, predictions=self.logits, reduction=tf.losses.Reduction.NONE) # losses = tf.reshape(losses, shape=tf.shape(self.labels)) # ipdb.set_trace() losses = tf.reshape(tf.reduce_sum(losses, axis=-1), shape=tf.shape(self.labels)) losses = tf.boolean_mask(losses, mask) with tf.variable_scope("train"): # concat forward outputs and backward outputs # ipdb.set_trace() # print(self.rnn_output[0].get_shape) self.loss = tf.reduce_mean(losses) # for tensorboard tf.summary.scalar("train_loss", self.loss) # We have to use e instead of 2 as a base, because TensorFlow # measures the cross-entropy loss with the natural logarithm with tf.variable_scope("eval"): self.loss_eval = tf.reduce_mean(losses) self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1), tf.int32) self.pred_and_target_logits = [tf.reduce_max(self.logits, axis=-1)] # for tensorboard tf.summary.scalar('dev_loss', self.loss_eval)]]></content>
      <tags>
        <tag>NLP, Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Simplified-deep-learning笔记]]></title>
    <url>%2F2018%2F07%2F12%2FSimplified-deep-learning%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[概率论和信息论 基本概念 贝叶斯准则：在已知P(y|x)和P(x)的情况下，$P(x|y) = \frac{p(x)P(y|x)}{P(y)}$, 贝叶斯准则经常被用在已知参数的先验分布情况下求后验分布。信息熵：描述某个概率分布之间的相似度的度量。记做 交叉熵：描述两个概率分布之间的相似度的指标，在机器学习中使用它作为分类任务的损失函数。记做： 常见分布 伯努利二元分布 Multinoulli分布 高斯分布 拉普拉斯分布：有着和高斯分布很相近的形式，概率密度为： 常用函数： Logistics sigmoid函数 ReLU函数 Softplus函数： 结构化概率模型： 概率图模型：使用图的概念来表示概念之间的概率依赖关系，下面就是关于a,b,c,d,e之间的有向图模型，通过该图可以计算：$$ P(a,b, c, d, e) = p(a)p(b|a)p(c|a,b)p(d|b)p(e|c)$$ 机器学习基础性能度量 准确率 错误率 精度 召回率 F1值 容量、过拟合和欠拟合泛化能力是模型在未知数据上的表现是否良好。通常情况下，机器学习的模型是要作用在未知的数据上的，具有良好泛化能力的模型才是符合需求的。 训练误差测试误差欠拟合过拟合：模型在训练集和测试集上的误差差距过大，通常由于模型过分拟合了训练集中的随机噪音，导致泛化能力较差。 最大似然估计在已知分布的样本上，但是不知道分布的具体参数的情况下，根据样本值推断最有可能产生样本的参数值。 最大似然估计的一种解释是使$p_{model}$和$p_{data}$之间的差异性尽可能的小，形式化的描述为最小化两者的KL散度。]]></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logit模型]]></title>
    <url>%2F2018%2F07%2F11%2Flogit%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[解读logistics模型Odds在统计学中，概率（Probability）和Odds都是用来描述某件事情发生的可能性。Odds表示事情发生概率和不发生概率的比值。 我们知道，p的取值范围是[0, 1],所以，可以看出Odds的取值是0 – 正无穷大 如果对Odds取自然对数，就可以将概率从[0, 1]映射到正负无穷大了Odds的对数称之为Logits 和Logistics模型对于分类任务，我们希望得到数据点属于某个类别的概率，如果使用线性模型做分类的时候，如何将线性模型方程和概率联系到一起呢？就是通过Logits！！通过令：$$ logits = wx + b $$从而，将线性方程和概率p映射到了一起。Logitics模型也叫做logits模型。]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[exporing-the-linmits-of-lm]]></title>
    <url>%2F2018%2F06%2F28%2Fexporing-the-linmits-of-lm%2F</url>
    <content type="text"><![CDATA[Exploring the Limits of Language Modeling论文主要讨论了当前的RNN模型在大规模的语言建模方面的进展。 IntroductionLM是NLP的核心任务，可以准确将句子的分布模型可以编码句法结构，而且能够提取到预料中可能包含的到两信息。 当训练大量的data时候，LM能够简洁的抽取到训练数据的知识编码。最近，大规模的LM的研究表明， RNN和N-grams的信息结合起来是很好的，因为它可能有不同的有优势来补充N-grams模型。 本文的贡献： 对当前的大规模的LM模型尽量进行归一化 设计了一种基于character level 的CNN 的softmax loss，train时候很有效。和full softmax结果相近，但是参数更少 产生了新的state-of-the-art的结果。由51.3降到了30.0 对多个模型进行ensemble，得到结果进一步降到了23.7 分享模型帮助更多的研究 Related Work 2.2 Convolution Embedding ModelsIncorporating character-level inputs to build word embedding for various NLP problems character embedding信息的方法： 使用双向的LSTM对字符进行运算，最终输出的state vector concat到一起输入到模型中。 使用CNN对character 进行卷积，取max pooling，然后丢入到一个2层的Highway network中。 2.3 Softmax Over Large Vocabularies当Vocabulary很大的时候，这种方法的计算量是巨大的。目前已经提出的方法：NCE，importance sampling, Hierarchical Softmax等，本论文发现，Importance Sampling更有用，解释了它和NCE的联系。 LM Improvements 联合概率公式： NCE &amp; Importance Sampling Sampling方法适用于训练阶段，它能够产生loss的近似值，但是更容易计算。 NCE是提出一个代理二分类的任务，其中分类器被训练为区分真实数据（正样本）和来自任意分布的样本（负样本）如果我们知道了noise 和data的数据分布，优化这个二分类classifier就是： Y是二元结果k表示negative samples的数量$p_d$和$p_n$表示data和noise的分布。 The other 方法是基于Importance Sampling（IS）该方法提出通过重要性抽样直接估计分配函数（包括所有单词的综合）和估计值。 NCE方法有非常完美的理论证明：随着噪声样本k的数量增加，NCE导数趋近于softmax函数的梯度。Mnih 和 Teh (2012) 认为抽取25个噪声样本就足以达到常规softmax方法的效果，速度能提升大约45倍。 NCE与IS的相似性jozefowicz认为NCE与IS的相似点不仅在于它们都是基于采样的方法，而且相互之间联系非常紧密。NCE等价于解决二分类任务，他认为IS问题也可以用一个代理损失函数来描述：IS相当于用softmax和交叉熵损失函数来优化解决多分类问题。他觉得IS是多分类问题，可能更适用于自然语言的建模，因为它迭代更新受到数据和噪声样本的共同作用，而NCE的迭代更新则是分别作用。事实上，Jozefowicz等人选用IS作为语言模型并且取得了最佳的效果。 CNN Softmax使用普通的softmax计算的logit值是：$$ z_w = h^T e_w $$ 使用CNN来做e_w:$$ e_w = CNN(chars_w) $$使用softmax得到的e_w之间是相互独立的，但是使用CNN-softmax得到的的词向量都是具有平滑的映射关系的。所以，需要小的learing rate进行训练 但是,Char-CNN的缺点是缺乏对于一词多义和具有相同拼写的单词的区分，因此增加一个correction factor。 其中M是投影低维嵌入矢量corrw的矩阵，其返回到投影的LSTM隐藏状态h的维度。 优势： 参数更少，相比于使用Embedding方法 OOV单词更容易scored Char LSTM PredictionA class of models that solve this problem more efficiently are character-level LSTMs 使用char lstm来产生word representation。因此，我们通过将字级LSTM隐藏状态h输入到一个小LSTM中来预测目标单字每次一个字符，从而将单词和字符级模型结合起来 在word representation输入到LSTM(黄色)之后，产生了一个hidden state h，输入到另外一个小的Character-level LSTM（紫色），每次 产生一个character ExperimentDataset1BWord Benchmark data se Model setup PPL 不对数据集进行任何预处理 采用character作为输入和输出，每个word都是作为一串character IDs。word会被处理为具有特殊标识begin和end的形式。最大长度为10.eg:cat会别转换成”$cat^@@@@@”实验表明，最大单词的长度为50的时候，效果最好我们使用256个characters 。非ascii符号表示为字节序列 Model ArchitectureLSTMs，带有一个project layers（eg: a bottleneck between hidden states）, 在20-steps truncated BPTT方法训练效果最好。 使用Dropout在每一个LSTM层，LSTM forget gates bias设置为1.0， 使用large number的features 4096， 然后使用线性变换，将其变化到符合LSTM project sizes。 训练参数Adam + LR=0.2 LSTM最大计算展开长度为20 batch_size = 128 clip gradient = 1 使用较大的Noise Sampling：8192。 Result 和分析size matterLayer层数很重要，试验表明，两层LSTM效果最好，8192 + 1024的state维度 regularization 很重要使用dropout可以提升结果 对于少于4096或者更少的unit的LSTM，dropout = 0.1比较好对于更多unit的LSTM，dropout = 0.25是更好的选择。 Importance Sampling 更有数据效率Word embedding和Character CNN使用Character CNN的优势是可以让Embedding layer得到任意单词的表示，实验表明，char-cnn是可行的，不会降低性能。另外，char-cnn具有更少的参数， Smaller model with CNN SoftmaxConclusionThus, a large, regularized LSTM LM, with projection layers and trained with an approximation to the true Softmax with importance sampling performs much better than N-grams.]]></content>
      <tags>
        <tag>NLP, LM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Noise Contrastive Estimation and Negative Sampling]]></title>
    <url>%2F2018%2F06%2F26%2FNoise%20Contrastive%20Estimation%20and%20Negative%20Sampling%2F</url>
    <content type="text"><![CDATA[Notes on Noise Contrastive Estimation and Negative Sampling结论：NCE在计算正负样本的条件概率大小的时候，考虑了负样本和分布情况，预测的结果不仅和正样本有关，同时和负样本的分布也有关系。负样本的label是 soft label但是，而Negative没有考虑负样本的分布，直接将正负样本都当做是已经存在的（存在即是必然），抽样出来的负样本就是假设已经存在的负样本。对NCE的进一步简化。负样本的lable是hard label Abstract在进行估计LM的参数估计的时候，因为需要评估整个字典的每个word的概率，所以计算起来非常困难。有两种相关的策略用来解决这个问题：Noise ccontrastive estimation 和 Negative Sampling这篇论文指出了，尽管二者非常相似，但是NCE是渐进无偏的一半参数估计技术，而Negative sampling可以理解为一个二分类模型，他对于学习word representation非常有用，但是不能作为一个通用的方法。 Introduction对于LM需要估计下一个单词w。 和前面所叙述的一样，vocab太大，会导致计算概率非常困难。 NCE和NS方法都是将这种计算代价大的方法转化为二分类的代理问题，使用相同的参数，但是更容易进行计算。 Noise contrastive estimation (NCE)NCE将语言模型估计问题减少为估计概率二元分类器的参数的问题，该概率二元分类器使用相同的参数来区分样本与经验分布样本与噪声分布产生的样本 NCE方法两类数据产生过程如下： 从真实数据分布p(c)中采样采样一个数据c， 从数据的真实分布$p(w|c)$得到一个真实样本，标记为1 从’noise’ 分布 q(w)选择k个noise samples ,标记为0. 对于给定正负样本的联合概率就可以表示为： 转转化为条件概率： 这些方程式是根据数据的真实分布做出的。 NCE是将经验分布$\tilde{p}$用模型预测分布$p_{\theta}$替换，$\theta$选则能让条件分布的似然概率最大的权值。 为了减少partition function（1式中的分母部分），NCE做了两个假设： partition function的大小Z(c)可以看做是参数$z_c$的估计，所以，对于每个c，NCE都会引入一个参数 将$z_c$固定为1的时候，最有效。 然后就可以得到经验分布： 然后就可以得到Loss function Negative SamplingNS方法定义条件概率为： 等价于NCE中k取了vocab大小，而且单词的分布式均匀分布的。 参考了一个新的论文：https://arxiv.org/pdf/1402.3722.pdf 考虑到一个单词对(w, c) of word and context.我们的目标实参数最大化所有观测的结果（正确的单词对）来自于data（预测为1） NCE在计算正负样本的条件概率大小的时候，考虑了负样本和分布情况，预测的结果不仅和正样本有关，同时和负样本的分布也有关系。但是，而Negative没有考虑负样本的分布，直接将正负样本都当做是已经存在的（存在即是合理），抽样出来的负样本就是假设已经存在的负样本。对NCE的进一步简化。]]></content>
      <tags>
        <tag>NLP, paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用的正则表达]]></title>
    <url>%2F2018%2F06%2F10%2F%E5%B8%B8%E7%94%A8%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%2F</url>
    <content type="text"><![CDATA[正则表达式 常用正则表达式大全！（例如：匹配中文、匹配html） 匹配中文字符的正则表达式： [u4e00-u9fa5] 评注：匹配中文还真是个头疼的事，有了这个表达式就好办了 匹配双字节字符(包括汉字在内)：[^x00-xff] 评注：可以用来计算字符串的长度（一个双字节字符长度计2，ASCII字符计1） 匹配空白行的正则表达式：ns*r 评注：可以用来删除空白行 匹配HTML标记的正则表达式：]&gt;.?| 评注：网上流传的版本太糟糕，上面这个也仅仅能匹配部分，对于复杂的嵌套标记依旧无能为力 匹配首尾空白字符的正则表达式：^s|s$ 评注：可以用来删除行首行尾的空白字符(包括空格、制表符、换页符等等)，非常有用的表达式 匹配Email地址的正则表达式：w+([-+.]w+)@w+([-.]w+).w+([-.]w+)* 评注：表单验证时很实用 匹配网址URL的正则表达式：[a-zA-z]+://[^s]* 评注：网上流传的版本功能很有限，上面这个基本可以满足需求 匹配帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$ 评注：表单验证时很实用 匹配国内电话号码：d{3}-d{8}|d{4}-d{7} 评注：匹配形式如 0511-4405222 或 021-87888822 匹配腾讯QQ号：[1-9][0-9]{4,} 评注：腾讯QQ号从10000开始 匹配中国邮政编码：[1-9]d{5}(?!d) 评注：中国邮政编码为6位数字 匹配身份证：d{15}|d{18} 评注：中国的身份证为15位或18位 匹配ip地址：d+.d+.d+.d+ 评注：提取ip地址时有用 匹配特定数字： ^[1-9]d*$ //匹配正整数 ^-[1-9]d*$ //匹配负整数 ^-?[1-9]d*$ //匹配整数 ^[1-9]d*|0$ //匹配非负整数（正整数 + 0） ^-[1-9]d*|0$ //匹配非正整数（负整数 + 0） ^[1-9]d.d|0.d[1-9]d$ //匹配正浮点数 ^-([1-9]d.d|0.d[1-9]d)$ //匹配负浮点数 ^-?([1-9]d.d|0.d[1-9]d|0?.0+|0)$ //匹配浮点数 ^[1-9]d.d|0.d[1-9]d|0?.0+|0$ //匹配非负浮点数（正浮点数 + 0） ^(-([1-9]d.d|0.d[1-9]d))|0?.0+|0$ //匹配非正浮点数（负浮点数 + 0） 评注：处理大量数据时有用，具体应用时注意修正 匹配特定字符串： ^[A-Za-z]+$ //匹配由26个英文字母组成的字符串 ^[A-Z]+$ //匹配由26个英文字母的大写组成的字符串 ^[a-z]+$ //匹配由26个英文字母的小写组成的字符串 ^[A-Za-z0-9]+$ //匹配由数字和26个英文字母组成的字符串 ^w+$ //匹配由数字、26个英文字母或者下划线组成的字符串 在使用RegularExpressionValidator验证控件时的验证功能及其验证表达式介绍如下: 只能输入数字：“^[0-9]*$” 只能输入n位的数字：“^d{n}$” 只能输入至少n位数字：“^d{n,}$” 只能输入m-n位的数字：“^d{m,n}$” 只能输入零和非零开头的数字：“^(0|[1-9][0-9]*)$” 只能输入有两位小数的正实数：“^[0-9]+(.[0-9]{2})?$” 只能输入有1-3位小数的正实数：“^[0-9]+(.[0-9]{1,3})?$” 只能输入非零的正整数：“^+?[1-9][0-9]*$” 只能输入非零的负整数：“^-[1-9][0-9]*$” 只能输入长度为3的字符：“^.{3}$” 只能输入由26个英文字母组成的字符串：“^[A-Za-z]+$” 只能输入由26个大写英文字母组成的字符串：“^[A-Z]+$” 只能输入由26个小写英文字母组成的字符串：“^[a-z]+$” 只能输入由数字和26个英文字母组成的字符串：“^[A-Za-z0-9]+$” 只能输入由数字、26个英文字母或者下划线组成的字符串：“^w+$” 验证用户密码:“^[a-zA-Z]w{5,17}$”正确格式为：以字母开头，长度在6-18之间， 只能包含字符、数字和下划线。 验证是否含有^%&amp;’’,;=?$”等字符：“[^%&amp;’’,;=?$x22]+” 只能输入汉字：“^[u4e00-u9fa5],{0,}$” 验证Email地址：“^w+[-+.]w+)@w+([-.]w+).w+([-.]w+)*$” 验证InternetURL：“^http://([w-]+.)+[w-]+(/[w-./?%&amp;=]*)?$” 验证电话号码：“^((d{3,4})|d{3,4}-)?d{7,8}$” 正确格式为：“XXXX-XXXXXXX”，“XXXX-XXXXXXXX”，“XXX-XXXXXXX”， “XXX-XXXXXXXX”，“XXXXXXX”，“XXXXXXXX”。 验证身份证号（15位或18位数字）：“^d{15}|d{}18$” 验证一年的12个月：“^(0?[1-9]|1[0-2])$”正确格式为：“01”-“09”和“1”“12” 验证一个月的31天：“^((0?[1-9])|((1|2)[0-9])|30|31)$” 正确格式为：“01”“09”和“1”“31”。 匹配中文字符的正则表达式： [u4e00-u9fa5] 匹配双字节字符(包括汉字在内)：[^x00-xff] 匹配空行的正则表达式：n[s| ]*r 匹配HTML标记的正则表达式：/.*|/ 匹配首尾空格的正则表达式：(^s)|(s$) 匹配Email地址的正则表达式：w+([-+.]w+)@w+([-.]w+).w+([-.]w+)* 匹配网址URL的正则表达式：http://([w-]+.)+[w-]+(/[w- ./?%&amp;=]*)? (1)应用：计算字符串的长度（一个双字节字符长度计2，ASCII字符计1） String.prototype.len=function(){return this.replace([^x00-xff]/g,”aa”).length;} (2)应用：JavaScript中没有像vbscript那样的trim函数，我们就可以利用这个表达式来实现 String.prototype.trim = function() { return this.replace(/(^s)|(s$)/g, “”); } (3)应用：利用正则表达式分解和转换IP地址 function IP2V(ip) //IP地址转换成对应数值 { re=/(d+).(d+).(d+).(d+)/g //匹配IP地址的正则表达式 if(re.test(ip)) { return RegExp.$1Math.pow(255,3))+RegExp.$2Math.pow(255,2))+RegExp.$3255+RegExp.$41 } else { throw new Error(“Not a valid IP address!”) } } (4)应用：从URL地址中提取文件名的javascript程序 s=”http://www.9499.net/page1.htm&quot;; s=s.replace(/(./){0,}([^.]+)./ig,”$2”) ;//Page1.htm (5)应用：利用正则表达式限制网页表单里的文本框输入内容 用正则表达式限制只能输入中文：onkeyup=”value=value.replace(/[^u4E00-u9FA5]/g,’’) “onbeforepaste=”clipboardData.setData(‘’text’’,clipboardData.getData(‘’text’’).replace(/[^u4E00-u9FA5]/g,’’))” 用正则表达式限制只能输入全角字符： onkeyup=”value=value.replace(/[^uFF00-uFFFF]/g,’’) “onbeforepaste=”clipboardData.setData(‘’text’’,clipboardData.getData(‘’text’’).replace(/[^uFF00-uFFFF]/g,’’))” 用正则表达式限制只能输入数字：onkeyup=”value=value.replace(/[^d]/g,’’) “onbeforepaste= “clipboardData.setData(‘’text’’,clipboardData.getData(‘’text’’).replace(/[^d]/g,’’))” 用正则表达式限制只能输入数字和英文：onkeyup=”value=value.replace(/[W]/g,’’) “onbeforepaste=”clipboardData.setData(‘’text’’,clipboardData.getData(‘’text’’).replace(/[^d]/g,’’ 作者：图灵95链接：https://www.nowcoder.com/discuss/24910?type=0&amp;order=0&amp;pos=23&amp;page=1来源：牛客网]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习常见到的问题]]></title>
    <url>%2F2018%2F06%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[机器学习项目机器学习项目一般都会牵扯到数据预处理，建模，评估，可视化，部署上线等一系列流程。 数据预处理如何处理缺失数据（missing data）？各种方法有什么利弊 用平均值、中值、分位数、众数、随机数替代。效果一般，等价为认为增加了噪音 用其他变量作为特征设计一个预测模型。效果比1略好，但是有缺陷。如果预测不准，预测结果没有意义，如果预测的很准，说明这个特征和其他特征相关性很大，没有必要加入到模型中 最精确的做法：将变量映射到高维空间，比如性别有男、女和缺失三种情况，则可以映射为是否男，是否女，是否缺失（one hot形式）。连续变量也可以这样处理。不需要考虑缺失值，保留了完整的数据信息。但是计算量会大大增加。 如何将描述变量(categorical variables)转为连续变量(continuous variables)?如何处理有序变量？ 有些特征虽然也像无序特征那样只取限定的几个值，但是这些值之间有顺序的含义。例如一个人的状态status有三种取值：bad, normal, good，显然bad &lt; normal &lt; good。 当然，对有序特征最简单的处理方式是忽略其中的顺序关系，把它看成无序的，这样我们就可以使用处理无序特征的方式来处理它。在实际问题中，这种处理方式其实用的很多。 当然有些问题里有序可能会很重要，这时候就不应该把其中的顺序关系丢掉。一般的表达方式如下： status取值 向量表示123bad (1, 0, 0) normal (1, 1, 0)good (1, 1, 1) 上面这种表达方式很巧妙地利用递进表达了值之间的顺序关系。 如何处理无序变量？可以使用one hot 编码方式 如何进行特征选择？如何进行数据压缩？特征选择：包裹式，过滤式，嵌入式 数据压缩：主成分分析，自编码等 数据不均衡的问题数据不均衡的处理方法主要包括 Sampling方法：基于采样的方法，对数据集进行过采样或者欠采样。欠采样会带来信息丢失的问题，过采样又会导致过拟合问题。还有一种采样是插值方法，如smote(选择每个正样本的k近邻，然后在该样本和近邻样本的连线上随机采样)，对数据进行重新生成的方法。 采用多个分组将多数的类别数据拆分成多个少数组数据，然后分别进行训练取bagging。这个方法更适合于集成方法 kernel类模型，还可以通过修改核函数偏移超平面，抵消不均衡问题。 通过修改cost function的计算方法，对少数类的损失赋予一个较大的权值，让模型更注重少数类的学习。 模型解释什么是欠拟合和过拟合？如何应对这两种情况什么是偏差与方差分解(Bias Variance Decomposition)？与欠拟合和过拟合有什么联系？评估方法分类模型评估方法？准确度精确度（预测的准不准）recall值（预测的全不全）F1 scoreAUC等 回归问题评估方法？MSEMAE 数据不均衡的评估方法？AUCF1 scoreF均值（调和均值，通过控制recall的权值控制更重视precision和recall中的哪一个） 模型之间的对比深度学习是否要比其他学习模型好？视情况而定没有免费午餐定理]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch使用笔记]]></title>
    <url>%2F2018%2F05%2F30%2Fpytorch%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[优化器使用为每个参数单独设置选项 123optim.SGD([&#123;'params': model.base.parameters()&#125;, &#123;'params': model.classifier.parameters(), 'lr': 1e-3&#125; ], lr=1e-2, momentum=0.9) 这意味着 model.base 的参数将会使用 1e-2 的学习率,model.classifier 的参数将会使用 1e-3 的学习率, 并且 0.9 的 momentum 将应用于所有参数. 12345678ignored_params = list(map(id, model.fc.parameters()))base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())optimizer = torch.optim.SGD([ &#123;'params': base_params&#125;, &#123;'params': model.fc.parameters(), 'lr': 1e-2&#125; ], lr=1e-3, momentum=0.9) 如何调整学习率class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)通过 gamma 在每一个 epoch 里面的 step_size 设置每个参数组的初始学习率衰减变量. 当 last_epoch=-1, 设置初始 lr 为 lr.1234567891011# Assume optimizer use 0.05 as lr# lr = 0.05 if epoch &lt; 30# lr = 0.005 if 30 &lt;= epoch &lt; 60# lr = 0.0005 if 60 &lt;= epoch &lt; 90。。。scheduler = StepLR(optimizer, step_size=30, gamma=0.1)for epoch in range(100): scheduler.step() train(...) validate(...) class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)[source]和上个方法类似，知识LR的衰减是在到达milestones才开始的。gamma是衰减的系数 class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode=&#39;rel&#39;, cooldown=0, min_lr=0, eps=1e-08)当一个指标已经停止提升时减少学习率.模型通常受益于通过一次2-10的学习停止因素减少学习率 这个调度程序读取一个指标质量 以及看到 ‘patience’ 的数量在一个 epoch 里面如果没有提升, 这时学习率已经减小. 1234567&gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)&gt;&gt;&gt; scheduler = ReduceLROnPlateau(optimizer, 'min')&gt;&gt;&gt; for epoch in range(10):&gt;&gt;&gt; train(...)&gt;&gt;&gt; val_loss = validate(...)&gt;&gt;&gt; # Note that step should be called after validate()&gt;&gt;&gt; scheduler.step(val_loss)]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter11-实践方法论]]></title>
    <url>%2F2018%2F05%2F30%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter11-%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%B3%95%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[在机器学习的日常开发中，实践者需要决定是够收集足够的数据，增加或者减少模型容量、增加或者删除正则化项、改进模型的优化、改进模型的近似推断或者调试欧兴的软件实现等。 性能度量精度，查准率（precision）召回率，查全率 Recall F1 Score = 2 / (1 / p + 1 / R) 默认的基准模型如全连接网络，ReLU，PReLU等激活函数，Dropout， Adam算法，批标准化 决定是够收集更多的数据怎么判断是否要收集更多的数据？如果模型在训练数据上都没有办法表现良好，一种可能是模型有问题，改进模型。另外一种可能就是数据的质量。 如果模型在训练数据上表现良好，但是在测试数据上表现很差（过拟合），收集更多的数据是解决方法之一（最好的方法），但是代价很大。这个时候，可以尝试调整超参数，加入正则化策略，BN等） 选择超参数手动选择手动选择参数的主要目的是调整模型的有效容量以匹配任务的复杂性。 对于某些超参数，当数值越大的时候，会发生过拟合。例如中间层隐藏单元的数量，增加数量能够提高模型的容量，容易发生过拟合。有些超参数太小，也会发生过拟合，如最小权值衰减系数为0，此时学习算法具有最大的容量，反而容易发生过拟合。 学习率可能是最重要的超参数了，相比其他超参数，它以一种非常复杂的方式控制模型的复杂度。当学习了适合问题时候，模型的有效容量最高。如果超参数过大或者过小，都会对训练产生影响。 网格搜索网格搜索选择所有超参数的组合进行训练，然后选择出最好的超参数。问题：计算量随着超参数的数量呈指数增加 随机搜索过程如下，首先，为每个超参数确定一个边缘分布，例如伯努利分布，或者对数尺度上的均匀分布。例如log_learningrate ~ U(a, b) 我们不需要离散化超参数的值，允许在更大的参数空间上进行搜索，而不产生额外的计算代价。 调试优化 可视化计算中模型的行为 可视化最严重的错误 拟合极小的数据集 监控激活函数和梯度的直方图]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter10-序列建模：循环和递归网络]]></title>
    <url>%2F2018%2F05%2F30%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter10-%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%EF%BC%9A%E5%BE%AA%E7%8E%AF%E5%92%8C%E9%80%92%E5%BD%92%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[CNN和RNN都是NLP中常用的模型，两个模型捕捉特征的角度也不太一样，CNN善于捕捉文本中关键的局部信息，而RNN则善于捕捉文本的上下文信息（考虑语序信息），并且有一定的记忆能力，两者都可以用在文本分类任务中，而且效果都不错。 RNN的提出主要是为了解决序列问题的长期依赖问题。 问题：梯度爆炸和消失原因：$$ h^{t} = tanh(W_x X + W_h h^{t-1}) $$$$ \frac{\partial h^t}{\partial h^{t-1}} = (1 - tanh^2)* W_h $$ 所以，根据BPTT，如果梯度从t层传到l层，需要累乘(l - t)次参数值。 如果W的特征值有大于1或者小于1的情况下，会发生梯度爆炸或者消失。 LSTMLSTM解决了RNN中常见的梯度消失问题 推导过程如下：$$ C^t = f_t C^{t - 1} + i_t \hat{C^t} $$所以，$$ \frac{\partial C^t}{\partial C^{t - 1}} = f_t + … $$…表示的值可以忽略不考虑。 $f_t$是模型自动控制的数值，所以，能够解决梯度消失的问题。]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BatchNormalization]]></title>
    <url>%2F2018%2F05%2F30%2FBatchNormalization%2F</url>
    <content type="text"><![CDATA[BatchNormalization是什么？BN的算法如上图所示，首先对某一层神经元的输出进行normalize，然后通过设置两个可以学习的参数进行scale and shift。强制进行归一化会导致模型表达能力的降低，使用scale and shift是为了保证BN能够有能力恢复成原有的输入形式，保证了模型的表达能力。 对于全连接的网络，BN是对每一个神经元的输出进行批标准化的。而对于CNN这种共享权值的特殊结构，BN是采用空间 BatchNormalization的方法，做法是对每个kernel的所有输出进行批标准化处理。 为什么要有BN？BN的提出是为了解决神经网络训练中会出现的internal covarite shift的问题。这个是指，在训练过程中，每一层神经元参数更新的假设都是基于其他层神经元参数没有变化的前提，然而事实上，这个假设是不成立的。批标准化通过强制的重参数化输出，解决了这个问题。 BN本质上是解决了梯度消失的问题参考链接：https://www.zhihu.com/question/38102762首先，解释下为什么会有梯度消失 BN的作用是对输出进行重参数化$$ h_l = BN(w_lh_{l-1}) = BN(\alpha w_lh_{l-1}) $$进行反向求导的时候可以得到： 在这里，我们可以将$BN(\alpha w_l h_{l-1})$简化成$\alpha w_l h_{l-1}$这个时候，从l层传到k层的梯度信息就可以写成：$$ \nabla_{h_k} l = \nabla_{h_l} l \prod_{i=k+1}^l{\alpha_i w_i} $$ 所以，通过控制alpah的大小，就可以解决梯度消失的问题。 BN的优势 解决了internal covariate shift问题，可以使用更大的学习率 减少了梯度消失的问题 对初始化要求降低了。参数提高了k倍，输出不变。]]></content>
      <tags>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter9-卷积网络]]></title>
    <url>%2F2018%2F05%2F29%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter9-%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[卷积网络动机（为什么）原理：自然信号具有层次化结构的属性，更高层次的属性时通过低层次属性组合获得的。如在图形中，边缘的局部属性组合图案，图案组合成目标。文本，语音都存在类似的层次结构。卷积网络的特点是能够捕获到输入数据的局部特征，然后通过多层结构将低层次特征组合成高层次特征。核心的思想是：局部连接，权值共享，池化操作，多层结构 局部连接局部连接也叫做稀疏连接，全连接网络中的输出单元会和每一个输入单元进行交互，带来的问题是参数的爆炸。CNN通过控制卷积核大小远小于输入大小，减少了大量参数。 权值共享CNN模型中，某一层的卷积核组会对所有的输入数据进行处理，在处理过程中，卷积核参数是不变的。目的也是为了减少参数量。 池化操作池化操作是将相邻位置语义相似的特征组合成一个特征，带来了输入表示的不变性。另外，也可以减少下一层的数据量，降低计算量。 多层连接多层连接是为了能够使得将下层提取到的低层次特征组合成高层次特征，特征层次越高，更容易使用简单分类器完成分类任务的特征。但是带来的问题有过拟合，梯度消失等问题。目前也有相应的解决方法。过拟合：dropout，BN，正则化等，梯度消失：BN，ReLU，残差连接结构等。 池化https://www.zhihu.com/question/23437871/answer/24696910 pooling的结果是使得特征减少，参数减少，但pooling的目的并不仅在于此。pooling目的是为了保持某种不变性（旋转、平移、伸缩等），常用的有mean-pooling，max-pooling和Stochastic-pooling三种。mean-pooling，即对邻域内特征点只求平均，max-pooling，即对邻域内特征点取最大。根据相关理论，特征提取的误差主要来自两个方面：（1）邻域大小受限造成的估计值方差增大；（2）卷积层参数误差造成估计均值的偏移。一般来说，mean-pooling能减小第一种误差，更多的保留图像的背景信息，max-pooling能减小第二种误差，更多的保留纹理信息。Stochastic-pooling则介于两者之间，通过对像素点按照数值大小赋予概率，再按照概率进行亚采样，在平均意义上，与mean-pooling近似，在局部意义上，则服从max-pooling的准则。 深度CNN为什么要深？自然信号中具有层次化结构的特点，CNN模型中高层能够将低层次的特征组合成高层次特征。高层次的特征能够使用简单的分类器就完成分类任务。所以，模型越深，提取到的特征层次越高，分类的效果就会越好。但是，也会带来过拟合的风险。 为什么可以做到深度？ 数据量多 GPU强大的并行计算能力 ReLU，BN，Dropout，残差连接等trick方法。]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep-learning-面试题]]></title>
    <url>%2F2018%2F05%2F24%2Fdeep-learning-%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[内容来自：https://github.com/elviswf/DeepLearningBookQA_cn1. 列举常见的一些范数及其应用场景，如 L0，L1，L2，L∞，Frobenius 范数范数是将向量映射到非负值的函数。定义如下： 平方范数$L^2$对每个元素的导数和整个向量相关，所以在原点附近增长的非常的缓慢，在机器学习应用中，区分0和非零的元素非常重要。L1范数可以保证各个位置的斜率相同，同时保持简单的数学形式。 最大范数： Frobenius范数表示矩阵的大小，类似于矩阵的L2范数。 L1范数&amp;L2范数L1正则化会产生稀疏解，L2正则化会使得权值趋向于0但是不等于0.原因：L1范数在任意位置的导数都是相同的，权值是朝着0的位置缩减，衰减步长不变，权值很容易就会衰减到0.L2范数在某个位置的导数和整个向量的位置有关，衰减的步长是和位置成正比的，所以越接近0的位置衰减越慢，权值会趋近于0但是不会等于0. 正则化的原理首先，模型出现过拟合原因是训练数据中有噪音，模型过度拟合这些噪音数据导致学习到的模型泛化能力差。正则化是通过对参数的大小进行惩罚，限制模型的参数空间，从而减少了模型的有效容量，抑制拟合噪音的能力，从而提高泛化能力。 简单介绍一下贝叶斯概率与频率派概率，以及在统计中对于真实参数的假设。频率学派，其特征是把需要推断的参数θ视作固定且未知的常数，而样本X是随机的，其着眼点在样本空间，有关的概率计算都是针对X的分布。另一派叫做贝叶斯学派，他们把参数θ视作随机变量，而样本X是固定的，其着眼点在参数空间，重视参数θ的分布，固定的操作模式是通过参数的先验分布结合样本信息得到参数的后验分布。 作者：秦松雄链接：https://www.zhihu.com/question/20587681/answer/23060072来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 概率密度的万能近似器高斯混合模型是概率密度的万能近似器，任何平滑的概率密度都可以用具有足够多组建的高斯混合模型以任意精度逼近。 简单介绍一下 sigmoid，relu，softplus，tanh，RBF 及其应用场景 sigmoid函数用阿里产生Bernoulili分布的参数，在变量取绝对值非常大的时候，会出现饱和情况，函数会变得很平。 softplus函数：$f(x) = log(1 + exp(x))$ Relu函数：计算及其简单，在非零位置求导很快。不需要复杂的运算。缓解了梯度消失的问题；缺点：如果初始化或者learning rate设置的不好，会导致神经元die的问题。 5.Jacobian，Hessian 矩阵及其在深度学习中的重要性答：p56-p62Jacobian是一阶的偏导。Hessian矩阵是二阶的偏导。二阶导数可以告诉我们一阶导数是怎么进行变化的。Hessian矩阵可以确定一个临界点是否是局部极大点，全局极小点或者是鞍点。正定是全局极小点，负定是局部极大点，有负有正是鞍点。 6.KL 散度在信息论中度量的是那个直观量答：p46KL散度用来衡量两个单独的概率分布的差异(信息熵的差值)和KL散度密切相关的是交叉熵。交叉熵是计算在样本的真实分布的前提下，预测样本分布的信息熵。等价于KL 散度，因为真实样本分布是确定的。 7. 数值计算中的计算上溢与下溢问题，如 softmax 中的处理方式答：p52-p53上溢出是计算的数值过大。下溢出是接近于0的时候，因为四舍五入为0，一般会设置一个eps，保证不为0. 减去最大值。softmax(x) = (exp(x - max(x)))/ sum(exp(x_i - max(x))) 8. 与矩阵的特征值相关联的条件数 (病态条件) 指什么，与梯度爆炸与梯度弥散的关系答：p53;输入被轻微扰动，输出就发生了很大变化的情况。 9. 在基于梯度的优化问题中，如何判断一个梯度为 0 的零界点为局部极大值／全局极小值还是鞍点，Hessian 矩阵的条件数与梯度下降法的关系答：p56-p62Hessian矩阵在局部极小点只有正特征值，在鞍点，同时具有正负特征值。 10.KTT 方法与约束优化问题，活跃约束的定义答：p60-p61 11. 模型容量，表示容量，有效容量，最优容量概念答：p70;p71;p72 12. 正则化中的权重衰减与加入先验知识在某些条件下的等价性答：p73 13. 高斯分布的广泛应用的缘由答：p40 14. 最大似然估计中最小化 KL 散度与最小化分布之间的交叉熵的关系答：p84 15. 在线性回归问题，具有高斯先验权重的 MAP 贝叶斯推断与权重衰减的关系，与正则化的关系答: p87 16. 稀疏表示，低维表示，独立表示答：p92 17. 列举一些无法基于地图 (梯度？) 的优化来最小化的代价函数及其具有的特点答：p97 维度灾难 18. 在深度神经网络中，引入了隐藏层，放弃了训练问题的凸性，其意义何在答：p119-122 19. 函数在某个区间的饱和与平滑性对基于梯度的学习的影响答：p98 20. 梯度爆炸的一些解决办法答：p185 21.MLP 的万能近似性质答：p123 22. 在前馈网络中，深度与宽度的关系及表示能力的差异答：p125 23. 为什么交叉熵损失可以提高具有 sigmoid 和 softmax 输出的模型的性能，而使用均方误差损失则会存在很多问题。分段线性隐藏层代替 sigmoid 的利弊答：p140 24. 表示学习的发展的初衷？并介绍其典型例子: 自编码器答：p3 25. 在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚答：p142 26. 在深度学习神经网络中，所有的层中考虑使用相同的权重衰减的利弊答：p142 27. 正则化过程中，权重衰减与 Hessian 矩阵中特征值的一些关系，以及与梯度弥散，梯度爆炸的关系答：p142-144 28.L1／L2 正则化与高斯先验／对数先验的 MAP 贝叶斯推断的关系答：p144 29. 什么是欠约束，为什么大多数的正则化可以使欠约束下的欠定问题在迭代过程中收敛答：p147 页底 Chapter 7.3 30. 为什么考虑在模型训练时对输入 (隐藏单元／权重) 添加方差较小的噪声，与正则化的关系答：p149-p150 Chapter 7.5-7.6 31. 共享参数的概念及在深度学习中的广泛影响答：多任务学习 p151;p156 Chapter 7.7; 7.9 32. Dropout 与 Bagging 集成方法的关系，以及 Dropout 带来的意义与其强大的原因答：p159-p165 Chapter 7.12 33. 批量梯度下降法更新过程中，批量的大小与各种更新的稳定性关系答：p170 Chapter 8.1.3 34. 如何避免深度学习中的病态，鞍点，梯度爆炸，梯度弥散答：p173-p178 Chapter 8.2.1 35.SGD 以及学习率的选择方法，带动量的 SGD 对于 Hessian 矩阵病态条件及随机梯度方差的影响答：p180；p181-p184 Chapter 8.3; 36. 初始化权重过程中，权重大小在各种网络结构中的影响，以及一些初始化的方法；偏置的初始化答：初始化权重：p184； Chapter 8.4偏置初始化：p186页底 Chapter 8.4 37. 自适应学习率算法: AdaGrad，RMSProp，Adam 等算法的做法答：AdaGrad:p187;RMSProp:p188;Adam:p189 Chapter 8.5.1-3 38. 二阶近似方法: 牛顿法，共轭梯度，BFGS 等的做法答：牛顿法：p190 Chapter 8.6.1;共轭梯度: p191-p193; Chapter 8.6.2BFGS:p193-p194 Chapter 8.6.3 39.Hessian 的标准化对于高阶优化算法的意义答：p195 Chapter 8.7.1 40. 卷积网络中的平移等变性的原因，常见的一些卷积形式答：平移等变性：p205页底； Chapter 9.3常见的一些卷积形式：p211-p218 Chapter 9.5 41.pooling 的做法的意义答：p207; p210 Chapter 9.3-4 42. 循环神经网络常见的一些依赖循环关系，常见的一些输入输出，以及对应的应用场景答：p230-p238 Chapter 10.2 43. seq2seq，gru，lstm 等相关的原理答：seq2seq:p240-p241; Chapter 10.4gru:p250; Chapter 10.10.2lstm:p248 Chapter 10.10.1 44. 采样在深度学习中的意义答：p286 第一段 Chapter 12.4.3 45. 自编码器与线性因子模型，PCA，ICA 等的关系答：线性因子模型可以扩展到自编码器和深度概率模型: p304-p305; Chapter 13.5PCA:p298; Chapter 13.1ICA:p298 Chapter 13.2 46. 自编码器在深度学习中的意义，以及一些常见的变形与应用答：意义: p306 Chapter 14.1常见变形: p306-p313 Chapter 14.5应用: p319 Chapter 14.9 47. 受限玻尔兹曼机广泛应用的原因答：p400: 想特别了解的人注意这句话： See Mohamed et al. (2012b) for an analysis of reasons for the success of these models. Chapter 20.2 48. 稳定分布与马尔可夫链答：p362 Chapter 17.3 49.Gibbs 采样的原理答：p365 Chapter 17.4 50. 配分函数通常难以计算的解决方案答：p368 Chapter 17.5.2“遇到难以处理的无向图模型中的配分函数时， 蒙特卡洛方法仍是最主要工具” 51. 几种参数估计的联系与区别: MLE／MAP／贝叶斯答：P82/85/87 Chapter 5.5 52. 半监督的思想以及在深度学习中的应用答：p329-p332 Chapter 15.3 53. 举例 CNN 中的 channel 在不同数据源中的含义答：p219-220 Chapter 9.7 54. 深度学习在 NLP，语音，图像等领域的应用及常用的一些模型答：p272-p293 Chapter 12.1-5 55.word2vec 与 glove 的比较答：How is GloVe different from word2vec?； GloVe 以及 Word2vec 能称为 deep learning 么？这俩模型的层次其实很浅的； http://t.cn/RvYslDf 这个问题没找到答案，我去找了 quora 和知乎上的相关问题以及 quora 一个回答提及的论文。 （若有人在书中找到，请批评指正） 56. 注意力机制在深度学习的某些场景中为何会被大量使用，其几种不同的情形答：p288 Chapter 12.4.5.1 57.wide&amp;deep 模型中的 wide 和 deep 介绍答：https://arxiv.org/pdf/1606.07792.pdf#### 此问题答案未在书中找到，为此我去找了原论文，论文图 1 有详细的介绍。 （若有人在书中找到，请批评指正） 58. 核回归与 RBF 网络的关系答：p89 Chapter 5.7.2 59.LSTM 结构推导，为什么比 RNN 好？答：p248 Chapter 10.10 60. 过拟合在深度学习中的常见的一些解决方案或结构设计答：p143-159； Chapter 7.1-12包括：Parameter Norm Penalties(参数范数惩罚); Dataset Augmentation (数据集增强); Early Stopping(提前终止); Parameter Tying and Parameter Sharing (参数绑定与参数共享); Bagging and Other Ensemble Methods(Bagging 和其他集成方法)；Dropout. 另外还有 Batch Normalization。 61. 怎么理解贝叶斯模型的有效参数数据会根据数据集的规模自动调整答：关于非参数模型：p72 ； Chapter 5.2非参数模型不依赖于特定的概率模型，它的参数是无穷维的，数据集的规模的大小影响着模型使用更多或者更少的参数来对其进行建模。(并未在书中找到准确的答案，若有更好的回答，请联系我改正) 本答案是根据问题在Deep Learning上找到的答案；有些答案只是自己读书后在书上做的笔记的具体页面，毕竟原 po（http://t.cn/RObdPGk） 说还有另外一本书，所以该答案可能不是特别准确也不完善，答案也是给大家做个参考，若发现答案有问题，请联系我并指正，大家共同进步，谢谢！]]></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[focal loss]]></title>
    <url>%2F2018%2F05%2F24%2Ffocal-loss-for-dense-object-detection%2F</url>
    <content type="text"><![CDATA[Focal lossFocal loss的提出主要是为了解决在物品检测中，前景物品和背景类别的极度不平衡问题。 首先，介绍下交叉熵： 我们定义$p_t$为：这样，交叉熵可以写成：$$ CE(p, y) = CE(p_t) = -log(p_t) $$ 从上图汇总可以看出，对于容易分类的情况（p &gt;&gt; 0.5)，也会产生一个不小的loss，如果大量累加这种easy example，这些small loss将会碾压少见的类。1234567891011class CrossEntropy(nn.Module): def __init__(self, eps=1e-6): self.eps = eps def forward(self, preds, targets): # preds: b x num_classes # labels: b x 1 y = one_hot(targets, preds.size(-1)) preds = preds.clamp(self.eps, 1.0 - self.eps) loss = -1 * targets * torch.log(preds) return loss Balance CrossEntroy对于类别不均衡的情况，可以引入一个权值因子$\alpha$，用来调节学习的情况。可以对类别少的设置大的错误惩罚权值，这样系统更不容易将类别少的对象识别错。 12345678910111213class BalancedCrossEntropy(nn.Module): def __init__(self, alphas, eps=1e-6): # alphas: num_classes, super(BalancedCrossEntropy, self).__init__() self.eps = eps self.alpahs = torch.LongTensor(alphas) def forward(self, preds, targets): y = one_hot(targets): weights = self.alpahs[targets] preds = preds.clamp(self.eps, 1.0 - self.eps) loss = -1 * targets * weights * torch.log(preds) return loss.sum() Focal Loss Deﬁnition虽然使用$\alpha$平衡了postive/negative类别不均衡的问题，但是并没有区分出easy/hard的问题，我们通过重塑损失函数以减少简单例子的loss比例，从而让学习更集中在hard的负面例子上。 在实际中，我们使用alpha-Balance的FL变种 代码如下：12345678910111213141516class FocalLoss(nn.Module): def __init__(self, gamma=0, eps=1e-7): super(FocalLoss, self).__init__() self.gamma = gamma self.eps = eps def forward(self, input, target): y = one_hot(target, input.size(-1)) logit = F.softmax(input, dim=-1) logit = logit.clamp(self.eps, 1. - self.eps) loss = -1 * y * torch.log(logit) # cross entropy loss = loss * (1 - logit) ** self.gamma # focal loss return loss.sum()]]></content>
      <tags>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件随机场介绍-转载]]></title>
    <url>%2F2018%2F05%2F23%2F%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E4%BB%8B%E7%BB%8D-%E8%BD%AC%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[转载自苏剑林的文章https://kexue.fm/archives/5542 对比下普通逐镇softmax和CRF的异同逐帧softmaxCRF主要是用于序列标注问题，可以简单理解为给序列的每一帧都进行分类。可以很自然的想到用softmax做多分类。 但是这种方法没有考虑到输出上下文的关联。 条件随机场然而，当我们设计标签时，比如用 s、b、m、e 的 4 个标签来做字标注法的分词，目标输出序列本身会带有一些上下文关联，比如 s 后面就不能接 m 和 e，等等。逐标签 softmax 并没有考虑这种输出层面的上下文关联，所以它意味着把这些关联放到了编码层面，希望模型能自己学到这些内容，但有时候会“强模型所难”。 而 CRF 则更直接一点，它将输出层面的关联分离了出来，这使得模型在学习上更为“从容”： 数学上的理解当然，如果仅仅是引入输出的关联，还不仅仅是 CRF 的全部，CRF 的真正精巧的地方，是它以路径为单位，考虑的是路径的概率。 模型概要假设输入有n帧，每一帧的标签有k种可能，那么理论上就会有$k^n$种可能。可以讲这种可能性用网络图进行可视化。在下图中，每个点代表一个标签的可能性，点之间的连线表示标签之间的关联，每一种标注结果，都对应着图中的一条完整路径。** 在序列标注任务重，我们的正确答案是唯一的。比如“今天天气不错”，如果对应的分词结果是“今天/天气/不/错”，那么目标输出序列就是 bebess，除此之外别的路径都不符合要求。 换言之，在序列标注任务中，我们的研究的基本单位应该是路径，我们要做的事情，是从 k^n 条路径选出正确的一条，那就意味着，如果将它视为一个分类问题，那么将是 k^n 类中选一类的分类问题。 这就是逐帧 softmax 和 CRF 的根本不同了：前者将序列标注看成是 n 个 k 分类问题，后者将序列标注看成是 1 个 k^n 分类问题。 具体来讲，在 CRF 的序列标注问题中，我们要计算的是条件概率： 为了得到这个概率，CRF包含了两个假设： 假设一：该分布式指数族分布 这个假设意味着存在函数$f(y_1, …, y_n; x)$使得其中Z(x)是归一化因子，因为这个是条件分布，所以，归一化因子和x有关。这个f函数可以看作是一个打分函数，打分函数取指数并且归一化后得到概率分布。 假设二：输出之间的关联发生在相邻的位置，并且关联是指数加性的， 这个假设意味着函数$f(y_1, …, y_n; x)$可以 进一步简化为： 这时候，可以看到，我们只需要对每一个标签和相邻标签进行打分即可，然后将所有的打分求和得到总分。打分函数分为两种，一种是转移特征函数g，一种是状态特征函数h。 线性CRF虽然已经进行了很多的简化，但是3式表示的模型还是太复杂，难以求解。于是考虑到当前深度学习模型中，RNN 或者层叠 CNN 等模型已经能够比较充分捕捉各个 y 与输出 x 的联系，因此，我们不妨考虑函数 g 跟 x 无关，那么：这个时候，g实际上是一个有限的，待训练的参数矩阵而已，而单标签的打分函数h，我们可以通过RNN或者CNN建模。 模型的概率分布变成： 归一化因子为了训练CRF模型，我们用最大似然方法，也就是用 作为损失函数，可以算出它等于 其中第一项是原来概率式的分子的对数，它目标的序列的打分，虽然它看上去挺迂回的，但是并不难计算。真正的难度在于分母的对数logZ(x)这一项。 归一化因子，在物理上也叫配分函数，在这里它需要我们对所有可能的路径的打分进行指数求和，而我们前面已经说到，这样的路径数是指数量级的（$k^n$），因此直接来算几乎是不可能的。 事实上，归一化因子难算，几乎是所有概率图模型的公共难题。幸运的是，在CRF模型中，由于我们只考虑了临近标签的联系（马尔可夫假设），因此我们可以递归地算出归一化因子，这使得原来是指数级的计算量降低为线性级别。具体来说，我们将计算到时刻t的归一化因子记为Zt，并将它分为k个部分 其中Z(1)t,…,Z(k)t分别是截止到当前时刻t中、以标签1,…,k为终点的所有路径的得分指数和。那么，我们可以递归地计算 它可以简单写为矩阵形式 其中$Z_t=[Z^{(1)}_t,…,Z^{(k)}_t]$；而G是对$g(y_i,y_j)$各个元素取指数后的矩阵，即$G=e^{g(y_i,y_j)}$；而$H(y_{t+1}|x)$是编码模型$h(y_{t+1}|x)$（RNN、CNN等）对位置t+1的各个标签的打分的指数，即$H(y_{t+1}|x)$=$e^h(y_{t+1}|x)$，也是一个向量。式(10)中，$Z_tG$这一步是矩阵乘法，得到一个向量，而⊗是两个向量的逐位对应相乘。 归一化因子的递归计算图示。从t到t+1时刻的计算，包括转移概率和j+1节点本身的概率归一化因子的递归计算图示。从t到t+1时刻的计算，包括转移概率和j+1节点本身的概率 如果不熟悉的读者，可能一下子比较难接受(10)式。读者可以把n=1,n=2,n=3时的归一化因子写出来，试着找它们的递归关系，慢慢地就可以理解(10)式了。 归一化因子的递归计算图示。从t到t+1时刻的计算，包括转移概率和j+1节点本身的概率 动态规划那么剩下的最后一步，就是模型训练完成后，如何根据输入找出最优路径来。跟前面一样，这也是一个从$k^n$条路径中选最优的问题，而同样地，因为马尔可夫假设的存在，它可以转化为一个动态规划问题，用viterbi算法解决，计算量正比于n。 实现https://github.com/bojone/crf/]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[alc2018-fine-tuned-language-models-for-text-classification]]></title>
    <url>%2F2018%2F05%2F17%2Falc2018-fine-tuned-language-models-for-text-classification%2F</url>
    <content type="text"><![CDATA[Fine-tuned Language Models for Text Classiﬁcation摘要迁移学习已经对CV领域革新了，但是现有的NLP方法仍然需要特定的任务修改和从头开始训练。论文提出一种Fine-tuned Language Models（FitLaM）,能够有效的对NLP任何任务进行迁移学习，并且介绍了使用微调技术得到的state-of-the-art的结果 介绍主要是介绍下CV领域的迁移学习方法一个成功的NLP迁移学习应该和CV一样有其对应的标准类似 该方法可以利用大量的可用数据 利用进一步的优化任务，导致进一步提高下游的任务 可以依赖一个可以用作多数NLP任务的模型 在实践中很容易使用 FitLaM依赖于一个简单的RNN网络，无需任何的修改，我们只用一个或者多个特定的任务的线性层增强模型，对现有的方法来说，只涉及s少数的参数更新。 同时，文章还提出一种新的微调技术，可以进行有差别的调整参数，低层次的层调节的幅度要小于高层次的层，已保留语言模型获得的知识。 论文贡献点 提出一中NLP的有效迁移学习方法 提出FitLaM，这是一种可以实现任意NLP任务的类CV迁移学习方法 提出Discriminative Fine-Tuing方法，保留以前的知识，防止在微调期间发生灾难性遗忘 引入了文本分类中的BPTT，一种通过线性层将分类器损失反向传播到任何序列大小的RNN的输出 对微调训练LM介绍了一些关键的技术 模型超越了五个公开数据集，多数数据集误差减少了18%-24% 提供预训练模型和代码 相关工作CV领域的迁移学习Hypercolumns在NLP中，最近有人提出一种没有使用word embedding进行迁移学习的方法。方法是将pretrain embeedding通过其他的任务捕获到上下文的context信息，然后将这种’embedding‘和word embedding以及中间层的输入连接到一起。 Multi-task learningMTL是将多个任务联合进行学习。 Fine-Tuning对预训练的模型进行微调 Fine-tuned Language Models (FitLaM)这个模型在一个大的通用领域语料上预训练高度优化的语言模型（LM),并将其调整到目标任务上。 LM试图通过前面的单词来预测下一个单词的概率。模型依赖于使用不同的方法（从n-gram到RNN）的语料库联合概率的自回归因式分解，这些方法在基准测试中都达到了state-of-the-art的结果。在实验中，使用最先进的语言模型（AWD-LSTM），这是一个具有很强正则化策略的正则LSTM。类似于CV，我们将会使用更高性能的语言模型改善下游任务。 LitLaM包含以下步骤： 通用领域的LM预训练 对目标任务LM微调 目标任务分类器微调 通用领域的LM预训练在Wikitext-103，共有28595篇预处理好的Wikipedia文章。通过这些数据集预训练LM模型，然后将模型的参数保存下来，作为下游任务的预训练模型。 Target task LM fine-Tuning使用之前的单词，预测下一个单词。 目标任务的数据可能来自于不同的分布，所以，需要对目标任务的LM进行优化。在给定的通用LM模型上进行训练，这个阶段的收敛速度要快很多，因为它只需要适应目标数据的优化，并且允许使用小的数据集也能够训练一个强大的LM。 Gradual unfreezing逐渐解冻方法，实验表明从最后一层开始逐渐解冻的模型最有用，这是因为这层包含最少的一般知识。首先，解冻最后一层，然后微调所有的未解冻层。然后，解冻下一层，重复这个过程。知道所有的层都进行了精细的调整。 注意，我们进行训练的时候，是每次都会解冻一层，然后训练所有解冻的层，而不是一次只训练一层。 Fine-tuning with cosine annealing 使用余弦退火的方法进行微调这种方法经过试验效果最好。我们只训练一个epoch，并且按照下面的schedule在每个batch进行降低学习率的 在这里$n_t$表示step t对应batch的学习率。 Warm-up reverse annealing实验发现，在训练所有层的时候增加学习率很有用。 Target task classiﬁer ﬁne-tuning对于分类任务进行fine tuning对于分类任务，我们采用对LM增加一层或者更多的线性block。 每个block都是使用BN，dropout，relu以及最后输出到一个softmax函数中。注意这些任务中，只有一层的参数需要重新开始训练。 Concat pooling文本分类的信号通常包含在几个字中，这可能出现在文本中的任意位置。由于输入文档中可能包含数百个单词，如果仅仅考虑最后的隐藏装填，信息可能丢失。所以，我们对于最后一步的hidden state和所有hidden states的max pooling和均值pooling的结果$$ H = {h_1, …, h_T} $$$$ h_c = [h_T, maxpool(H), meanpool(H)] $$ 微调分类任务是迁移学习最关键的部分。过度调节可能导致灾难性的遗忘，消除通过预训练得到的信息。过于谨慎有可能导致收敛缓慢以及过拟合。 Discriminative ﬁne-tuning由于不同层能够捕捉到不同类型的信息，他们应该进行不同程度的调整。 适当处理不同层的最简单方法是一次一层的训练模型，类似于贪心层次训练方法和“链解冻”。然后这引入了一个顺序的要求，阻碍了并行性。并且每次训练都会重新遍历一次数据集，很容易在小的数据集上过拟合。所以，提出使用discriminative fine-tuning 与其对所有的层使用相同的学习率，有区别的微调允许我们使用不同的学习率调节每个层，对于上下文，在时间步t的参数更新公式为： 对于discriminative fine-tuning,我们将参数按照层进行划分为${ \theta^1, …, \theta^L}$。相似的，也可以得到每一层的学习率 然后，在进行参数更新的时候，根据以下规则进行调整参数。 经验发现，首先选择第L层的学习率，然后根据$\eta^l = \eta^{l + 1} * 0.3$效果很好 BPTT for Text Classiﬁcation (BPT3C)语言模型通过反向传播进行更新参数，为了对大规模文档分类器进行精细调节，我们提出BPTT3C 首先将文档划分大小b的批次 在每个批次训练的开始，模型参数初始化为之前批次的最后的状态 跟踪最大池化和平均池化的隐藏状态 梯度被反向传播到哪些对预测有贡献的隐藏状态的参数 实际上，我们可以使用可变长的反向传播序列。 Bidirectional LMExperiments 没有找打相关的代码，不知道具体的实现是怎么样的。感觉这种方法还是主要使用与分类任务上。期待代码的公布和测试了。]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[simhash]]></title>
    <url>%2F2018%2F05%2F16%2Fsimhash%2F</url>
    <content type="text"><![CDATA[simhash主要是用于海量数据的去重方面。 和传统的hash的区别传统hash算法是将原始文本均匀随机映射到一个签名值，传统的hash算法产生两个hash值，对于内容相同的文本在一定概率是相同的；如果不相同，除了说明原始文本不相等之外，不再提供任何信息。即是相差一个字节的内容，产生的hash值也会完全不一样，所以，传统hash只能用来衡量文本是否相同，而不能做相似度的衡量。simhash就是一种局部敏感hash算法，他产生的hash签名在一定程度上可以表示原始内容的相似度。 我们主要解决的是文本相似度计算，要比较的是两个文章是否相识，当然我们降维生成了hash签名也是用于这个目的。看到这里估计大家就明白了，我们使用的simhash就算把文章中的字符串变成 01 串也还是可以用于计算相似度的，而传统的hash却不行。我们可以来做个测试，两个相差只有一个字符的文本串，“你妈妈喊你回家吃饭哦，回家罗回家罗” 和 “你妈妈叫你回家吃饭啦，回家罗回家罗”。 通过simhash计算结果为： 1000010010101101111111100000101011010001001111100001001011001011 1000010010101101011111100000101011010001001111100001101010001011 通过传统hash计算为： 0001000001100110100111011011110 1010010001111111110010110011101 大家可以看得出来，相似的文本只有部分 01 串变化了，而普通的hash却不能做到，这个就是局部敏感哈希的魅力。 simhash算法的思想对于海量的数据去重，需要对去重算法的效率有很高的要求，局部敏感hash算法可以讲原始文本映射为数字（hash签名），在内容相近的文本内容对应的hash签名也比较相近。可以完美解决这个问题。 流程 分词，去除stop-words（有时候会做词根stem处理） 对单词赋予权值（frequency，or tf-idf) 对每个单词计算一个b比特的唯一hash编码值 对每个单词的hash值，将0变成-1，然后乘以对应的word的权值 对所有的words，按照列进行相加，将和大于0的设置为1，否则设置为0 这样，我们就可以得到每个文本的simhash签名了。 计算相似度通过hamming distance进行计算。 参考链接： http://www.cnblogs.com/maybe2030/p/5203186.html]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter16-深度学习中的结构化概率模型]]></title>
    <url>%2F2018%2F05%2F15%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter16-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%84%E5%8C%96%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[结构化概率模型是使用图来描述概率分布中的随机变量之间的相互作用关系，从而描述一个概率分布。这些模型也叫做图模型。 非结构化建模的挑战在分类问题中，会丢弃掉输入数据中大多数的信息，然后产生单个的输出。比如在识别一张图片的时候，模型会丢弃掉背景图片的信息。 对多随机变量的分布进行建模，是一个非常具有挑战性的任务，假设我们对二值分布进行建模，对于32x32的图像，存在着$2^{3071}$种可能。 结构化图模型为随机变量之间的直接作用提供了一个正式的框架。这个方式大大减少了模型的参数，模型只需要少量的数据就可以进行有效估计了。 使用概率图模型描述模型结构图模型中，每个节点表示变量，每条边表示直接相互作用的关系。只有直接的相互作用才会被建模表示出来。 有向模型又称作，贝叶斯网络变量x通过有向无环图$G$和一些列的局部概率分布得到 通过这种表示可以大大减少存储的参数个数。 无向图又称为，马尔科夫随机场有向图模型适用于存在一个很明显的理由描述每一个箭头的时候。变量之间存在着因果关系的情况下。 然后，并不是所有的情况都是具有明确的方向关系。当相互左右的并没有本质的方向，使用无向图更适合。 举个例子，你是否生病，你同事是够生病，以及你的室友是够生病。这种相互传染的关系应为没有一个明确的方向，所以，我么会使用无向图去描述关系。 其中，团是指图中节点的一个子集，其中的每个节点都是全连接的。 配分函数我们需要对为归一化的概率函数进行归一化： 归一化的常数Z被称作是配分函数。 有向模型和无向模型之间的一个重要的区别就是有向模型是通过从起始点的概率分布直接定义的，反之无向模型的定义显得更加宽松，通过$\phi$函数转化为概率分布而定义 基于能量的模型无向图中的许多理论都是基于对于所有的x，p(x) &gt; 0。为了满足这个条件，最简单的是使用基于能量的模型。其中$$ \hat{p}(x) = exp(-E(x)) $$E(x)表示能量函数。 分离和d-分离图中隐含的条件独立性成为分离。]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[introduce-to-crf]]></title>
    <url>%2F2018%2F05%2F15%2Fintroduce-to-crf%2F</url>
    <content type="text"><![CDATA[An Introduction to Conditional Random Fields结构化预测方法本质上是将分类和图模型结合一起，结合了图模型对多元数据进行紧凑建模的能力，以及分类方法使用大量的输入特征的预测能力。 Introduction对于多变量预测的问题，一种方法是把每个位置的输出看成是独立的，但是问题是输出之间其实是有着复杂的依赖关系。例如，临近的文档和区域更倾向于有着相同的labels. 一种自然的方法用来表示输出变量相互依赖的方式是使用图模型。概率图模型，包括贝叶斯网络，神经网络，因子图，马尔科夫随机场等模型族在内的图形模型代表了许多变量的复杂分布。然后可以得到对于给定的概率密度因子分解，得到一组满足分布的特定的条件独立关系。 已经有很多使用概率图模型的工作，尤其是在统计自然语言处理上，一直都在聚焦于generative模型，能够明确的从输入中得到一个联合概率密度$p(y, x)$.生成模型也有限制，如果x的维度非常大，或者特征之间有着复杂的依赖关系，那么生成模型会很难得到联合概率密度。建模输入之间的依赖关系可能会导致难以处理的模型，但忽略它们会导致性能下降。 一种解决的方法是直接对条件概率$p(y|x)$建模，这是分类任务需要的。这就是条件随机场（CRF）。 CRFs本质上将分类和图模型的优点结合起来，将图模型的多元数据紧凑建模的能力和利用大量输入特征进行预测的能力结合起来。 条件模型（判别模型）的优点是仅仅设计x中变量的依赖关系在条件概率中补气作用，因此精确得到条件概率要比联合概率简单得多。因此，生成模型和CRFs的差异和朴素贝叶斯网络和逻辑回归之间的差异完全一样。实际上，多项的Logistics回归模型可以被看作是最简单的一种CRF，其中只有一个输出变量。 这个教程主要是使用CRF描述建模，推断和参数估计。首先是描述crf的建模问题，包括线性链的CRF，具有一般图结构的CRF以及包含潜在变量的隐藏CRF。我们描述CRF如何看做是逻辑回归过程的一般化，以及和马尔科夫模型的区别。 在第三章，描述推断，第四章描述学习。最后，讨论CRFs和其他的模型，包括其他结构预测方法，神经网络和最大熵马尔科夫模型。 Modeling从建模的角度上描述CRF，解释CRF如何将结构化输出表示为一个具有高维输入向量的函数。CRFs可以理解为逻辑回归分类器对任意图结构的拓展，也可以理解为结构化数据生成模型的判别模拟 图模型图模型在多变量概率分布的表示和推断上是一个强大的框架。多变量分布的表示的代价会非常大。比如一个n个二元变量的联合概率需要$O(2^n)$浮点数存储。图模型的解决方法是，在很多变量的表示可以看作是局部函数的乘积，每个局部函数都依赖于更小的变量子集。这种分解结果和变量之间的条件独立关系密切相关——这两种信息都可以很容易的被图表示出来。实际上，因式分解，条件独立性和图结构之间的这种关系包含了图模型框架的许多功能：条件独立观点对设计模型游泳，分解观点对设计推理算法有用。 无向图待续。。]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper-xgboost]]></title>
    <url>%2F2018%2F05%2F14%2Fpaper-xgboost%2F</url>
    <content type="text"><![CDATA[XGBoost: A Scalable Tree Boosting System论文提出的贡献点： 一个新的tree learning algorithm，可以有效处理sparse data 理论上合理的加权分位数程序可以在近似树学习中处理实例权重。 贡献点： 设计一个高可拓展性的end-to-end tree boosting系统 理论上合理的加权分位数程序可以在近似树学习中处理实例权重。 我们引入了一种新颖的并行树学习稀疏感知算法。 我们针对树学习提出了一种有效的缓存感知块结构 提出一个一个正则化的学习目标作为进一步的改进。 Tree boosting算法正则化的学习目标 上图显示了一个Ensemble算法的过程，可以概括为一个加法模型。和decision tree不同，每个回归树都会在叶子节点有一个连续的score值，我们只用$w_i$表示第i个节点的呃score值。 从而，就可以定义一个包含正则项的损失函数：在这里$l$表示一个自定义的凸loss function。加法的正则化项可以平滑最终学习到的权值，避免出现过拟合。 当去除了正则化项的时候，学习的目标就会变成传统的gradient tree boosting算法。 Gradient Tree Boosting]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter8-深度模型的优化]]></title>
    <url>%2F2018%2F05%2F13%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter8-%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[深度模型的优化优化的问题：寻找一组参数，能够显著的降低损失函数。损失函数通常包括整个数据集上的性能评估和额外的正则化 学习和纯粹的优化的不同深度学习的优化算法和和传统的优话算法有几个方面的不同： 在深度学习/机器学习的优化过程中，我们关注某些性能度量P,但是它可能是不可求解的，所以，我们是间接的优化P。我们希望通过降低代价函数 $J(\theta)$来提高P 而纯优化是最小化目标J本身。 经验风险最小化最小化平均训练误差的训练过程是经验风险最小化的过程。 代理损失函数我们真正关心的损失函数（比如分类误差）并不能高效的优化（因为不是凸函数）。例如，在二元分类中，我们真正关心的是0-1损失函数，$$ l(y,y^)=∑_{i=1}^mχ(yi≠y^i) $$但是，这个损失函数无法进行优化。 这种情况下，我们会选择使用代理损失函数，将原来的优化问题转化为一个近似的优化问题，而且近似的优化问题更容易求解。 提前终止 批量算法和小批量算法 假设样本之间独立，那么n个样本的均值的标准差是$\sigma / \sqrt{n}$，其中$\sigma$是样本值的真是方差，分母为$\sqrt{n}$表示更多的样本来估计梯度的方法的回报是小于线性的。 计算量的问题。 所以，我么会采用随机采样少量样本作为一个batch，然后计算平均值。 使用整个训练集的优化算法成为批量或者确定性 深度学习采用的是小批量或者小批量随机d的方法，通常简称为*随机（stochastic）方法 小批量带下通常由以下因素决定： 大批量会得到更加准确的梯度，但是回报是小于线性的 极小的批量无法充分利用多核的架构 批量大小和内存占用成正比 硬件对于特征的数（比如2的指数）的运行时间更小 小批量在学习过程中增加了噪音的影响，一定程度上起到了正则化的方法 不同的优化算法从不同的小批量中获取到不同的信息，一般而言，基于梯度的更新g相对鲁棒，在较小的批量就能成功，比如100. 基于Hessian矩阵的二阶更新方法则通常需要更大的批量。 小批量是随机抽取的这点很重要，因为样本的无偏估计要求样本之间是独立的。通常的做法是将训练样本shuffle。 神经网络中的挑战病态问题病态体现在随机梯度”卡“在某个位置，此时即使很小的更新步长也大大增加代价函数。 局部极小值对于非凸的问题，如深度学习，有可能存在多个局部极小值。 但是，大多数局部极小值都具有很小的代价函数，我们能不能找到全局最小点并不重要。 高原、鞍点和其他平坦区域对于高纬度非凸函数，局部极小值实际上都远小于另一种梯度为0的点：鞍点。 在鞍点出，Hessian矩阵同时有正负特征值。 Hessian在极小值处只有正特征值。 悬崖和梯度爆炸哦遇到梯度极大的悬崖结构，梯度更新很大将可能导致权值远离最优点 解决方法：梯度截断的方法。 长期依赖主要出现在序列模型中，由于变深的结构使得模型丧失了学习到先前信息的能力，让优化变得困难。 在多个事件步上讲输入重复和权值W相乘，将会导致结果丢弃了x中与权值W的主特征向量正交的成分。 基本算法随机梯度下降按照数据生成分布抽取m个独立同分布的赝本，九三他们的梯度均值，就可以得到梯度的无偏估计 在实践中，我们有必要随着时间推移逐渐降低学习率。 动量动量算法主要是用在加速训练，忒儿是处理高曲率，小但一致的梯度，或者是有噪音的梯度。 动量算法积累了之前的梯度指数级衰减的移动平均，并且在这个方向上继续移动。学习的效果图： 学习的算法是： Neserov动量对动量的方法进行了一次修正。在Nesterov动量中，梯度计算是在施加当前动量之后。可以解释为在标准动量方法中增加了一个矫正因子。算法如下： 其实，在随机梯度的情况下，nesterov并没没有改进收敛。 参数初始化策略初始化的策略会影响到深度学习模型的收敛情况。 初始化参数需要在不同单元间”破坏对称性“。如果两个在同一层的神经元的参数相同，那么学习算法将会使参数以相同的方向更新。 我们几乎总是初始化模型的权重为高斯或者均匀分布的值，二者的选择似乎不会有很大的区别。 更大的初始化权值具有更强的破坏对称性的作用，有助于避免冗余的单元。同时也有助于增强梯度传播的信号强度。但是，如果初始化权重太大，那么可能在前向传播或者反向传播中产生梯度爆炸的值。 同时，较大的权重也会容易使得激活函数饱和，导致饱和的单元的梯度完全消失。 如何继续初始化？参考链接：https://zhuanlan.zhihu.com/p/25931903 当我们对参数进行随机初始化时候，如果初始化的参数很小，得到的激活值会很快接近于0（(Tanh)，同时，梯度在进行反向传播的时候会累乘每一层的权值，权值过小，会导致梯度变得很小，参数无法更新。 此外，如果初始化的参数很大，则每一层的输出都非常大，经过激活函数之后的输出集中在-1和1上，此事，激活函数是出于饱和的情况，导数接近于0.在进行反向传播的时候，会导致梯度消失。 Xavier initialization 1w = Variable(np.random.randn(node_in, node_out)) / np.sqrt(node_in) 这种方法对于tanh激活函数有效，但是对于ReLU激活函数，还是会出现输出值趋近于0的情况。 He initializationHe initialization的思想是：在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2： 1w = Variable(np.random.randn(node_in, node_out) / np.sqrt(node_in / 2)) 推荐在ReLu网络中使用 BatchNormalization对输出进行重新的normlization。 自适应是学习算法自适应的学习算法是通过累积二阶动量的信息（梯度的平方和），然后将学习率处于二阶动量的根号。通过这种方法，可以让具有损失最大偏导的参数的学习率下降快，而具有小偏导的参数的学习率具有相对较小的下降 Adagrad 123# Assume the gradient dx and parameter vector xcache += dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) RMSProp在Adagrad的基础上，对二阶动量进行了指数加权衰减。使得在找到凸碗状结构后能够快速收敛12cache = decay_rate * cache + (1 - decay_rate) * dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) Adam增加了一阶的动量 123456# t is your iteration counter going from 1 to infinitym = beta1*m + (1-beta1)*dxmt = m / (1-beta1**t)v = beta2*v + (1-beta2)*(dx**2)vt = v / (1-beta2**t)x += - learning_rate * mt / (np.sqrt(vt) + eps) 二阶近似方法牛顿法牛顿法是基于二阶的泰勒级数展开得到的。其忽略了高阶的导数。然后以就可以得到牛顿参数的更新规则： 对于非二次的表面，只要Hessian矩阵保持正定，牛顿法就能够进行迭代使用。这意味着一个两步迭代的过程。 在深度学习中，目标函数表面通常具有非凸的特点，如鞍点，这个时候如果Hessian矩阵的特征值不全是正的，牛顿法实际上会朝着错误的方向进行更新。这种情况可以通过正则化Hessian避免。常用的正则化方法是在Hessian矩阵的对角线上增加常数$\alpha$ 这个时候，更新的公式就变成了： 优化技巧和元算法batch normalization他不是一个优化算法，而是一个自适应的重新参数化的方法，试图解决训练非常深的模型的困难。 在实践中，我们同时更新参数的所有层。某一层的参数的更新的假设是其他层的参数没有变化，而实际上，其他层的参数也是在发生变化的。这个就是Internal Covariate Shift” BatchNormalization提出一种几乎可以重新参数化所有深度网络的优雅方法，通过重新参数化显著减少了多层之间的协调更新问题。 为何可以解决梯度消失问题： 在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。 坐标下降在某些情况下，可以将一个优化的问题分解成几个部分，可以更快的解决这个问题。如果我们对于某个单一变量$x_i$最小化f(x),然后相对于另外一个变量$x_j$更新,反复循环所有变量，可以保证到达极小值。这种方法是坐标下降方法。 监督预训练在直接训练目标模型求解目标问题之前，训练简单的模型求解简化问题的方法统称为预训练。 贪心算法是将问题分解为许多部分，独立的在每个部分求解最优值。贪心算法虽然不能保证得到最优值，但是比求解最优联合解的算法高效的多。 贪心算法后面可以紧接着一个精调（find-tuning)阶段，搜索最优解。 贪心监督预训练方法每个阶段包括一个仅涉及最终神经网络的子集层的监督学习训练的任务。这种方法可以更好地指导深层次网络的中间层学习。]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning-nature]]></title>
    <url>%2F2018%2F05%2F13%2Fdeeplearning-nature%2F</url>
    <content type="text"><![CDATA[Deep learning(自然杂志文章)原文链接：https://www.nature.com/articles/nature14539 作者：Yann LeCun 1,2 , Yoshua Bengio 3 &amp; Geoffrey Hinton 深度学习是通过多层处理的计算模型，用来学习具有多个抽象级别的数据表达（分布式表达）。这些方法已经在图像，语音，物品检测和其他领域上运用。深度学习可以通过反向传播算法发现大型数据中的复杂结构，以指示机器如何改变内部参数，以用于根据上一层的表示计算当前层的表示。深度卷积网络在图像，视频，语音和音频领域取得了突破，而且，RNN网络已经显示了在序列型数据，如文本，闪耀着曙光。 正文内容机器学技术为现代社会提供了动力：从网络搜索到社交网络内容的过滤，在到电子商务网站的推荐，并越来越多地出现在相机和智能手机等消费类产品中。 机器学习系统用于识别图像中的对象，将语音转换为文本，将新闻项目，帖子或产品与用户兴趣相匹配，并选择相关的搜索结果。 这些应用程序越来越多地使用一类称为深度学习的技术。 传统的机器学习技术在处理原始形式的自然数据的能力方面受到限制。数十年来，构建模式识别或机器学习系统需要仔细的工程设计和相当的领域专业知识来设计一个特征提取器，将原始数据（如图像的像素值）转换为合适的内部表示或特征向量学习子系统（通常是分类器）可以检测或分类输入中的模式。 表示学习是一组方法，它允许机器获得原始数据并自动发现检测或分类所需的表示。深度学习方法是具有多层次表示的表示学习方法，通过组合简单但非线性的模块获得，每个模块都将较低级别的表示（从原始输入开始）转换为更高级别的表示抽象层次。有了足够的这种转换的组合，就可以学习非常复杂的功能。对于分类任务，更高层次的表示扩大了对歧视很重要并抑制不相关变化的输入方面。例如，图像以像素值阵列的形式出现，并且第一表示层中的学习特征通常代表图像中特定取向和位置处存在或不存在边缘。第二层通常通过识别特定的边缘布置来检测图案，而不管边缘位置的小变化。第三层可将图案组装成对应于熟悉对象的部分的较大组合，并且后续层将检测对象作为这些部分的组合。深度学习的关键方面是这些特征层不是由人类工程师设计的：他们是通过使用通用学习程序从数据中学习的。 深度学习在解决多年来困扰人工智能社区的问题方面取得了重大进展。事实证明，他们擅长发现高维数据中错综复杂的结构，因此可应用于科学，商业和政府等众多领域。除了在图像识别和语音识别中打破记录之外，它在预测潜在药物分子的活性，分析粒子加速器数据，重建脑电路以及预测非编码DNA突变对基因表达和疾病的影响。也许更令人惊讶的是，深度学习对自然语言理解1中的各种任务，特别是主题分类，情感分析，回答系统和语言翻译，产生了极其有希望的结果。 我们认为深度学习在不久的将来会有更多的成功，因为它只需要很少的手工设计，所以它可以很容易地利用可用计算和数据量的增加。目前深度学习领域不断出现的新的学习算法和架构将会加速这一进程。 监督学习机器学习最常见的形式，无论是否deep，都是监督学习。想象一下，我们想要建立一个可以将图像分类为包含房屋，汽车，人或宠物的系统。我们首先收集房屋，汽车，人和宠物图像的大型数据集，每个数据都贴上对应的标签。在训练过程中，机器会输入一张图像，并以分数向量形式输出，每个分类一个数值。我们希望所需类别具有所有类别中的最高分数，但这在训练之前不太可能发生。我们计算一个目标函数，用于度量输出分数与原始标签之间的误差（或距离）。机器然后修改其内部可调节参数以减少此错误。这些可调参数通常称为权值，它们是可以看作是定义机器输入输出功能的“旋钮”的实数。在一个典型的深度学习系统中，可能有数以百万计的这些可调权重，以及数亿个用于训练机器的有标签样本。 为了适当地调整权重向量，学习算法计算梯度向量，对于每个权重，梯度表示错误增加减少的最快方向。然后向与梯度向量相反的方向调整 在所有训练样本上平均的目标函数可以看作是权值的高维空间中的一种丘陵景观。负梯度矢量表示该景观中最陡下降的方向，使其接近最小值，其中输出误差平均较低。 实践中，大多数从业人员使用称为随机梯度下降（SGD）的方法。这包括显示几个样本（batch)的输入向量，计算输出和错误，计算这些示例的平均梯度，以及相应地调整权重。从训练集中重复许多批次的过程直到目标函数的平均值停止下降。它被称为随机因为每个小例子集都给出了所有例子中平均梯度的有噪音的估计。与更复杂的优化技术相比，这个简单的过程通常会出人意料地快速找到一组好的权重。在训练之后，系统的性能是通过称为测试集的一组不同的例子来测量的。这用于测试机器的泛化能力 - 它能够对训练期间从未见过的新输入产生合理的答案。 机器学习的许多当前实际应用都使用线性分类器，适用在手工设计的特征。二元线性分类器计算特征向量分量的加权和。如果加权总和高于阈值，则输入被分类为属于特定类别。 自20世纪60年代以来，我们已经知道，线性分类器只能将其输入空间划分为非常简单的区域，即由超平面分隔的半空间。但是，诸如图像和语音识别之类的问题需要输入 - 输出功能对输入的无关变化（诸如对象的位置，方向或照明的变化，或者语音的音高或口音的变化）不敏感，而非常对特定的微小变化敏感（例如，白狼和一种叫做萨摩耶的狼类白色狗的区别）。在像素级别，不同姿势和不同环境中的两个萨摩耶犬的图像可能彼此非常不同，而萨摩耶犬和狼在相同位置和相似背景下的两幅图像可能彼此非常相似。线性分类器或任何其他“浅”分类器在原始像素上运行时，无法区分后两者，而将前两者归入同一类别。这就是为什么浅的分类器需要一个好的特征提取器来解决选择性不变的困境 —— 一个产生对图像方面有选择性的，对辨别很重要的表示的方法，但它对不相关的方面是不变的，例如动物。为了使分类器更强大，可以像使用核方法解决非线性特征的问题，但是像高斯内核那样的通用特征不允许学习者远离训练例子。传统的选择是手工设计好的特征提取器，这需要大量的工程技术和领域专业知识。但如果使用通用学习算法可以自动学习好的特征，就可以避免这种情况。这是深度学习的关键优势。 深度学习体系结构是一个简单模块的多层栈，所有（或大部分）这些模块都需要学习，其中许多模块计算非线性输入输出映射。堆栈中的每个模块转换其输入以增加表示的选择性和不变性。对于多个非线性图层，例如5到20的深度，系统可以实现其输入的极其复杂的功能，这些功能同时对细微的细节（区分萨摩耶和白狼）敏感，并且对大量不相关的变化（如背景，姿势，照明和周围的物体不敏感。 图1： 多层神经网络和反向传播。 深度学习的许多应用都使用前馈神经网络体系结构（图1），该体系结构学习将固定大小的输入（例如图像）映射到固定大小的输出（例如，几个类别中的每一个的概率） 。为了从一层到下一层，一组单元计算来自前一层的输入的加权和，并通过非线性函数传递结果。目前，最流行的非线性函数是整流线性单元（ReLU），它只是半波整流器f（z）= max（z，0）。在过去的几十年中，神经网络使用更平滑的非线性，例如tanh或sigmoid，但ReLU通常在多层网络中学习速度更快，允许训练深度监督网络没有无监督的预训练。不在输入或输出层中的单位通常称为隐藏单位。隐藏层可以看作是以非线性方式扭曲输入，使得类别在最后一层可以线性分离（图1）。 在20世纪90年代后期，神经网络和反向传播很大程度上被机器学习社区所遗弃，并被计算机视觉和语音识别社群所忽视。人们普遍认为，学习有用的，多阶段的，特征提取器，但很少有先验知识是不可行的。特别是，人们普遍认为简单的梯度下降会困在局部最优解中，对于这种情况下，不会有小的变化会降低平均误差（梯度更新停止）。 实际上，局部最优解很少是大型网络的问题。无论初始条件如何，该系统几乎总能达到非常类似的好的解决方案。最近的理论和实证结果强烈表明局部最小值一般不是一个严重问题。相反，优化曲面上总是充满了大量的鞍点，其中梯度为零，并且曲面在大多数维度上向上弯曲，并在剩下的位置向下弯曲。分析似乎表明，在非常多的情况下，只有很少的向下弯曲方向的鞍点出现，但几乎所有鞍点位置的目标函数值都非常相似。因此，算法卡住哪个鞍点并不重要。 一些由加拿大高级研究所（CIFAR）召集的研究人员在2006年左右恢复了对深前馈网络的兴趣。研究人员介绍了无监督学习过程，可以创建特征检测器层而不需要标记数据。学习每层特征检测器的目的是为了能够重建或模拟下面图层中的特征检测器（或原始输入）的活动。通过使用这个重建目标对多个逐渐更复杂的特征检测器进行“预先训练”，深度网络的权重可以初始化为合理的值。然后可以将最后一层输出单元添加到网络顶部，并且可以使用标准反向传播对整个深层系统进行微调。这对识别手写数字或检测行人非常有效，特别是当标签数据量非常有限情况下。 这种预训练方法的第一个主要应用是语音识别，它可以通过快速图形处理单元（GPU）的出现而实现，这些单元便于编程并且允许研究人员以10或20倍的速度训练网络。在2009年，该方法被用来将从声波中提取的系数的短时间窗口映射到可能由窗口中心的帧表示的各种语音片段的一组概率。它在标准语音识别基准测试上取得了创纪录的成绩，该基准测试使用了一个小词汇表，并且很快开发出来，可以在大型词汇表上创造破记录的结果。到2012年，许多主要演讲组织正在开发2009年以来的deep net版本，并且已经部署在Android手机中。对于较小的数据集，无监督的预训练有助于防止过拟合，从而在标记示例的数量很少时导致显着更好的泛化，或者在传输设置中我们有许多“源”任务的示例，但很少一些“目标”任务。一旦深度学习得到修复，事实证明，预训练阶段只需要小数据集。 但是，有一种特殊类型的深层前馈网络比相邻层之间具有完全连接性的网络更容易训练和推广。这是卷积神经网络（ConvNet）。它在神经网络不受欢迎的时期取得了许多实际成功，并且最近已被计算机视觉社区广泛采用。 Convolution neural networksConvNets被设计为处理以多个阵列形式出现的数据，例如由三个包含三个颜色通道中的像素强度的二维阵列组成的彩色图像。许多数据模式都是以多个数组的形式存在的：1D用于信号和序列，包括语言; 2D用于图像或音频频谱图;和3D用于视频或体图像。 ConvNets背后有四个关键思想，它们充分利用自然信号的特性：局部连接，共享权重，池化操作和多层结构。 典型的ConvNet的架构（图2）被构建为一系列阶段。前几个阶段由两种类型的层组成：卷积层和池化层。卷积层中的单元输出被组织在特征映射中，其中每个单元通过一组称为滤波器组的权重连接到前一层的特征映射中的局部片。这个局部加权和的结果之后通过诸如ReLU的非线性函数。feature map中的所有单元共享同一个滤波器组。同一层中的不同特征映射使用不同的滤波器得到的。这种架构的原因是双重的。首先，在图像等阵列数据中，局部值组通常高度相关，形成易于检测的独特局部图案。其次，图像和其他信号的本地统计数据对位置不变。换句话说，如果一个图案可以出现在图像的一个部分，它可能出现在任何地方，因此不同位置的单元共享相同的权重，并在阵列的不同部分可以检测到相同的图案。在数学上，由特征映射执行的过滤操作是离散卷积，因此是名称。 尽管卷积层的作用是检测来自前一层的特征的局部连接，但是池化层的作用是将语义相似的特征合并成一个特征。因为形成图案的特征的相对位置可能有所不同，所以可通过检测图案粗纹理化的每个特征的位置来完成。典型的池化单元计算一个特征地图（或几个特征地图）中局部区域单元的最大值。相邻的池化单元从多于一行或一列移动的块中获取输入，从而减少该表示的维度并创建小变化和扭曲的不变性。卷积，非线性和池化的两个或三个阶段被堆叠，然后是更多的卷积和完全连接的层。通过ConvNet反向传播梯度与通过常规深层网络一样简单，从而允许对所有滤波器组中的所有权重进行训练。 深层神经网络利用了许多自然信号是层次化结构的属性，其中更高层次的功能是通过组合较低层次的功能获得的。在图像中，边缘的局部组合形成图案，图案组装成部分，并且部分形成目标。从声音，音素，音节，单词和句子的语音和文本中存在类似的层次结构。当前一层中的元素在位置和外观上有所不同时，池化操作可以使得到的表示变化很小。 ConvNets中的卷积和池化层直接受到视觉神经科学中简单细胞和复杂细胞的经典概念的启发[43]，整体结构让人联想到视觉皮质腹侧通路中的LGN-V1-V2-V4-IT层次 。当ConvNet模型和猴子显示相同的图像时，ConvNet中高级单元的激活解释了猴子颞颞叶皮质中随机160个神经元的一半方差。 ConvNets的根源在于可识别，其架构有点类似，但没有端到端的监督学习算法，如反向传播。被称为时间延迟神经网络的原始1D ConvNet被用于识别音素和简单单词。 回溯到20世纪90年代早期的卷积网络有许多应用，从用于语音识别文档阅读的时间延迟神经网络开始。文件阅读系统使用ConvNet与实施语言约束的概率模型共同训练。到20世纪90年代后期，这个系统读取了美国所有支票的10％以上。 Microsoft 后来部署了一些基于ConvNet的光学字符识别和手写识别系统。 ConvNets在20世纪90年代早期还对自然图像中的物体检测进行了试验，包括脸部和手部以及脸部识别。 Image understanding with deep convolutional networks自21世纪初以来，ConvNet已经在图像中的物体和区域的检测，分割和识别方面取得了巨大的成功。这些都是标记数据相对充足的任务，例如交通标志识别，尤其是连接组学的生物图像的分割，以及自然图像中人脸，文本，行人和人体的检测。 ConvNets最近取得的一项重大成就是人脸识别。 重要的是，图像可以在像素级进行标记，这将在技术上得到应用，包括自主移动机器人和自动驾驶汽车。 Mobileye和NVIDIA等公司正在即将推出的汽车视觉系统中使用这种基于ConvNet的方法。其他重要的应用涉及自然语言理解和语音识别。 尽管取得了这些成功，但ConvNets在2012年ImageNet竞赛之前一直被主流计算机视觉和机器学习社区所抛弃。当深度卷积网络应用于包含1,000个不同类别，大约100万张图像的数据集时，他们取得了令人瞩目的成果，几乎将之前最好的方法的误差率减半。这种成功来源于GPU的高效使用，ReLU，一种新正则化技术Dropout，以及通过变形现有技术来生成更多训练样例的技术。这一成功带来了计算机视觉方面的革命; ConvNets现在是几乎所有识别和检测任务的主要方法，并且在某些任务上接近人的表现。最近的一个惊人的演示结合了ConvNet和经常性网络模块来生成图像标题（图3）。 最近的ConvNet体系结构具有10到20层，数以亿计的权重和单单元之间数十亿的连接。 尽管训练这种大型网络可能需要两周时间，但硬件，软件和算法并行化的进展已将训练时间缩短到几个小时。 基于ConvNet的视觉系统的性能已经导致包括Google，Facebook，微软，IBM，雅虎，Twitter和Adobe在内的大多数主要科技公司，以及数量迅速增加的初创公司启动研发项目，并且部署基于ConvNet的图像理解产品和服务。 ConvNets很容易在芯片或现场可编程门阵列66,67中实现高效的硬件实现。 许多公司如NVIDIA，Mobileye，Intel，Qualcomm和三星正在开发ConvNet芯片，以实现智能手机，相机，机器人和自动驾驶汽车的实时视觉应用。 Distributed representations and language processing（分布式表示和预言过程）深度学习理论表明，与不使用分布式表示的传统机器学习的算法相比，深度网络具有两种不同的指数级优势。 通过分布式表示，可以对训练过程学习特征进行新的组合。（例如，n个二元特征可能有2 n个组合） 在深度学习中的层次化表示 多层神经网络的隐藏层学会以一种方式来表示网络的输入，以便于预测目标输出。通过训练一个多层神经网络来预测下一个词的顺序，从早期单词的本地语境中可以很好地证明这一点。上下文中的每个单词作为one hot 向量传递给给网络，即一个位置的值为1，其余为0.在第一层中，每个单词创建不同的激活模式，或者单词向量（图4）。在语言模型中，网络的其他层学习将输入的单词向量转换为预测的下一个单词的输出单词向量，这可以用来预测词汇表中任何单词出现为下一个单词的概率。网络学习包含许多激活部分的单词向量，每个单词部分都可以被解释为单词的区分特征，正如在学习符号的分布式表示的上下文中首先证明的那样。这些语义特征在输入中没有明确表示。学习过程发现它们是将输入和输出符号之间的结构化关系分解为多个“微观规则”的好方法。当单词序列来自大量真实文本并且单个微观规则不可靠时，学习单词向量也表现得很好。例如，在训练预测新闻报道中的下一个单词时，周二和周三的学习单词向量与瑞典和挪威的单词向量非常相似。这种表示称为分布式表示，因为它们的元素（特征）不是相互排斥的，它们的许多配置对应于观察数据中看到的变化。这些词向量可学习到的特征，这些恩正不是由专家提前确定的，而是由神经网络自动发现的。从文本中学习的单词的向量表示现在在自然语言应用中被广泛使用。 representation 表示问题是逻辑启发式和神经网络启发式认知范式之间争论的核心。在逻辑启发范例中，符号的一个实例是唯一的属性，它与其他符号实例相同或不相同。它没有与其使用相关的内部结构;并用符号来推理，它们必须在明确选择的推理规则中与变量绑定。相比之下，神经网络只是使用大的激活向量，大权重矩阵和非线性函数来执行快速“直观”推断，这是支持毫不费力的常识推理的基础。 在引入神经语言模型之前，语言统计建模的标准方法没有利用分布式表示：它基于对长度最大为N（称为N-gram）的短符号序列的出现频率进行计数。可能的N-gram的数量约为$V^N$，其中V是词汇量大小，因此考虑到少数词汇的背景需要非常大的训练语料库。 N-gram将每个单词视为一个原子单位，因此它们不能在语义上相关的单词序列间进行概括，而神经语言模型可以因为它们将每个单词与实值特征向量相关联，并且语义相关单词彼此在向量空间中靠近（图4）。 Recurrent neural networks 第一次引入反向传播时，其最令人兴奋的用途是训练递归神经网络（RNN）。对于涉及顺序输入的任务，如语音和语言，通常使用RNN更好（图5）。 RNN一次处理一个输入序列的一个元素，在它们的隐藏单元中保持一个“状态向量”，隐含地包含关于该序列的所有过去元素的历史的信息。当我们考虑不同离散时间步的隐藏单元的输出，就好像它们是深层多层网络中不同神经元的输出一样（图5，右），我们很清楚如何应用反向传播来训练RNN。 RNN是非常强大的动态系统，但是训练它们已被证明是有问题的，因为反向传播的梯度在每个时间步长都会增长或缩小，所以在很多时间步骤中它们通常会爆炸或消失。 由于他们的体系结构以及训练方式的进步，已发现RNN非常善于预测文本中的下一个字符或序列75中的下一个字词，但它们也可以用于为更复杂的任务。例如，一次读一个英文句子一个单词后，可以训练一个英文“编码器”网络，以便其隐藏单元的最终状态向量能很好地表示该句子分布式表达的thought向量。这个thought向量然后可以用作共同训练的法语“解码器”网络的初始隐藏状态（或作为额外输入），其输出法语翻译的第一个词的概率分布。如果从这个分布中选择一个特定的第一个单词并作为输入提供给解码器网络，那么它将输出翻译的第二个单词的概率分布，依此类推，直到选择完全停止。总的来说，这个过程根据依赖于英语句子的概率分布生成法语单词序列。这种看起来相当幼稚的机器翻译的方式很快就成为state-of-the-art的方法，并且这对于理解一个句子是否需要像使用推理规则操纵的内部符号表达式这样的任西引起了严重的怀疑。它与日常推理涉及许多同时进行类比的观点更相容，每种类比都有助于得出结论的合理性。 可以学会将图像的含义“翻译”为英语句子（图3），而不是将法语句子的含义翻译成英语句子。这里的编码器是一个深层Con-vNet，它将像素转换成最后隐藏层中的活动向量。解码器是一种类似于用于机器翻译和神经语言建模的RNN。近来对这种系统的兴趣激增（参见参考文献86中提到的例子）。 RNN一旦在时间上展开（图5），可以被看作是非常深的前馈网络，其中所有层共享相同的权重。虽然他们的主要目的是学习长期依赖性，但理论和实证证据表明，很难学习长时间存储信息。 为了纠正这一点，一个想法是用明确的存储来扩充网络。这种第一种建议是使用特殊隐藏单元的长期短期记忆（LSTM）网络，其自然行为是长时间记忆输入。一个称为存储单元的特殊单元就像一个累加器或一个门控衰减神经元：它在下一个时间步有一个权重为1的自身连接，因此它复制自己的实值状态并累积外部信号，但是这个自连接被另一个学习决定何时清除内存内容的单元乘上门控值。 随后证明LSTM网络比传统的RNN更有效，特别是当它们在每个时间步上多层处理输入，使得整个语音识别系统从声学到转录中的字符序列在一条流程中完成（end2end)。 LSTM网络或相关形式的门控单元目前也用于在机器翻译上表现如此出色的编码器和解码器网络。 在过去的一年中，一些作者提出了不同的建议，用存储器模块来扩充RNN。提案包括神经图灵机，其中网络通过RNN可以选择从中读取或写入的“磁带”存储器来增强，以及存储器网络，其中常规网络通过一种联想存储器。内存网络在标准问题回答基准测试中表现出色。记忆用于记住后来要求网络回答问题的故事。 除了简单记忆之外，神经图灵机和记忆网络正用于通常需要推理和符号操作的任务。 神经图灵机可以被教授’算法’。 除此之外，当他们的输入由未分类的序列组成时，他们可以学习输出分类的符号列表，其中每个符号伴随有指示其在列表中的优先级的实际值。 记忆网络可以被训练以在类似于文本冒险游戏的设置中跟踪世界的状态，并且在阅读故事之后，他们可以回答需要复杂推断的问题。 在一个测试示例中，网络显示了“指环王”的15个句子版本，并正确回答了诸如“现在佛罗多在哪里？”等问题。 The future of deep learning无监督学习对恢复对深度学习的兴趣具有促进作用，但后来被纯监督学习的成功所掩盖。虽然我们在本文中没有关注它，但我们预计无监督学习在长期内变得更加重要。人类和动物的学习在很大程度上是无人监督的：我们通过观察世界来发现世界的结构，而不是通过被告知每个对象的名称。 人类视觉是一个活跃的过程，它以一种智能的，特定任务的方式使用具有大型低分辨率环绕的小型高分辨率凹孔依次对光学阵列进行采样。我们期望未来远景的许多进展来自端到端训练的系统，并将ConvNets与RNN结合使用，并使用强化学习来决定在哪里寻找。结合深度学习和强化学习的系统还处于初级阶段，但它们在分类任务时已经胜过被动式视觉系统，并在学习玩许多不同的视频游戏时产生令人印象深刻的结果。 自然语言理解是深度学习在未来几年内将会产生巨大影响的另一个领域。我们预计，使用RNNs理解句子或整个文档的系统在学习策略时可以更好地选择性地参与一个部分。 最终，人工智能的重大进展将通过结合表示学习和复杂推理的系统来实现。虽然深度学习和简单推理已经用于语音和手写识别很长一段时间，但是需要新的范例来替代通过对大型向量进行操作的符号表达式的基于规则的操纵。]]></content>
      <tags>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习岗位面试问题整理笔记]]></title>
    <url>%2F2018%2F05%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B2%97%E4%BD%8D%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%95%B4%E7%90%86%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[最近在看知乎，看到有些内容挺好的，摘录一下原文链接：https://zhuanlan.zhihu.com/p/25005808 https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b SGD中的S（stochastic)代表什么随机啊，还能代表什么？ It is called stochastic because each small set of examples gives a noisy estimate of the average gradient over all examples. 它被称为随机因为每个小例子集都给出了所有例子中平均梯度的有噪音的估计。 CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。 CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。 局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。 什么样的资料集不适合深度学习？ 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。 对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法?没有免费的午餐理论。 广义线性模型是怎被应用在深度学习中?深度学习从统计学角度，可以看做递归的广义线性模型。广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。下图是一个对照表： 梯度消失问题？比如，常用的激活函数sigmoid，在进行反向传播的时候，梯度信息往后进行传递，需要乘以激活值对应的导数。我们知道，sigmoid的导数的最大值是0.25，如果经过多层的反向传递，梯度最终会衰减到0。这就是梯度消失。 所以，目前的深度学习主要是 使用Relu作为激活函数，减缓梯度消失的问题。 Dying ReLUs问题？如果神经元通过前向传播得到的值小于0，通过ReLU函数的时候得到的结果为0，而且此时对应的导数也是0，梯度信息无法向后传递，参数无法更新。神经元可能会永久“dying” RNN中的梯度爆炸在进行时间方向的反向传播的时候，梯度信息往后传播过程需要累乘当前RNN的神经元参数W。经过T次累乘后，如果参数W的最大特征值大于1，梯度会爆炸；如果小于1，梯度会消失。 CNN VS DNN VS RNN全连接的DNN结构中，下层的神经元和所有上层的神经元都是连接的，带来的问题是参数的膨胀。不仅容易过拟合，而且很容易陷入到局部最优解。 为了解决这个问题，CNN通过卷积核的方法，同一个卷积核在所有图像的输入都是共享参数的，达到了稀疏连接的效果。卷积之后的输出也是能够保留原先图像的局部信息。从而，CNN的参数相比于DNN，大大减少，也可以进行更深的堆叠。 在RNN中，神经元的输出会作为下一个时间步的输入，通过在这种循环的机制实现对序列变长的数据进行运算处理。 GLOVE加权的最小二乘目标，直接优化的目标是最小化两个单词向量的点积和其共同出现次数的对数之间的差异：$$ J=∑{i,j=1}^{V}f(X{ij})(w^T_iw_j+bi+b~j−logXij)^2 $$]]></content>
      <tags>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter7-深度学习中的正则化]]></title>
    <url>%2F2018%2F05%2F12%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[正则化可以定义为：对学习算法进行修改，目的是减少泛化误差而不是训练误差。 参数范数惩罚我们通常是只对权重进行惩罚，而不对偏置进行惩罚。精确拟合偏置所需要的数据要少于拟合权重的数据。 正则化偏置可能会导致明显的欠拟合情况。 L2正则化权重衰减权重衰减过程中，每个权值的衰减方向是指向0的方向，而步长是和w大小成正比，所以，权值会朝着0衰减，但是步长会越来越慢，最终达到0的附近。 L1正则化L1正则化产生系数解。可以用作特征选择。 作为约束的范数惩罚通过直接给模型的参数设置越是来进行正则化。 数据增强 图像进行翻转，平移等 为输入层注入噪音 噪音鲁棒性一种方法是添加方差极小的噪音，这等价于对权重事实范数惩罚。 另一种使用方法是将噪音增加到权重。主要是用于RNN网络中。 对输入label添加噪音大多数数据集的label都会有一定的错误。错误的label将会误导系统，所以，可以通过对label增加噪音降低错误label的影响。 标签平滑的方法：将分类目标中的0和1分别替换成$\frac{\theta}{k - 1}$和$1 - \theta$ 多任务学习方法Early Stop提前终止等价于L2正则化 参数绑定和参数共享Bagging和其他集成方法通过结合几个模型降低泛化误差。一般采用模型平均的方法。Bagging方法的原理是，通过平均多个高方差低偏差的模型的结果，可以显著的降低总和结果的方差，而不会改变模型的偏差。 这是方差的公式决定的。 Dropout提供一种廉价的bagging集成近似，能够训练和评估指数级数量的神经网络 对抗训练对抗样本产生的原因是过度线性。]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter6-前馈网络]]></title>
    <url>%2F2018%2F05%2F12%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter6-%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[第六章：深度前馈网络也叫做多层感知器，是最典型的深度网络。 我们先从线性模型开始。 线性模型有着明显的缺陷，他的模型能力被限制在线性函数中，无法解释任何两个输入变量间的相互作用。为了拓展线性模型表示非线性函数，可以讲输入x进行非线性的变化$\phi (x)$如何选择映射函数呢？ 选择使用一种通用的映射函数，比如核函数。 手动设计映射函数$\phi$ 深度学习的策略是学习一个映射函数$\phi$ 这个让我突然想到了之前知乎上有人提问说，SVM和神经网络的关系。有人回答说， SVM和神经网络都是为了解决感知器应用局限的问题，对于非线性的问题，SVM通过使用核函数等方法进行纠结，而神经网络采用的是多层感知器进行叠加解决。 实现XOR我们使用两层的神经网络进行求解。可以看出，通过网络的stacking过程，可以大大拓展模型的数据拟合能力。在更现实的应用中，学习的表示可以帮助模型泛化。 在模型中使用ReLu作为非线性的激活函数$$ h = ReLU(W^Tx + c) $$可以致命整个网络是$$ f(x;W,c,w,b)=w^T max(0, W^Tx + c) + b $$ 可以得到一组解解： 基于梯度的学习神经网络的非线性导致了损失函数成为非凸的，意味着，模型的训练通常只能使得损失函数达到一个非常小的值，而不一定是全局最优解。 用于非凸损失函数的SGD没有收敛性的保证，对于参数的呃初始值十分敏感。 代价函数多数情况下，模型定义了一个后验分布$p(y|x;\theta)$，我们可以通过使用训练数据和预测数据之间的交叉熵作为损失函数。即是*数据分布和模型之间的交叉熵** 对于深度学习的设计，反复出现的一个主题是代价函数的梯度必须足够大和具有足够的预测性，为学习算法提供一个好的指引。饱和的函数会破坏这一目的，因为他们的梯度变得非常小，更新的信息无法进行传递。 多数情况下，输出单元中会包含一一个指数函数，他的变量在取绝对值非常大的时候，梯度会出现饱和情况（梯度很小），而负对数似然中的对数函数就会消除输出单元中的指数效果，从而更好地传递梯度信息。 均方误差和平均绝对误差在使用梯度下降的时候往往效果很差，一些饱和的输出单元在使用这些代价函数的时候会产生非常小的梯度。 输出单元用于高斯输出分布的线性单元$$ y = Wh + b $$线性输出层用来产生条件高斯分布的均值$$ p(y|x) = N(y; \hat{y}, I) $$最大似然概率此事等价于最小均方误差 用于伯努利分布的sigmoid单元二值型变量ysigmoid由于指数函数的存在，会出现梯度的饱和性，阻止基于梯度的学习做出好的改进。所以，要使用对数似然估计 用于多重伯努利分布的softmax单元和sigmoid一样，softmax在极端负和极端正的情况下也会出现饱和。这个时候，基于梯度的损失函数也会饱和，学习将会停止。 softmax保证数值稳定的变体：$$ softmax(z) =softmax(z - max(z)) $$ 隐藏单元如何选择合适的隐藏单元类型 整流线性单元是隐藏单元的默认选择。 多数情况下，隐藏单元都可以描述为接受一个输入向量x，计算仿射变换$z = W^tx + b$，然后使用一个逐元素的非线性函数g(z)。隐藏单元的差别仅仅在于激活函数g(z)的形式 整流线性单元及其拓展ReLU:$g(z) = max(0, z)$ Leaky ReLU： 参数化ReLU Maxout单元：$$ g(z) = max z_j $$ maxout单元可以学习具有k个段的分段线性凸函数。。。。 看到了很有意思的一句话：LSTM通过求和在时间上传播信息。 架构设计万能近似定理表明，无论我们试图学习什么样的函数，一个足够大的MLP一定可以表示这个函数。 单层的网络足以表示任意的函数，但是网络层可能大的不可实现，并且无法正确学习和泛化。使用更深的网络能够减少期望函数所需要的神经元个数已经具有更好的泛化性能。 更深的模型的泛化性能越好，但是也要考虑到数据量的问题。 反向传播全连接MLP中的反向传播算法前向传播和损失函数的计算 反向传播过程 历史小结神经网络的核心思想是20世纪80年代提出的，到现在都没有发生重大改变。。近年来神经网络性能的改进可以归因于两个因素： 较大的数据集减少了统计泛化对神经网络的挑战的程度 更强大的计算机和更好的软件设施 少量的算法上的变化也显著的改善了神经网络性能 哪些算法的变化 使用交叉熵族损失函数替代均方误差损失函数 使用整流线性单元替代原有的sigmoid激活函数 ReLU从神经科学的角度去考虑 对于某些输入，生物神经元是完全不活跃的 对于某些输入，生物神经元的输出和输入成正比 多数时间，生物神经元都是不活跃状态。（稀疏激活）]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[acl2018 Chinese NER Using Lattice(网格)LSTM]]></title>
    <url>%2F2018%2F05%2F11%2FLSTM%2F</url>
    <content type="text"><![CDATA[摘要部分论文采用一种网格结构的LSTM模型，用来做汉语的NER，这个模型能够编码一串输入的字（characters），同时还有所有可能的words，用来匹配一个字典。这种方法相比于基于字的方法，可以更好地利用wrod和word sequence 的信息。 相比于基于word的方法，Lattice LSTM不会遇到分割错误的问题。 门控的RNN可以使得模型能够选择最相关的characters和words，从而得到更好的编码信息。实验也表明了这种Lattice LSTM效果更好。 介绍（introduction)目前，英文的NER的state-of-the-art模型是LSTM-CRF，character的信息也有正和到word的表示中。 Chinese NER和分词密切相关。命名实体的边界就是word的边界。一种直觉的解决方法是，先分词，在做NER。 然后，这种方法会有问题，如果分词不对，将会导致NER的错误。分词问题还没有解决，所以，这种方法可能会导致很严重的问题。 character-based NER模型没有使用word和sequence的信息。所以，我们使用网格结构的LSTM表示sentence中的word的信息，将其整合到character-based的LSTM-CRF中。 由于格子中有指数数量的字-字路径，我们利用lattice lstm自动控制sequence从开始到结束的信息流。门控的cells用来动态的路由到每个字符。通过对NER数据的训练，lattice LSTM可以从上下文宗找到更多有用单词，从而获得更好的性能。和character sequence labeling任务相比，模型有用到显示的word信息，而且不会遇到分词带来的错误信息。 Lattice可以看做是tree-structed的RNN的拓展。 模型介绍我们采用LSTM-CRF作为模型的主要结构。使用的符号介绍如下：一个输入sequence可以表示为：$$ s = (c_1, …, c_m) $$$c_j$表示为第j个character也可以表示为：$$ s = (w_1, …, w_n) $$$w_i$表示第i个word$t(i, k)$用来表示第i个word中的第k个character Character-Based model每个character都可以表示为其embedding表示$$ x_j^c=e^c(c_j) $$$e^c$表示为a character embedding lookup table. 一个标准的CRF模型使用hidden states$h^c$作为输入进行sequence labeling char+bichar 通过concat bigram的embedding结果，增强字符的表示。$$ x_j^c = [e^c(c_j); e^b(c_j, c_{j + 1})] $$ char+softword 使用分割得到的word作为character的一个增强信息使用。$$ x_j^c = [e^c(c_j); e^s(seg(c_j))] $$ $e^s$用来表示分割label的embedding lookup table。 word-based model$$ e_i^w = e^w(w_i) $$ 整合character的表达 word+char LSTM通过双向的LSTM学习单词$w_i$的hidden states。$h_{t(i,1)}^c, …, h_{t(i, len(w))}$然后就可以得到word $w$中i-th的character的表达 word + char CNN Lattice model 模型中总共有四种vectorsinput vector, output hidden vecotr, cell vector, gate vector使用recurrent 模型得到一个character cell vector $c_j^c$和一个hidden vector $h_j^c$。在这里$c_j^c$用来表示recurrent信息的流动。$h_j$用于CRF。 LSTM原理 和单纯的character-based model相比，$c_j^c$考虑到了词典的subsequents$w_{b,e}^d$，每个sebsequence可以表示为：$$ X_{b, e}^w = e^w(w_{b, e}^d) $$ 问题：如何才能使用到word的信息呢？通过LSTM，可以得到word$c_{b,e}^w$对应的context vector. 通过subsequence 的表达 $c_{b, e}^w$ ，可以讲更多的信息流入到$c_e^c$距离而言，在图二中的长江大桥，因为字典中存在两个word长江，长江大桥。我们会将所有的word表示$c_{b, e}^w$连接到cell $c_e^c$上。 得到了word的表达$c_{b, e}^w$之后，可以使用一个门用来控制每个单词对最终得到的表达的影响。 进而可以总和所有的word的信息和我们得到的character的信息，得到最终的cell state 将最终得到的h输入到CRF中。 Decoding and Training一个标准的CRF层是使用顶层的h作为输入。对于Lattice model，输入为$h_1, h_2, .., h_n$因为有n个characters 在这里$y’$表示任意的label sequence，$W_{CRF}^{l_i}$表示$h_i$对于label $l_i$的状态特征的参数。$b_{CRF}^{(l_{i-1},l_i)}$表示bias，用来表示状态转移特征的参数，只依赖于当前位置的label和上一个label 使用Viterbi算法进行求解。损失函数定义为： 实验以及实验结果]]></content>
      <tags>
        <tag>NLP, paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本分类中的特征选择和权重计算研究-读博士论文笔记]]></title>
    <url>%2F2018%2F05%2F10%2F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E5%92%8C%E6%9D%83%E9%87%8D%E8%AE%A1%E7%AE%97%E7%A0%94%E7%A9%B6-%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第一章 引言研究现状文本分类的步骤 建立数据集和预处理 收集文本，标注类别，去除非文本内容、编码转换和处理乱码 保留词干（steamming）或者回复原型（lemmatize）处理stopwords 切分训练集和测试集 文本向量化 将文本转化为向量的过程 文本标引 特征单元：如在BOW中，每个词就是一个特征，对应特征空间中一个维度 权重计算 降维 特征选择 特征抽取 将多个特定语法关系的词合并成一个符合特征 通过聚类方法，将相近的特征合并 矩阵分解的方法 学习分类器 测试和评价 特征单元英文中，常用的特征单元有词（word）、词串（word n-gram)和词组（phrase）目前表明，词是文本分类中表现较好的特征 中文中，常见的特征单元有词，字，子串（character n-grams)及其组合。 实验表明，中文中，单字的语义质量不好，性能较差，可以作为其他特征单元的补充。在信息检索中，bi-grams的性能最优，略好于词。在文本分类中，词特征和子串特征都具有较好的分类性能。 特征选择 不相关特征会使分类器对训练集过拟合，从而降低文本分类性能 相关但是冗余的特征也会影响性能 计算代价的考虑 方法 局部特征选择 全局特征选择：在整个类别集上进行特征选择，得到的特征自己对各个类别是一样的。 文本频度（document frequency，df)训练集中初选特征t_i的文本数； 信息增益（information gain，IG）：属于信息论概念，表示特征在文本中出现提供的信息量大小 $X^2$检验量目前来说表现最好的指标之一。 后面就是介绍分类模型和数据集，忽略。 第二章 中文文本的特征单元词特征和二字串特征中文文本中没有类似于英文中的空格之类的显示表示词边界的标识，所以需要继续自动分词。 中文文本处理中，词并不是必须的特征单元，字符串类型（characters n-grams)也是高效常用的特征单元，尤其是二字串（bigrams). 子串切分不同于词切分的一个显著区别：词在文本中是互不重叠的，而字串是重叠的 如：我爱天安门词切分：我/爱/天安门/二字串切分：我爱/爱天/天安/安门/ 所以，按照二字串切分后的特征数量要比词特征数量多很多。]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件随机场（转载）]]></title>
    <url>%2F2018%2F05%2F09%2F%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%2F</url>
    <content type="text"><![CDATA[转载链接：https://zhuanlan.zhihu.com/p/25558273 词性标注任务词性标注（POS Tagging）的目标是使用类似ADJECTIVE, NOUN, PREPOSITION, VERB, ADVERB, ARTICLE的标签对句子（一连串的词或短语）进行打签。例如，句子“Bob drank coffee at Starbucks”的标注的结果就应该是“Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN)”。 那么就让我们建立一个条件随机场来为句子做词性标注。正如分类器所做，我们首先要设定一组特征方程 $f_i$。 CRF的特征函数在CRF中，每个特征函数都要以下列信息作为输入： 一个句子s 词在句子中的位置i 当前词的标签$l_i$ 前一个词的标签$l_{i-1}$ 可以表示为$f_j(s, i, l_i, l_{i - 1})$ 特征函数的输出的是一个实数值（尽管这些值一般是0或1）。 （注意：通过限制我们的特征只依赖于当前与之前的词的标签，而不是句子中的任意标签，实际上我建立了一种特殊的线性CRF，为简单起见，本文不讨论更广义的CRF） 例如，某个特征函数就可以用来衡量当上一个词是“very”时，当前词有多少程度可以被标为一个形容词。 从特征到概率接下来，我们需要为每个特征函数$f_j$设置一个权重$\lambda_j$ 给定一个句子s，现在可以通过累加句中所有词的加权的特征为s的打标结果$l$打分$$ score(l|s))=\sum_{j=1}^{m}\sum_{i=1}^{n} \lambda_{j}f_{j}(s,i,l_{i}^{‘},l_{i-1}^{‘}) $$ 其中，第一个求和是对遍历特征函数j的求和，内层的求和是对句子中的每个位置i进行遍历求和。 最终，我们通过指数和归一化的方法将转换这些得分为0，1的概率值。 特征函数示例我们以POS tagging任务为例： $f_1(s, i, l_i, l_{i-1})=1$ 如果l_i=ADVERB, 且第i个词是以“-ly”结尾；否则为0 如果该特征的权重值 $\lambda_{1}$ 为取值较大的正数，则该特征表明我们倾向于将以 -ly 结果的单词标记为ADVERB。 $f_{2}(s,i,l_{i},l_{i-1})=1$ if $i=1, l_{i} = VERB$ 且句子以问号结尾；否则，为0。 同样，如果该特征的权重值 $\lambda_{2}$ 为取值较大的正数，就偏向于将问句里的第一个词标为VERB（例如 “Is this a sentence beginning with a verb?”）。 $f_{3}(s,i,l_{i},l_{i-1})=1 if l_{i-1} = ADJECTIVE 且 l_{i}= NOUN$；否则，为0。 同样，如果权重值为正，表明形容词后面倾向于跟着名词。 $f_{4}(s,i,l_{i},l_{i-1})=1 ifl_{i-1} = PREPOSITION$ 且$l_{i}=PRE POSITION$。 如果该特征方程的权重值 $\lambda _{4}$ 为负，意味着介词后面一般不会紧跟着一个介词，所以我们应该避免这样的标注。 就是这样！总结起来就是：为了构建一个条件随机场，你需要定义一连串的特征方程（以整个句子、当前位置和相近位置的标签为输入），然后为它们赋值，再做累加，如果必要，在最后将累加得分转换为一个概率值。 看起来像是逻辑回归(Smells like Logistic Regression…)CRF的概率公式看起来会有些眼熟. 这是因为实际上CRF就是序列版本的逻辑回归（logistic regression）。正如逻辑回归是分类问题的对数线性模型，CRF是序列标注问题的对数线性模型。 看起像HMM回顾隐马可夫模型（Hidden Markov Model）它是另一种用于词性标注（和序列标注）的模型 。CRF使用任意的特征函数组用于得到标注得分，HMM采用生成方式进行标注， 定义如下： $p(l,s)=p(l_{1})\prod_{i}p(l_{i}|l_{i-1})p(w_{i}|l_{i})$其中 $p(l_{i}|l_{i-1})$ 为 转移概率（例如，一个介词后面紧跟着一个名词的概率）。 $p(w_{i}|l_{i})$ 为 发射概率（例如，当已知是名词，会出现“dad”的概率）。 那么，HMM与CRF比较会是如何? CRF更加强大- CRF可以为任何HMM能够建模的事物建模，甚至更多。以下的介绍就可以说明这一点。 设HMM概率的对数为log p(l,s)=log p(l_{0})+\sum_{i}log p(l_{i}|l_{i-1})+\sum_{i}log p(w_{i}|l_{i})。如果我们将这些对数概率值看作二进制的转换指示符与发射指示符的权重，这就完全具备了CRF的对数线性形式。 也就是说，我们可以对任意的HMM建立等价的CRF： 为每个HMM的转换概率 $p(l_{i}=y|l_{i-1}=x)$，定义一组转换形式为$f_{x,y}(s,i,l_{i},l_{i-1})=1$ if $l_{i}=y$ 且$l_{i-1}=x$的CRF转换特征。给定每个特征权重值为$w_{x,y)=log p(l_{i}=y|l_{i-1}=x)$。类似的，为每个HMM的发射概率 $p(w_{i}=z|l_{i}=x)$，定义一组CRF发射特征形如 $g_{x,y}(s,i,l_{i},l_{i-1})$ if $w_{i}=z$且 $l_{i}=x$。给定每个特征的权重值为$w_{x,z}=log p(w_{i}=z|l_{i}=x)$。这样，使用这些特征方程由CRF计算得到的 p(l|s)是与相应的HMM计算得到的得分是精确成正比的，所以每个HMM都存在某个对等的CRF。 但是，出于以下两个原因，CRF同样可以为更为丰富的标签分布建模： CRF可以定义更加广泛的特征集。 而HMM在本质上必然是局部的（因为它只能使用二进制的转换与发射特征概率，导致每个词仅能依赖当前的标签，而每个标签仅依赖于上一个标签），而CRF就可以使用更加全局的特征。例如，在上文提到的词性标注特征中就有一个特征，如果句子的结尾包含问号，那么句子中的第一个字为动词（VERB）的概率会增加。 CRF可以有任意权重值。HMM的概率值必须满足特定的约束（例如，$0&lt;=p(w_{i}|l_{i})&lt;=1$，$\sum_{w}p(w_{i}=w|l_{i})=1$,而CRF没有限制（例如，$log p(w_{i}|l_{i})$可以是任意它想要的值）。 权重学习（Learing Weights）让我们回到如何学习CRF的权重这个问题。有一种方式是使用梯度上升（gradient ascent）。 假设我们有一组训练样本（包括句子与相关的词性标注标签结果）。一开始，为我们的CRF模型随机初始化权重值。为了使这些随机初始的权重值最终调整为正确的值 ，遍历每个训练样本，执行如下操作： 遍历每个特征函数 f_{i} ，并为\lambda_{i} 计算训练样本的对数概率梯度值: 注意，梯度公式中的第一项是特征f_{i} 在正确标注下的贡献，梯度公式中的第二项是特征 f_{i} 在当前模型中的贡献。这就是你所期望的梯度上升所要采用的公式。 将$\lambda_{i}$ 朝着梯度方向移动： $$\lambda_{i} = \lambda_{i}+\alpha [\sum_{j=1}^{m}f_{i}(s,j,l_{j},l_{j-1})-\sum_{l’}p(l’|s)\sum_{j=1}^{m}f_{i}(s,j,l_{j}^{‘},l_{j-1}^{‘})]$$其中α是某个学习率（learning rate）。 重复上述步骤，直到达到某种停止条件（例如，更新值已低于某个阈值）。换而言之，每一步都是在抽取，当前模型与我们想要学习到的模型的差异，然后将$\lambda_{i}$ 朝正确的方向移动。 找到最佳标注（Finding the Optimal Labeling）假设我们已经训练好了CRF模型，这时来了一个新的句子，我们应当如何去标注它？ 最直接的方式，就是为每个可能的标注 l计算 p(l|s)， 接着选取一种概率值最大的打标。然而，对一个大小为k标签组和一个长度为m的句子，有 k^{m}种可能的打标方式，这就方式得检查的打准方式是指数级的个数。 一种更好的办法是使（线性链式）CRF满足最佳子结构（optimal substructure）的属性，使得我们可以使用一种（多项式时间复杂度）动态规划算法去找到最佳标注，类似于HMM的Viterbi算法。 更有趣的应用(A More Interesting Application)好吧，词性标注略显无聊，除了它还有很多种其他的标注方法。那现实中什么时候还会用到CRF呢？ 假设你想要从Twitter上了解人们想要在圣诞节上得到什么样的礼物： 如何才能找出哪些词代表礼物呢？为了收集到上图中的数据，我只是简单的去匹配形如“I want XXX for Christmas” 和 “I got XXX for Christmas”的句式。然而，一个更加复杂的CRF变种就可以做GIFT词标注（甚至可以添加其也的类似GIFT-GIVER、GIFT-RECEIVER的标签以获取更丰富的信息），将其转换为词性标注问题来看待。我们可以基于类似“如果上一个词是一个GITF-RECIVER且它的前一个词是‘gave’，那这个词就是一个GITF”或者“如果后面紧跟着的两个词是’for Christmas’，那么这个词就是一个GIFT”的规则去构建特征。]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序列标注模型]]></title>
    <url>%2F2018%2F05%2F09%2F%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87%EF%BC%9A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%A0%94%E7%A9%B6-%E8%AE%A1%E5%B3%B0%2F</url>
    <content type="text"><![CDATA[序列标注模型输出是一个结构化的标签序列，通常，标签之间是相互联系的，构成了标签的结构信息。利用这些结构信息，可以达到比传统分类方法更高的性能。所以，序列标注问题不是一个传统的分类问题，而是一个复杂的结构化机器学习问题。 第一章主要是概括的介绍序列标注模型。 上世纪60年代，HMM是最早应用于标注任务的模型。该模型是一个生成式模型，将序列的标签看做是一个隐状态序列，观测的序列是由隐状态序列产生的，因此需要建立观测序列和隐状态序列孩子间的联合概率分布。 条件随机场和最大熵的方法是判别式模型，可以利用大量甚至冗余的特征拟合标签序列的条件分布概率，当用作训练的数据非常多的时候往往会取得更高的性能。 判别模型建一个从句子$x=(x_1, x_2, …, x_l)$到标签$y=(y_1, …, y_l)$的函数。 为了建立x和y的函数关系，一般情况需要用一组特征函数向量化这两个序列。变成特征向量：这一过程称之为特征抽取或者特征表示 距离而言，我们可以定义$\phi_1(x, y) = 1$可以表示为：当且仅当当前词是“我”，且y=’名词’。 特征函数通常需要手动设计。 使用线性模型 但是，在序列标注模型中，通常标签都是互相联系的。所以，需要时更好的方法来解决这个问题。 第二章 相关工作语素：语言的最小单位。 序列标注任务实际上是为句子中的每一个语素标上一个具有特征含义的符号。 模型介绍对数线性模型NLP任务中，通常利用上下文作为特征，但是这样的特征是高维且稀疏的特征。在高维空间中，通常样本之间是线性可分的。所以，采用对数线性模型。上式通过softmax函数，可以得到条件分布函数。 这种形式叫做对数线性模型，比如，最大熵模型，条件随机场，句法分析模型等。 链式条件随机场 链式随机场是一个无向图模型。根据概率图模型的理论，x和y之间可以建立如下的条件概率模型：]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter5]]></title>
    <url>%2F2018%2F05%2F08%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter5%2F</url>
    <content type="text"><![CDATA[机器学习基础机器学习：对于某类任务T，和性能度量P，一个计算机课程可以认为从经验E中学习是指，经过经验E改进之后，它在renwuT上由性能度量P衡量的性能有所提升。 任务T 分类 回归 机器翻译 结构化输出 输出值是向量或者其他形式的包含多个值的数据结构 输出值之间内部紧密相关。 。。。 策略期望损失模型的期望损失：理论上，模型关于联合分布P(X,Y)的平均意义下的损失。 但是，期望损失是不可求的，因为联合分布不知道。 经验风险对于给定的数据集，模型关于训练数据集的平均损失函数 期望风险是模型关于联合分布的期望风险，经验风险是模型关于训练数据的平均损失。 经验风险最小化和结构风险最小化当模型容量很小的时候，采用经验风险最小化（训练误差）作为学习策略会产生过拟合。 结构风险最小化就是为了防止过拟合而提出的策略。 结构风险最小化等价于正则化。 经验E主要在介绍监督学习和无监督学习。 示例：线性回归定义输出为：$$ y=w^Tx $$ 模型容量，过拟合，欠拟合泛化能力：模型在未知的数据上表现良好的能力。 我们会对训练数据和测试数据进行独立同分布的假设，即是说，每个数据集中的样本都是独立同分布的，训练集和测试集数据是同分布的。 模型容量：表示模型拟合各种函数的能力。容量低的模型很难拟合数据，容量高的模型容易发生过拟合。 一种控制模型容量的方法是选择假设空间，即算法可以选择的解决方案的函数集。 奥卡姆剃刀理论原则上，同样可以解释已知观测数据的假设中，我们选择最简单的哪一个。 统计学习理论告诉我们：训练误差和测试误差之间的差异的上界会随着模型容量增加而增加；随着数据样本的增多而减小。 没有免费午餐理论在所有的可能的数据生成分布上平均之后，每个分类算法在未事先观测的点上都有相同的错误率。 这意味着，机器学习的研究并不是找一个通用的学习算法或者绝对优秀的算法，而是根据具体的任务，去找到最适合的算法，解决任务。 正则化没有免费午餐理论告诉我们，没有最优的学习算法，特别是没有最优的正则化方法。 超参数和验证集超参数不是通过学习得到的，而是由设计者决定的。 验证集的目的是为了得到最优的超参数。 交叉验证估计、偏差和方差从频率学派角度看，我们假设真是参数$\theta$固定但是未知。而点估计是数据的函数。由于数据是随机采样出来的，数据的任何函数的都是随机的，因此点估计量是个随机变量 偏差偏差定义为：点估计的平均期望和真实参数的差。$$ bias(\theta_m) = E(\theta_m) - \theta $$ 如果偏差等于0，表示无偏估计 方差期望的变化程度是多少。 二者关系偏差-方差分解是解释学习算法泛化性能的一种重要工具。泛化的误差可以分解为偏差、方差和噪音的和。$$ E(f;D)=bias^2(x) + var(x) +e $$ 偏差度量了学习算法的预测期望和真实结果的偏离程度，即是刻画了学习算法的拟合能力， 而方差度量了数据扰动对模型性能的影响； 噪声则是学习算法能到到的误差的下限，即是刻画学习问题本身的难度 偏差和方差是一种trade-off的关系，在学习过程中，学习训练不足时候，模型拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时是偏差住到了泛化误差。 当模型的训练程度加深，模型拟合能力增强，数据中的扰动能够被学习器学到，方差就会逐渐主导泛化能力错误率。 刚开始阶段，偏差大，方差小，此时是模型是处于欠拟合状态。之后，随着学习程度加深，会出现偏差小，方差大的情况，此时是处于过拟合状态。 最大似然估计解释MLP的观点是将它看做最小化训练集上的经验分布$p_{data}$和模型分布之间的差异。二者之间的差异可以通过KL散度衡量。 我们知道KL散度是模型预测结果的交叉熵减去真实分布的熵，而后者是固定不变的。 所以，极小化KL散度，等价于极小化交叉熵，也是等价于极小化负对数似然函数。 条件对数似然函数和均方误差对于线性回归问题，我们是假设噪音数据的分布是在$wx$这条直线作为中心的两边正态分布的。 我们就可以定义为：$$ p(y|x)=N(y;\hat{y}(x; w), \sigma^2) $$于是便可以得到添加对数似然函数。$$ NLL = -m log \sigma - \frac{m}{2}log(2\pi) - \sum_{i=1}^m \frac{||\hat{y}^{(i)} - y^{(i)}||^2}{2\sigma^2} $$ 可以看到，最后一项就是MSE 最大似然性质 当样本数目趋向无穷大时候，收敛率而言是最好的渐进估计。 似然估计具有一致性，以为则当训练数据趋向无穷大时，参数的最大似然估计会收敛到参数的真实分布。 当样本数目小到会发生过拟合的时候，正则化策略可以用来获取训练数据有限情况下方差较小的最大似然有偏版本。 贝叶斯估计频率派估计视角是真实参数是未知的定值，点估计$\hat{\theta}$是考虑数据集上的函数的随机变量。 贝叶斯角度完全不同，他是用概率反映知识状态的确定性程度。数据集能够被观察到，所以不是随机的，真实参数是未知的，所以可以表示为随机变量。 在观测到数据之前，我们将参数已有的知识表示成先验概率分布$p(\theta)$ 监督学习算法概率监督学习算法大多数的监督学习算法都是基于估计概率分布$p(y|x)$得到后验概率之后，可以通过最大化对数似然估计搜索最优解。 使用梯度下降等方法。 支持向量机支持向量机只输出类别，不输出概率。 支持向量机最强大的内容是核技巧。 核函数可以使得学习是隐式的在特征空间中进行，不需要显式地定义特征空间和映射函数。 高斯核函数：$$ K(u, v) = exp(- \frac{||u-v||^2}{2 \sigma^2}) $$ 他会沿着v中从u向外辐射的方向减小。 我们可以将其看成一种模板匹配，训练标签y相关的训练样本x变成了类别y的模板，当测试点$x^{‘}$到x距离很小的时候，对应高斯核响应很大，表示二者很相似。 模型进而会对其赋予输入label y较大的权重。 其他监督算法 k-最近邻法 决策树算法无监督学习算法无监督学习算法的学习任务是找到数据的“最佳表示”。常见的三种包括低维表示、稀疏表示和独立表示。 低维表示是将数据尽可能压缩到一个较小的表示中。 稀疏表示是将数据集嵌入到输入项大多数为0的表示中 独立表示试图分开数据分布中变化的来源，表示的维度统计独立。 主成分分析k-means聚类算法随机梯度下降后面有详细去讲，在这里就不多说了。 促使深度学习发展的挑战维度灾难问题局部不变性和平滑正则化为了更好地泛化，机器学习需要由先验信念引导应该学习什么样的函数。 最广泛的隐式“先验”是平滑先验或者局部不变性先验。这个先验表明学习的函数不应该在小的区域内发生很大的变化 这也是L2正则的作用，限制参数大小，防止参数过大导致分类曲面不够“平滑” 深度学习是假设数据由因素和特征组合产生，这些因素和特征可能来自一个层级结构的多个层级。 深度学习的分布式表达带来的指数增益有效解决了维度灾难的问题。 流行学习流形假设：现实生活中的图像，文本，声音的概率分布都是高度集中的；另外，每个样本都是被其他高度相似的样本包围，这些高度相似的样本可以通过变换来遍历流形得到。]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep learning 第四章：数值计算]]></title>
    <url>%2F2018%2F05%2F08%2F%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[数值计算上溢出和下溢出典型的例子：softmax如果发生上溢出，会具有毁灭性的，所以，在进行运算的时候，会减去向量中的最大值。 123def softmax(a): a_max = np.max(a) return np.exp(a - a_max) / np.sum(np.exp(a - a_max)) 病态条件条件数是指：函数相对于输入的微小变化而发生变化的快慢程度 病态条件是：输入被轻微的扰动，导致输出的结果发生巨大的变化。 基于梯度的优化方法梯度下降方法。$f^{‘}(x)=0$的点称之为临界点。包括了极小值，极大值或者鞍点 Jacobian和Hessian矩阵Jacobian矩阵包含了所有的的偏导数的矩阵。 我们对二阶的导数也很感兴趣，二阶导数提供了导数如何进行变化的信息。是对曲率的衡量。 Hessian矩阵就是将所有的二阶导数合并成一个矩阵。可以表示为： 海塞矩阵的作用：可以用来判断临界点是极大点，极小点还是鞍点对海塞矩阵求解特征值，如果是正定的，则该临界点是全局极小点如果是负定的，则是全局最大点。如果是既有正，又有负，则是鞍点。 我们可以使用海塞矩阵的条件数衡量二阶导数的变化。如果Hessian条件数很差，那么梯度下降法表现同样很差，因为一个方向的梯度增加的很快，另外一个增加很慢，在进行梯度下降不知道导数的这种变化，所以不知道应该优先探索梯度为负的方向。 病态条件使得学习步长很难选择，步长要足够小，防止在正曲率很大的方向冲出去。但是如果太小，在其他较小曲率方向上又会学习不明显。 解决方法： BatchNormalization，强制数据满足正态分布 自适应学习方法（Adam等） Hessian矩阵和牛顿法 一般认为牛顿法可以利用曲率的信息，比梯度下降更容易收敛。 约束优化问题参考SVM，主要的方式将使用拉格朗日函数，将约束的问题转为无约束的问题，然后根据KKT条件，将问题转为对偶形式，进行求解。]]></content>
      <tags>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率题]]></title>
    <url>%2F2018%2F05%2F08%2F%E6%A6%82%E7%8E%87%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[概率题 一个桶里面有白球、黑球各100个，现在按下述规则取球： i 、每次从桶里面拿出来两个球； ii、如果取出的是两个同色的求，就再放入一个黑球； iii、如果取出的是两个异色的求，就再放入一个白球。问：最后桶里面只剩下一个黑球的概率是多少？答：动态规划，令f[i,j]表示有i个白球，j个黑球的概率。已知f[100,100] = 1/2, 求f[0,1]。拿到两个白球: f[i-2,j+1] = i/(i+j) (i-1)/(i+j-1) f[i,j] + f[i-2, j + 1]拿到两个黑球: f[i, j-1] = j/(i+j) (j-1)/(i+j-1) f[i,j] + f[i, j - 1]拿到一黑一白: f[i, j-1] =2 i/(i+j) j/(i+j-1) * f[i,j] + f[i, j - 1] 10个人出去玩，集合时间有10分钟，每个人都在该时间内到达，概率均匀分布，彼此独立，那么最后一个人最有可能到达的时间是?遇到这种想不明白，最好的方法就是枚举。若最后一个人在10分钟到达（概率1/10），其他人也都已经到达了（概率是1），总概率是 1^9 (1/10)若最后一个人在9分钟到达（概率1/10），其他人到达的概率是(9/10)9，总概率是 (9/10)^9 (1/10)依此类推。可见概率最大的是第10分钟。 已知随机数生成函数f()，返回0的概率是60%，返回1的概率是40%。根据f()求随机数函数g()，使返回0和1的概率是50%，不能用已有的随机生成库函数。调用f()两次即可，会出现4种结果(0,0), (0,1), (1,0), (1,1)，其中出现(0,1), (1,0)的概率是一样的，可以构造出等概率事件，比如出现(0,1)可返回0，出现(1,0)可返回1，如果出现其他两种情况则舍掉重新调用。 给定rand5()，实现一个方法rand7()。也即，给定一个产生0到4（含）随机数方法，编写一个产生0到6（含）随机数的方法。随机数函数的关键是确保产生每一个数的的概率相等。我们可用通过5 * rand5() + rand5()产生[0:24]，舍弃[21:24]，最后除以7取余数，则可得到概率相等的[0:6]的数值。 100个人排队，每个人只能看到自己之前的人的帽子的颜色（假设只有黑白两色），每个人都得猜自己帽子的颜色，只能说一次，说错就死掉，别人可以听到之前的人的答案以及是否死掉。请问用什么策略说死掉的人最少。假设只有3个人，假设ture = 白，false = 黑，用这个公式x3 = (x1 == x2)，用人话就是1和2的帽子颜色一样的话就说白，不一样的话就说黑。这个策略第一个人死的概率是1/2，剩下的两个都不会死。推广到4个人，也就是x4 = (x3 == (x1 == x2))，照理可以推广到100人。但问题就是人很难判断，只能靠计算机来算。另一个解题方法：“最后一个人看一下前面黑帽子的个数是奇数还是偶数，比如约定奇数说黑，偶数说白。这样前面的人都可以推断出来正确的结果。” 54张牌，平均分成三堆，大小王在同一堆的概率？先平均分三堆，大王在第一堆的概率是1/3, 小王在剩下的53张牌中，有17/53的概率和大王同一堆。依此类推，大王还可能在2,3堆，因此1/3∗17/53∗3=17/53 买饮料，三个瓶盖可以换一瓶，请问要买100瓶饮料，最少需要买多少瓶？答：设要买x瓶。x+x/3&gt;=100对么？小心！x/3瓶如果满3瓶还可以再换的，想象某人堵在小卖部门口狂开瓶。因此应该是x+x/3+x/9+…+x/3n&gt;=100n=log3xx(1−1/3n)/(1−1/3)&gt;=100x(1−1/x)&gt;=200/3x=68不过我返回去算了下, 发现x=68只能买99瓶…毕竟算的时候当x是实数了，因此还是再返回来推一下的靠谱，x=69。 在半径为1的圆中随机选取一点。从[0, 2*pi)随机选取一个角度，再在这个方向的半径上随机选取一个点。但半径上的点不能均匀选取，选取的概率要和离圆心的距离成正比，这样才能保证随机点在圆内是均匀分布的。 一根木棒，截成三截，组成三角形的概率是多少？设第一段截x，第二段截y，第三段1-x-y。考虑所有可能的截法。可能的截法中必须保证三条边都是正数且小于原来边长，则有0&lt;x&lt;1，0&lt;y&lt;1，0&lt;1-x-y&lt;1，画图可知，(x,y)必须在单位正方形的左下角的半个直角三角形里，面积为1/2。然后考虑能形成三角形的截法。首先要满足刚才的三个条件0&lt;x&lt;1，0&lt;y&lt;1，0&lt;1-x-y&lt;1，然后必须符合三角形的边的要求，即两边之和大于第三边，x+y&gt;1-x-y，x+1-x-y&gt;y，y+1-x-y&gt;x，化简即得0&lt;x&lt;1/2，0&lt;y&lt;1/2，1/2&lt;x+y&lt;1画图可知，此时(x,y)必须在边长为1/2的三角形的右上角的半个直角三角形里，面积为1/8。于是最终概率为 (1/8)/(1/2) = 1/4。 抛一个六面的色子，连续抛直到抛到6为止，问期望的抛的次数是多少因为每次抛到6的概率相等，都是1/6，于是期望的次数就是1/(1/6)=6次。 一个木桶里面有M个白球，每分钟从桶中随机取出一个球涂成红色（无论白或红都涂红）再放回，问将桶中球全部涂红的期望时间是多少？通过动态规划来进行求解。 你有一把宝剑。每使用一个宝石，有50%的概率会成功让宝剑升一级，50%的概率会失败。如果宝剑的级数大于等于5的话，那么失败会使得宝剑降1级。如果宝剑的级数小于5的话，失败没有效果。问题是：期望用多少个宝石可以让一把1级的宝剑升到9级？反正没有看的太懂，大概过程和上面的过程差不多。 已知有个randM()的函数，返回1到M随机自然数，怎样利用这个randM()构造randN()，随机1~N。当N&lt;M的时候，可以直接得到，将大于N的数去除掉即可。当N&gt;M，构造类似于(randM() - 1) * M + randM()，可以产生1——M^2，然后在M^2中选择N个构造1——N的映射。如果N&gt;&gt;M,可以对上面得到的randM(M^2)继续进行构造思路二：可以通过位操作进行。用已经知道的randM作为一个0，1生成器，生成所求数的最大位数词。然后组成这个数，当生成的数大于N的时候，舍弃重新开始。 已知一随机发生器，产生0的概率是p，产生1的概率是1-p，现在要你构造一个发生器，使得它产生0和1的概率均为1/2。考虑连续产生两个随机数，结果只有四种可能：00、01、10、11，其中产生01和产生10的概率是相等的，均为p*(1-p)，于是可以利用这个概率相等的特性等概率地产生01随机数。比如把01映射为0,10映射为1。于是整个方案就是：产生两个随机数，如果结果是00或11就丢弃重来，如果结果是01则产生0，结果是10则产生1。 已知一随机发生器，产生的数字的分布不清楚，现在要你构造一个发生器，使得它产生0和1的概率均为1/2。和上面类似，可以产生两个数a，b，第一个比第二个大的时候，判断为0，反之，判断为1 . 已知一随机发生器，产生0的概率是p，产生1的概率是1-p，构造一个发生器，使得它构造1、2、3的概率均为1/3；…。更一般地，构造一个发生器，使得它构造1、2、3、…n的概率均为1/n。关键是找到n个出现概率相等的事件。考虑到产生x个随机数，我们希望0和1的组合概率相等。即二者程度必须是偶数。然后为每种组合赋予一个值即可。要求是C(2*x, x) &gt;= n 给出从n个数中随机选择m个数的方法。n很大，可以认为是亿级别。m可以很小，如接近1；也可以很大，如接近n。一个直接的思路是一直重复地随机，直到随机到m个数为止。这个方法有两个弊端： 难以直到后面随机到的一个数是否在前面已经随机过了，因为数据量很大，无法保存在内存中，如果保存到外存中则时间花费太大。 如果m很大，甚至接近于n，则后面随机到的数字基本上都是前面随机过的，因而需要尝试的随机次数太多。 一个思路是每个数被选中的概率是m/n，则可以遍历一遍原数据，在遍历每个数字的同时以m/n的概率决定是否要选择当前数字，则当遍历完毕的时候，选择到的数字在平均意义就是m个。这个会随着n的增大而更好地趋近于m，但不能很精确地保证随机到的数字一定是m个。 以上思路虽然不能满足要求，但我们可以进行改进。刚才我们在遍历每个数字的时候都是以同样的概率m/n决定是否要选择该数字，实际上，在当前遍历数字的前面的数字的结果我们是已经知道了，我们可以根据前面的随机结果动态地调整当前的随机策略，使得最终能够保证随机到的数字一定是m个。12具体的做法是，**遍历第1个数字时有m/n的概率进行选择，如果选择了第1个数字，则第2个数字被选择的概率调整为(m-1)/(n-1)，如果没选择第1个数字，则第2个数字被选择的概率为m/(n-1)。即遍历到第i个数字的时候，如果此时已经选择了k个，则以(m-k)/(n-i+1)的概率决定是否要选择当前的第i个数字。这样可以保证每次都能够保证在剩下的数字中能选择适当的数使得总体选择的数字是m个。比如，如果前面已经随机了m个，则后面随机的概率就变为0。如果前面一直都没随机到数字，则后面随机到的概率就会接近1。最终得到的结果始终精确地是m个数字。** 给出从n个数中随机选择1个的方法。注意，n非常大，并且一开始不知道其具体值。数字是一个一个给你的，当给完之后，你必须立刻给出随机的结果。答案是要保证每个数字被选取的概率是相等，当第i个数来的时候，如果我们已经保证了前i-1个数每个数被选取的概率都是相等的，那么只要第i个数字被选取的概率是1/i，我们就可以知道所有i个数被选取的概率都是1/i了。所以只需要以1/i的概率决定是否要选取当前的第i个数字即可。于是可以保证对于任意的n，当给完n个数字时，选择每个数字的概率都是相等的，为1/n。 给出从n个数中随机选择m个的方法。注意，n非常大，并且一开始不知道其具体值。数字是一个一个给你的，当给完之后，你必须立刻给出随机的结果。这题是上一题的推广，于是可以仿照着进行。首先前m个数字是必须拿的。问题是当第i（i&gt;m）个数字来的时候，究竟是要丢弃这个数，还是保留这个数？如果要保留这个数的话，则必须得丢弃手中已有的m个数，那是怎么确定丢弃哪个呢？下面是就具体的做法。第i个数到来的时候，以m/i的概率决定是否要选择这个数字。如果选择了这个数字，则随机地替换掉手上m个数字中的一个。如果前i-1个数字的时候保证了每个数字被选取的概率相等，则这样做之后可以保证每个数字被选取的概率也相等，为m/i。 第i个数选择的概率是m/i，因为算法就是这样决定的。 考虑前i-1个数字中的任意一个，它在第i个数之前被选择的概率是m/(i-1)。在第i个数字的时候，这个数字要被选择的话又两种可能，一是第i个数没有被选中（概率是1-m/i），二是第i个数倍选中了（概率是m/i）但是替换掉的数字不是它（概率是1-1/m），于是这个数在第i个数时仍然被选择的概率是m/(i-1) ((1-m/i) + (m/i (1-1/m))) = m / (i-1) * ((i-1) / i) = m/i。由数学归纳法原理知，对于任意的n，当给完n个数的时候，选择的结果可以保证这n个数中每个被选中的概率都是相等 一个三角形， 三个端点上有三只蚂蚁，蚂蚁可以绕任意边走，问蚂蚁不相撞的概率是多少?乍一看有点蒙…其实很简单…1.每个蚂蚁在方向的选择上有且只有2种可能，共有3只蚂蚁，所以共有2的3次方种可能2.不相撞有有2种可能，即全为顺时针方向或全为逆时针方向。不相撞概率=不相撞/全部=2/8 A城一个商人有一头驴子和3000根胡萝卜.要将萝卜拉到1000公里外的B城去卖，只能用驴子驮。已知驴子一次性可驮1000根胡萝卜,但每走一公里要吃掉一根胡萝卜.问商人共可卖出多少胡萝卜?这题的问题出在给的条件不充分，可能有两种情况，一种是非理想状态的拉运萝卜，这种情况的话，一个也不能卖出去；第二种为理想状态下的，就是能在每走一公里的时候能够卸货下来，并且还没有其他的外在因素使得萝卜丢失，这样的理想状态下能够运送到B城833个萝卜。计算过程如下：1、因为有3000萝卜，所以消耗掉前面的1000个萝卜的时候，每前进一公里要来回3次消耗3个萝卜，因此第一次前进了X公里，X=1000/3=333余数为1；2、剩下的2001个萝卜在第334公里的时候需要来回3次才能运送完，我们就将这一公里路算在2000个萝卜在第一次运送的时候算在一起，那么第二次运送萝卜只需要来回2次消耗2个萝卜，因此第二次消耗1000个萝卜前进了Y公里，Y=1000/2=500；现在前进了500+333=833公里，剩下167公里后，萝卜也只剩下了1000个，所以就这1000个直接中途不卸货的一次到达B城，消耗167个萝卜，所以最后剩下1000-167=833个萝卜。 甲乙两个人答对一道题的概率分别为90%和80%，对于一道判断题，他们都选择了“正确”，问这道题正确的概率。请问这题怎么做，谢谢有人有以下解法，以下是否正确？题目为真且甲乙都选则正确的概率 / 甲乙都选择正确的概率题目为真且甲乙都选真/(题目真甲乙都选真+题目假甲乙都选真)(0.50.90.8)/{(0.50.90.8+0.50.10.2)}=72/74 飞机上有100个座位，按顺序从1到100编号。有100个乘客，他们分别拿到了从1号到100号的座位，他们按号码顺序登机并应当对号入座，如果他们发现对应号座位被别人坐了，他会在剩下空的座位随便挑一个坐。现在假如1号乘客疯了 -_-! (其他人没疯)，他会在100个座位中随机坐一个座位。那么第100人正确坐自己座位的概率是多少？注意登机是从1到100按顺序的。换个角度，相当于疯会传染，如果某个人的作为被疯子占了，那么他就会去做其他人的位置，当最后的时刻。相当于疯子要么坐上自己的位置，要么坐上了我的位置。二选一。 两个信封，一封里装的钱是另一封里的两倍，你选了一个打开一看，里面是十块钱，这个时候你可以选择换成另一个，问你该不该换？第一印象肯定是换不换无所谓，因为貌似二者不相关。但是随后又会想到，另一个信封有一半可能是五块，一半可能是二十块，期望值会是1/2(5+20) = 12.5，当然应该换。 三人陪审团，其中两人每人做出正确决定的概率是Ｐ。另一人总是靠扔硬币来决定支持谁。最后由多数票决定结果。另一个审判由一人决定，这个人做出正确决定的概率也是Ｐ。问三人陪审团与一人审判谁有更大概率做出正确决定？一样大，都是p 概率面试题 给你一个数组，设计一个既高效又公平的方法随机打乱这个数组（此题和洗牌算法的思想一致） 123456import randomdef shuffle(nums): for i in range(len(nums)): j = random.randint(0, i) nums[i], nums[j] = nums[j], nums[i]return 有一苹果，两个人抛硬币来决定谁吃这个苹果，先抛到正面者吃。问先抛这吃到苹果的概率是多少？给所有的抛硬币操作从1开始编号，显然先手者只可能在奇数（1,3,5,7…）次抛硬币得到苹果，而后手只可能在偶数次（2,4,6,8…）抛硬币得到苹果。设先手者得到苹果的概率为p，第1次抛硬币得到苹果的概率为1/2，在第3次（3,5,7…）以后得到苹果的概率为p/4（这是因为这种只有在第1次和第2次抛硬币都没有抛到正面（概率为1/4=1/2*1/2）的时候才有可能发生，而且此时先手者在此面临和开始相同的局面）。所以可以列出等式p=1/2+p/4，p=2/3。 一个面试题：快速生成10亿个不重复的18位随机数的算法(从n个数中生成m个不重复的随机数 1234567891011import randomdef random_choice(n, m): count = 0 for i in range(1, n + 1): if count == m: break t = random.randint(n - i) if t &lt; m: count += 1 return 你有两个罐子以及50个红色弹球和50个蓝色弹球，随机选出一个罐子然后从里面随机选出一个弹球，怎么给出红色弹球最大的选中机会?在你的计划里，得到红球的几率是多少? 题目意思是两个罐子里面放了50红色和50蓝色弹球，然后我任选一个罐子，从中选中一个红球的最大概率，是设计一个两个罐子里怎么放这100球的计划。一个罐子：1个红球另一个罐子：49个红球，50个篮球几率=1/2+(49/99)*(1/2)=74.7% 一副扑克牌54张，现分成3等份每份18张，问大小王出现在同一份中的概率是多少？（大意如此）不妨记三份为A、B、C份。大小王之一肯定在某一份中，不妨假定在A份中，概率为1/3。然后A份只有17张牌中可能含有另一张王，而B份、C份则各有18张牌可能含有另一张王，因此A份中含有另一张王的概率是17/(17+18+18)=17/53。 也因此可知，A份中同时含有大小王的概率为1/3 * 17/53。 题目问的是出现在同一份中的概率，因此所求概率为3(1/3 17/53)=17/53。 有一对夫妇，先后生了两个孩子，其中一个孩子是女孩，问另一个孩子是男孩的概率是多大？ 答案是2/3.两个孩子的性别有以下四种可能：（男男）（男女）（女男）（女女），其中一个是女孩，就排除了（男男），还剩三种情况。其中另一个是男孩的占了两种，2/3. 之所以答案不是1/2是因为女孩到底是第一个生的还是第二个生的是不确定的。 一个国家人们只想要男孩，每个家庭都会一直要孩子，只到他们得到一个男孩。如果生的是女孩，他们就会再生一个。如果生了男孩，就不再生了。那么，这个国家里男女比例如何？一个家庭的孩子数量可以为:1,2,3,4,5….. 对应的的男女分布为: “男”,”女男”,”女女男”,”女女女男”,”女女女女男”… 对应的概率分布为 1/2, 1/4, 1/8, 1/16, 1/32 。其中女孩的个数分别为 0,1,2,3,4…… 因此 S2=01/2 + 11/4 + 21/8 + 31/16 + 4*1/32 + ……… 可以按照题目2用级数求，也可以用错位相减法：S2=1/4+2/8+3/16+4/32+… 两边乘以2,得: 2*S2=1/2+2/4+3/8+4/16+5/32+.. 两个式子相减得 S2=1/2+1/4+1/8+1/16+1/32+…=1. 所以期望值都为1，男女比例是一样的。 有一个很大很大的输入流，大到没有存储器可以将其存储下来，而且只输入一次，如何从这个输入流中等概率随机取得m个记录不知道数据量有多大的问题。蓄水池抽样 或 reservoir sample。假设输入到第n个记录了，以m/n的概率取该数，如果取中则随机替换掉原来取中的m个记录中的一个。初始时，选中前m个记录。 有一个木桶，里面有M个白球，小明每分钟从桶中随机取出一个球涂成红色（无论白或红都涂红）再放回，问小明将桶中球全部涂红的期望时间是多少？ 12345678910111213141516171819202122232425262728数学期望类的题目，主要是要理解什么是数学期望，数学期望是干什么用的，关于这些问题的解答，大家可以自己去理解，思考或者翻书，我要讲的内容是如何利用这些数学期望的特点。现在我们开始解答上面的问题：令P[i]代表M个球中已经有i个球是红色后，还需要的时间期望，去将所有球都变成红色。 So，给出递归式：P[i]= (i/M) * P[i] + (1-i/M)* P[i+1] + 1我相信大家都能理解这个公式的含义，不过还是解释一下，在P[i]的情况下，我们选一次球，如果是红球，那么概率是i/M，子问题还是P[i]，如果是白球，那么概率是1-i/M，子问题是P[i+1]，注意你当前的选球操作要计算在内，即一次 化简如上递归式得：P[i] = P[i+1] + M/(M-i)，显然P[M] = 0;所以：P[M-1] = P[M] + M/1P[M-2] = P[M-1] + M/2…P[0] = P[1] + M/M综上：P[0] = 0 + M/1 + M/2 + … + M/M，至此问题已经解决，不过我希望大家学到的不是这个答案，而是分析这个题目的过程]]></content>
      <tags>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性模型]]></title>
    <url>%2F2018%2F05%2F07%2F%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[线性模型线性回归模型和LR模型]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大似然估计]]></title>
    <url>%2F2018%2F05%2F07%2F%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[最大似然估计最大似然估计就是找到一组参数，使得“事情发生的概率最大” 最大似然估计可以转化成对数形式的最大似然估计。而对数似然估计函数其实就是交叉熵！我们知道，交叉熵用来表示衡量模型预测的分布和真实分布的差异，所以，极大化对数似然估计，等价于极小化交叉熵。也就是极小化模型预测和真实分布的差异。 最大似然估计可以看做是最小化训练数据集上的经验分布p_data和模型分布之间的差异，两者之间的差异可以通过KL散度度量。 熵，交叉熵，KL散度参考链接：https://www.zhihu.com/question/41252833 熵信息量：是用来表示事件A的信息量，也叫做编码长度。熵用来表示多个事件的平均信息量$$ H(p)=\sum p(i) * log(\frac{1}{p(i)}) $$ 交叉熵模型分布q来自真实数据分布p的平均编码长度。$$ H(p, q)=\sum p(i) * log(\frac{1}{q(i)}) $$ KL散度用来衡量模型预测的分布q和真是分布的平均编码长度的差异$$ D(p||q)=H(p,q) - H(p) $$ 为什么使用交叉熵(对数似然估计）作为损失函数？ 因为数据的真是分布样本在数据给定的时候就已经确定（频率学派的观点），所以，真实分布的熵是一个定值。此时，KL散度和交叉熵是等价的关系。极小化交叉熵，等价于极小化KL散度，使得模型预测的分布接近样本真实的分布。 激活函数使用使用sigmoid或者softmax函数时，指数函数会出现在输入值很大的情况下，梯度非常小的情况，这会导致梯度信息无法传递。 而交叉熵中的log函数可以抵消掉指数函数的影响。]]></content>
      <tags>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率论和数理统计]]></title>
    <url>%2F2018%2F05%2F07%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E5%92%8C%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[本节内容会摘录下自己想要了解的概率论知识。 概率事件和概率 古典型概率 古典型概率的基本事件为n，事件A包含k个基本事件，则A的概率定义为：$$ P(A) = \frac{k}{n} $$ 几何形概率 落入区域A的概率和区域A的几何度量有关，而与位置和形状无关。 计算概率的公式 乘法公式$$P(AB)=P(A)P(B|A)$$ 全概率公式$$P(B)=\sum_{i=1}^{n}P(B|A_i)$$ 贝叶斯公式$$P(A|B)=\frac{P(AB)}{P(B)}=\frac{P(B|A)P(A)}{P(B)}$$ 伯努利公式（二项概率公式） n重伯努利实验中，事件A发生k次的概率 $$ P(A_k)=C_n^kp^kq^{n-k} $$ 伯努利实验 结果是两种对立的事件。 第二章 随机变量及其分布常见的离散型随机变量及其分布0-1分布(伯努利分布)随机变量只可能取0和1两个值$$ P{X=k}=p^kq^{1-k} $$ 二项分布即是n重伯努利分布 $$ P(A_k)=C_n^kp^kq^{n-k} $$ 几何分布 $$ P(X=n)=pq^{n-1} $$ 描述连续独立重复的伯努利实验中，首次取得成功的概率。 超几何分布 常见的连续性随机变量和概率密度均匀分布（uniform） 正态分布 常见概率分布表和他们的相互关系 第三章 多维随机变量及其分布联合概率分布边缘分布第四章 随机变量的数字特征数学期望离散型：$$ EX=\sum x_ip_i $$ 连续型：$$ EX = \int xf(x)dx$$ 数学性质： 注意，X+Y的期望和二者是够否独立没有关系 方差$$ D(X)=Var(X)=E[[X-EX]^2] = E[X^2] - (EX)^2$$ 方差的重要性质 协方差$$ Conv(X, Y)=E[(X-EX)(Y-EY)] $$ 相关系数 系数的值为1意味着X 和 Y可以很好的由直线方程来描述，所有的数据点都很好的落在一条 直线上，且 Y 随着 X 的增加而增加。系数的值为−1意味着所有的数据点都落在直线上，且 Y 随着 X 的增加而减少。 系数的值为0意味着两个变量之间没有线性关系。 第五章 大数定理和中心极限定理切比雪夫不等式 切比雪夫大数定理 中心极限定理（独立同分布情况）中心极限定理完美的适应在机器学习中的数据分布。我们假设数据样本之间是独立同分布的。 整个数据样本的分布就符合正态分布的情况了。 数理统计部分参数估计和假设检验最大似然估计]]></content>
      <tags>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter3]]></title>
    <url>%2F2018%2F05%2F07%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter3%2F</url>
    <content type="text"><![CDATA[chapter3 概率和信息论为什么需要概率论？ 概率法则告诉AI系统如何进行推理 可以使用概率论和统计从理论上分析AI系统 随机变量 概率分布用来描述随机变量在每一个可能取到的状态的可能性大小。 离散型随机变量和概率质量函数连续随机变量和概率密度函数概率密度函数满足的条件： p的定义域必须是x所有可能的集合 p(x) &gt;= 0 p(x)的积分等于1 边缘概率条件概率表示时间A在另一个事件B已经发生的条件下的概率 其他内容已经在下面的链接整理完毕。https://gjwei.github.io/2018/05/07/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[位运算]]></title>
    <url>%2F2018%2F05%2F05%2F%E4%BD%8D%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[总结下位运算的相关内容 并 A|B 交 A&amp;B 减法 A&amp;~B(即是求解A中不包括B的部分) 求反 ~A 设置某一位为1 A |= 1 &lt;&lt; n 清除其他位为0：A &amp;= ~(1 &lt;&lt; n) 测试第n位是否为0：A &amp;(1 &lt;&lt; n) == 0 抽取最后一位 A &amp; -A or A&amp;~(A-1) 移除最后一位 A&amp;(A-1) 示例： 统计给定数有多少位 123456int count(n): count = 0 while n: n = n &amp; (n - 1) count += 1 return count 判断某个数是否是4的指数 123def is_power_four(n): return !(n &amp; (n - 1)) and (n &amp; 0x55555555) # 第一个是判断只有一位有效，第二个是判断是否在正确的位置 ^的使用trick使用^删除完全相同的数字并保存奇数，或保存不同的位并删除它们。 求解两个数的和 12def get_sum(a, b): return a if b == 0 else get_sum(a ^ b, (a &amp; b) &lt;&lt; 1) 使用|的使用trick可以保留尽量多的1位翻转数字的位1234567def reverse(n): mask = 1 &lt;&lt; 31 res = 0 for i in range(32): if n &gt;&gt; i &amp; 1 == 1: res |= (mask &gt;&gt; i) return res &amp;的tirck可以用来选择特定的位求解一位unsigned int有多少位是1123456def hammingweight(n): count = 0 while n: n &amp;= (n - 1) count += 1 return count]]></content>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter2]]></title>
    <url>%2F2018%2F05%2F05%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter2%2F</url>
    <content type="text"><![CDATA[第二章：线性代数概念 标量 向量 矩阵：二维数组 张量：超过二维的数组 转置 在运算中，我们允许矩阵和向量相加，这种是隐式的复制向量b到很多位置，以满足相加的条件的方式，叫做广播 矩阵相乘很了解，忽略不看了。 单位矩阵和逆矩阵线性相关和生成子空间子空间是指原始数据线性组合能达到的点的集合 特征分解将矩阵分解成一组特征向量和特征值的形式。特征向量定义为：$$Av=\lambda v$$标量$\lambda$称为特征值。矩阵的特征分解可以记做：$$A=Vdiag(\lambda) V^{-1}$$ 正定矩阵：所有的特征值都是正的负定矩阵：特征值都是负的半正定矩阵：特征是都是非负的半负定矩阵：特征值都是非正的 奇异值分解$$ A=UDV^{T}$$矩阵U，V都是正交矩阵，D是对角矩阵 我们可以从特征分解的角度理解SVD A的左奇异值向量是$AA^T$的特征向量 A的右奇异值是$A^TA$的特征向量]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning笔记-chapter1]]></title>
    <url>%2F2018%2F05%2F05%2Fdeeplearning%E7%AC%94%E8%AE%B0-chapter1%2F</url>
    <content type="text"><![CDATA[第一章引言如何去理解深度学习？深度学习通过让计算机从经验中（数据）学习，并根据层次化的概念去理解世界。目的是从原始数据中抽取出高层次、抽象的特征。 让计算机从经验中学习获取知识，避免了由人类给计算机形式化指定他所需要的所有知识。 层次化的概念可以让计算机通过构造简单的概念来学习复杂的概念。这些概念概念可以建立一张很深的图（层次很多）。 所以，才有了深度学习 文中出现了一个有趣的例子，人们可以很容易的在阿拉伯数字下进行运算，在罗马数字的表示下，运算会非常的耗时间。以此说明“表示”的重要性。 紧接着，提出了“表示学习”表示学习是使用机器学习来发掘本身，而不仅仅是将表示映射到输出。 表示学习的典型例子：自编码器 分布式表示：对于每一个输入，都应该由多个特征表示，并且每个特征都应该参加到多个可能输入的表示。]]></content>
      <tags>
        <tag>deeplearning笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习文本匹配综述]]></title>
    <url>%2F2018%2F05%2F04%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[介绍最近想参加下蚂蚁金服的语义相似度匹配的比赛，看下一篇综述性质的文章，了解下深度学习是如何应用到这个领域的。http://cjc.ict.ac.cn/online/onlinepaper/pl-201745181647.pdf 论文内容文本匹配面临的问题： 词语匹配的多元性如”苹果“这个词，既可以当做水果，也可以是一个公司的名称 短语匹配的结构性多个词语可以按照一定的结构组合成短语，匹配两个短语需要考虑短语的结构信息。 层次性 深度模型的引入深度学习的方法，可以自动从原始数据中抽取特征，免去了大量的人工设计特征的开销。首先特征提取的过程是模型的一部分，根据数据的不同，可以很方便的适配到各种文本匹配的任务中。现在的深度模型加上词向量的技术，可以更好的解决词语匹配的多元性问题。 深度模型介绍 基于单语义文档表示的深度学习模型 多语义文档表示的深度模型 直接建模匹配模式的深度学习模型 单语义模型的做法是将单个文本表示成一个稠密向量（分布式表达），然后计算两个向量之间的相似度作为文本的匹配度。 多语义模型认为单一粒度的向量表示一段文本不够精细，需要多语义表示，也就是分别提取词，短语，句子等不同级别的表达向量，再计算不同粒度向量之间的相似度作为 直接建模的方法是更早的让两个文本进行交互，然后挖掘出交互后的模式特征，总和得到相似度。 文本匹配可以用在搜索引擎中，两者分别是查询项和文档；在问答系统中，两者分别是答案和问题。 问题简介抽象成数学的概念，两个训练数据的集合$S_{train}={(s_1^{(i)}, s_2^{(2)},r^{(i)})}$s表示文本，r表示对象之间的相似程度。 传统的方法主要是基于人工提取特征，问题的焦点是如何设置合适的文本匹配学习算法学习到最优的匹配模型。人工提取特征的缺点： 代价大，花费很多的时间精力做特征工程 基于主题模型的隐空间模型比较粗糙，不能精确建模文本匹配中的语义相似程度 传统方法很难发觉到一些隐含在大量数据中的含义不明确的特征 深度模型的方法深度学习用在NLP的优势： 深度模型可以将单词表示为语义空间中的向量，利用向量之间的距离可以更准确描述两个单词之间的语义相关性 模型自身是具有层次化和序列化的，能够自然的描述自然语言中的层次结构，序列结构和组合操作 可以很好地利用到大数据的优势和高性能计算的能力。 单语义文档表示将文档表示成一个向量的方法，通过表达的相似度凉衡量两个对象的匹配程度。 使用CNN来表示文档深度语义结构网络（Convolution Deep Semantic)。使用卷积网络，提取到sequence的分布式表达，然后将两个句子的向量拼接到一起，输入到一个全连接层中。 基于RNN的模型CNN无法捕捉到长距离依赖关系的问题。将两个s使用RNN表示成一个向量，然后计算两个向量表示之间的余弦距离作为相似度的度量，最终输出匹配值。 多语义文档表示的模型方法不仅考虑两个文本最终表示向量的相似程度，也会生成局部短语或者更长短语的表示进行匹配。多粒度的匹配可以更好地弥补单语义表示方法在压缩整个句子过程中的信息损失，从而达到更好的效果。 多粒度CNN（MultiGranCNN模型）将一个句子拆解成四种层次，单词级别，短语级别和长短句级别，句子级别。之后将两个句子不同级别的特征进行两两相似度匹配，得到一个相似度的矩阵，进行动态最大值池化得到两个句子的相似度得分。模型的架构如下： 多角度RNNRNN在扫描一个句子过程中，能够冲不同位置输出一个表示，这个表示是开始到当前位置的内容整合。Wan等人发现了多视角RNN网络（MV-LSTM) 使用双向的LSTM编码信息，然后两个句子中每个hidden state进行求解相似度计算，得到一个相似度矩阵，通过最大池化的方法得到一个向量，然后输入到分类器中。 小结下单语义和多语义的内容两种模型都是通过将两个对象分开表示，最终计算两个表示的相似度。不同的是，多语义文档表示方法会考虑不同粒度的表示。 直接建模匹配模型的方法这个方法主要是关注文本表示（局部化或者全局化）为核心的思路，直接建模匹配模式的深度歇息模型，旨在直接捕获到匹配的特征。实验表明，这种方法在相对复杂的问题上表现更为优秀。 主题深度匹配模型 CNN深度匹配模型（ARC-II）首先是吧句子表示成单词的向量序列，然后使用滑动窗口方法作为基本单元进行卷积操作，得到一个三维的张量，作为两个句子相互作用的初步表示。随后的卷积对这个三维张量进行卷积+池化的过程，得到一个描述两个句子整体的向量，然后输出到多层感知器中，输出相似结果。 MatchPyramidARC-II模型层次化的过程很模糊，MP模型重新定义了两个文本交互的方式——匹配矩阵。然后根据这个匹配矩阵进行二维的卷积提取匹配空间的模式，最后通过全连接层的网络得到两个句子之间的相似度。MatchPymaid核心思想是层次化构建匹配过程。首先定义的匹配矩阵是根据最细粒度的词向量之间的余弦相似度定义的，然后句子之间两两词之间都会计算相似度，根据此在句子空间位置刚好可以构建一个二维的矩阵。 之后，把匹配问题看成是这个二维匹配矩阵上的图像识别问题。在卷积层可以得到n-gram的匹配特征。最终得到非常好的效果。 Match-SRNN在得到匹配矩阵之后，Match-RNN发现使用二维的RNN来建模特征空间的模式更为合理。2D的GRU网络可以模拟最长公共子串的计算过程。利用之前提到的张量神经网络，捕获到两短文本间的基础交互信息，每个单词可以表示成分布式向量，每个单词之间的交互信息，表示成一个向量形式。]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试心得]]></title>
    <url>%2F2018%2F05%2F04%2F%E9%9D%A2%E8%AF%95%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[最近活的相当郁闷，不断接到面试被拒的通知。每场面试都是自我感觉良好，可见，确实是自己的水平确实是有待加强。前两天，老朱帮我提出了简历上的建议，才发现自己简历做的太不认真了，重点的知识点都没有突出，没法给人好的印象。这算是一方面吧，自己确实有点菜，不然也不会面了这么多家公司，全被拒了。剩下的时间，好好看书，好好写论文，写专利，秋招翻身]]></content>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则化方法]]></title>
    <url>%2F2018%2F05%2F03%2F%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[正则化方法正则化是对学习算法进行修改，目的是为了减少泛化误差而不是训练误差。利用偏差的增加换取方差的减少。 参数的范数惩罚L2参数正则化经过求解梯度后发现，L2正则每次更新权值，都是对权值乘以一个1-a的系数，所以，也叫做权重衰减。所以，L2正则会得到很多很小的权值。 L1参数正则化求解梯度后，可以发现，L1正则每次更新权值，都是对权值增加或者减少固定值a，不再是线性的缩小。这样的结果是有很多不重要特征的权值会直接所见到0. 产生稀疏解。作为约束的范数惩罚 通过一些显示的约束对模型进行正则化。比如在树模型中，我们会对每棵树的节点数，深度等进行约束显示。数据集增强 通过增加数据集，减少噪声数据的影响，从而可以提高模型的泛化能力。 噪声鲁棒性 对数据集通过注入噪声的方法，这个可以作为一种数据集增强的方法提前终止 DL这本书中说的是提前终止的效果等价于L2正则化。可以将优化的参数空间限制在初始参数值的小领域中，不让他发生太多变化。 Dropout提供一种廉价的近似Bagging的集成方法来防止出现过拟合。 对抗训练]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kmeans和em算法]]></title>
    <url>%2F2018%2F05%2F03%2Fkmeans%E5%92%8Cem%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[K-mean与高斯混合模型：K-means算法和EM算法的差别在哪里？答：CSDN博主JpHu说，K-Means算法对数据点的聚类进行了“硬分配”，即每个数据点只属于唯一的聚类；而GMM的EM解法则基于后验概率分布，对数据点进行“软分配”，即每个单独的高斯模型对数据聚类都有贡献，不过贡献值有大有小。]]></content>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习优化方法-BN]]></title>
    <url>%2F2018%2F05%2F03%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95-BN%2F</url>
    <content type="text"><![CDATA[BN &amp; LN批规范化的效果依赖于 minibatch 的大小，而且对于循环神经网络 RNN 无法下手. 本文将批规范化转换成层规范化——通过计算在一个训练样本上的一层上的神经元的求和输入的均值和方差.有答案分析了一遍paper，那我来给个直观的理解，batch vs layer normalization。batch是“竖”着来的，各个维度做归一化，所以与batch size有关系。layer是“横”着来的，对一个样本，不同的神经元neuron间做归一化。 我的理解：BN是在一个mini batch上进行归一化的，求解在所有batch上dim维度上的均值和方差，然后进行归一化BN是要求每个batch输出的维度相同，但是RNN是处理变长的数据的，所以输出的维度是不同的。 LN是在一个样本上进行归一化，对样本的同一层的神经元输出进行归一化。所以，归一化的结果和batch没有关系]]></content>
      <tags>
        <tag>deeplearning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优化器介绍]]></title>
    <url>%2F2018%2F05%2F03%2F%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[深度学习中的优化器AdaGrad亦称为适应性梯度（Adaptive Gradient），它允许学习率基于参数进行调整，而不需要在学习过程中人为调整学习率。AdaGrad 对具有较大梯度的参数相应地有一个快速下降的过程，而具有小梯度的参数在学习率上有相对较小的下降速度。因此，AdaGrad 成了稀疏数据如图像识别和 NLP 的天然选择。 RMSProp 算法（Hinton，2012）修改 AdaGrad 以在非凸情况下表现更好，它改变梯度累积为指数加权的移动平均值，从而丢弃距离较远的历史梯度信息。 Adamadam算法的提出者描述其为两种随机梯度下降扩展式的优点集合，即： 适应性梯度算法（AdaGrad）为每一个参数保留一个学习率以提升在稀疏梯度（即自然语言和计算机视觉问题）上的性能。 均方根传播（RMSProp）基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很有优秀的性能。Adam不仅如RMSProp一样利用了二阶动量信息，而且还使用了一阶的动量信息，并且对得到的动量进行了偏差纠正操作。 问题：限制更新只依赖于少数历史梯度确实会引起显著的收敛性问题。所以，Adam的更新学习速率在神经网络这样的非凸问题上会变化很大，无法收敛。 Amsgrad通过将二阶动量限制为历史梯度的最大值，保证了学习速率是单调递减的，从而可以保证收敛]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本分类之特征工程]]></title>
    <url>%2F2018%2F05%2F03%2F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B9%8B%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[文本分类之特征工程内容参考自http://www.jeyzhang.com/text-classification-in-action.html 本文的话题老生常谈，文本分类应该是很多NLPer非常常遇到和熟悉的任务之一了，下面总结一下博主在处理这类任务的过程中特征工程方面的经验，希望对各位NLP入门者或者在做此类任务的新手有所帮助。对于其他的文本处理任务，也会有一定的参考意义。 概述文本分类，顾名思义，就是根据文本内容本身将文本归为不同的类别，通常是有监督学习的任务。根据文本内容的长短，有做句子、段落或者文章的分类；文本的长短不同可能会导致文本可抽取的特征上的略微差异，但是总体上来说，文本分类的核心都是如何从文本中抽取出能够体现文本特点的关键特征，抓取特征到类别之间的映射。所以，特征工程就显得非常重要，特征找的好，分类效果也会大幅提高（当然前提是标注数据质量和数量也要合适，数据的好坏决定效果的下限，特征工程决定效果的上限）。 也许会有人问最近的深度学习技术能够避免我们构造特征这件事，为什么还需要特征工程？深度学习并不是万能的，在NLP领域深度学习技术取得的效果有限（毕竟语言是高阶抽象的信息，深度学习在图像、语音这些低阶具体的信息处理上更适合，因为在低阶具体的信息上构造特征是一件费力的事情），并不是否认深度学习在NLP领域上取得的成绩，工业界现在通用的做法都是会把深度学习模型作为系统的一个子模块（也是一维特征），和一些传统的基于统计的自然语言技术的特征，还有一些针对具体任务本身专门设计的特征，一起作为一个或多个模型（也称Ensemble，即模型集成）的输入，最终构成一个文本处理系统。 特征工程对于文本分类而言，工业界有哪些常用的特征呢？下面用一张图用来概括。 我主要将这些特征分为四个层次，由下往上，特征由抽象到具体，粒度从细到粗。我们希望能够从不同的角度和纬度来设计特征，以捕捉这些特征和类别之间的关系。下面详细介绍这四个层次上常用到的特征表示。 基于词袋模型的特征表示词袋模型的基本思想是将文本符号化，将一段文本表示成一堆符号的集合；由于中文文本的多样性，通常导致构建的词袋维数较大，仅仅以词为单位（Unigram）构建的词袋可能就达到几万维，如果考虑二元词组（Bigram）、三元词组（Trigram）的话词袋大小可能会有几十万之多，因此基于词袋模型的特征表示通常是极其稀疏的。 词袋模型的one-hot表示示意图如下（假设我们构建了一个2w维的词袋模型，每一维表示一个词）： 从上到下，可以看出几种不同的表示方法： 第1种：Naive版本，不考虑词出现的频率，只要出现过就在相应的位置标1，否则为0； 第2种：考虑词频（即term frequency），认为一段文本中出现越多的词越重要，因此权重也越大； 第3种：考虑词的重要性，以TFIDF表征一个词的重要程度（不了解TFIDF的点这里）。简单来说，TFIDF反映了一种折中的思想：即在一篇文档中，TF认为一个词出现的次数越大可能越重要，但也可能并不是（比如停用词：“的”“是”之类的）；IDF认为一个词出现在的文档数越少越重要，但也可能不是（比如一些无意义的生僻词）。 通常情况下，我们都会采用第3种方法。原因也很直观，文本中所出现的词的重要程度是不太一样的，比如上面的例子中“我”，“喜欢”，“学习”这3个词就要比其他词更为重要。除了TFIDF的表征方法，还有chi-square，互信息（MI），熵等其他一些衡量词重要性的指标（见这里）。但是一般TFIDF用得比较普遍。经验总结： 通常考虑unigram和bigram来构建词袋模型（trigram的话维数太高，取得的gain也不高）； 用TFIDF时，注意对TF作归一化，通常用词频除以文本的长度； 如果构建的词袋维数太高，可以用TF（或者TFIDF）来卡，将一些不常见的词（会有很多噪音词，如联系方式、邮箱之类的）过滤掉； 如果有一些先验的词袋，word count通常都是比较强的一维特征（比如情感分类中，正负情感词的出现次数），可以考虑； 基于词袋模型构建的特征通常高维但稀疏，通常使用非线性模型取得的效果较线性的要好，推荐大家尝试使用一些基于决策树的boosting模型，如GBDT；这也很好理解，较线性模型而言，非线性模型能够学习出更加复杂的规则，对于文本而言，体现在能够一定程度上考虑词出现的语境（context）情况，比如，对于识别文本是否为骂人语料，文本中出现“妈”，同时也出现“你”，那么为骂人的概率会增大。 词袋模型比较简单直观，它通常能学习出一些关键词和类别之间的映射关系，但是缺点也很明显： 丢失了文本中词出现的先后顺序信息； 仅将词语符号化，没有考虑词之间的语义联系（比如，“麦克风”和“话筒”是不同的词，但是语义是相同的）； 基于embedding的特征表示上一部分介绍了基于词袋模型如何提取文本特征，这主要是从词形的角度考虑的，并没有考虑词语之间的语义关联信息。提到语义关联，大家都会联想到著名的word2vec。word2vec的原理很简单，基本思想是用词出现的上下文来表示这个词，上下文越接近的词之间的语义相似性越高（分布假说）。例如，上一小节中举到的例子，“话筒”和“麦克风”两者的上下文可能非常接近，因此会被认为是语义接近的。（不过语义接近并不代表含义接近，例如“黑色”和“白色”的上下文是相似的，但所代表的含义可能却是相反的）。 词袋模型比较简单直观，它通常能学习出一些关键词和类别之间的映射关系，但是缺点也很明显： 丢失了文本中词出现的先后顺序信息；仅将词语符号化，没有考虑词之间的语义联系（比如，“麦克风”和“话筒”是不同的词，但是语义是相同的）；基于embedding的特征表示上一部分介绍了基于词袋模型如何提取文本特征，这主要是从词形的角度考虑的，并没有考虑词语之间的语义关联信息。提到语义关联，大家都会联想到著名的word2vec。word2vec的原理很简单，基本思想是用词出现的上下文来表示这个词，上下文越接近的词之间的语义相似性越高。例如，上一小节中举到的例子，“话筒”和“麦克风”两者的上下文可能非常接近，因此会被认为是语义接近的。（不过语义接近并不代表含义接近，例如“黑色”和“白色”的上下文是相似的，但所代表的含义可能却是相反的）。 目前做word embedding的方法很多，比较流行的有下面两种： word2vec GloVe word2vec和GloVe两者的思想是类似的，都是用词的上下文来表示这个词，但是用的方法不同：word2vec是predict-based，用一个3层的NN模型来预测词的上下文（或者反过来），词向量是训练过程的中间产物；而GloVe则是count-based的方法，通过对共现词矩阵做降维来获取词的向量。两者在效果上相差不大，但GloVe模型的优势在于矩阵运算可以并行化，这样训练速度能加快。具体两者的差别可以参考Quora上的回答。 有了word embedding之后，我们怎么得到文本的embedding呢？ 对于短文本而言，比较好的方法有： (1) 取短文本的各个词向量之和（或者取平均）作为文本的向量表示； (2) 用一个pre-train好的NN model得到文本作为输入的最后一层向量表示； 除此之外，还有TwitterLda，TwitterLda是Lda的简化版本，针对短文本做主题刻画，实际效果也还不错。 基于embedding的特征刻画的是语义、主题层面上的特征，较词匹配而言，有一定的泛化能力。 基于NN模型抽取的特征NN的好处在于能end2end实现模型的训练和测试，利用模型的非线性和众多参数来学习特征，而不需要手工提取特征。CNN和RNN都是NLP中常用的模型，两个模型捕捉特征的角度也不太一样，CNN善于捕捉文本中关键的局部信息，而RNN则善于捕捉文本的上下文信息（考虑语序信息），并且有一定的记忆能力，两者都可以用在文本分类任务中，而且效果都不错。 对于简单的文本分类任务，用几个简单的NN模型基本就够了（调参数也是一大累活儿）。网上有很多关于NN的实现，这里推荐一个TensorFlow的实现版本，里面有一个浅层的CNN和RNN实现（word-based和chat-based都有），代码也很好懂，可以快速实验验证效果。地址在这里。 最后我们可以将这些NNs预测的分值作为我们分类系统的一个特征，来加强分类系统的性能。 基于任务本身抽取的特征这一部分的特征主要是针对具体任务而设计的，通过我们对数据的观察和感知，也许能够发现一些可能有用的特征。有时候，这些手工特征对最后的分类效果提升很大。举个例子，比如对于正负面评论分类任务，对于负面评论，包含负面词的数量就是一维很强的特征。 这部分的特征设计就是在拼脑力和拼经验，建议可以多看看各个类别数据找找感觉，将那些你直观上感觉对分类有帮助的东西设计成特征，有时候这些经验主义的东西很有用（可能是模型从数据学习不出来的）。 特征融合在设计完这些特征之后，怎么融合更合适呢？对于特征维数较高、数据模式复杂的情况，建议用非线性模型（如比较流行的GDBT, XGBoost）；对于特征维数较低、数据模式简单的情况，建议用简单的线性模型即可（如LR）。下面分享一个我做特征融合的模型框架，任务是正负面评论分类（负面评论定义是不适合出现在网络上的评论，如政治敏感、带有人身攻击、强烈负面情绪的评论）。 其中，橙色框表示模型，蓝色框表示用到的特征，[]里面表示特征的维数。这里需要注意的是，训练子模型（GBDT/DNN）的训练数据和训练融合模型（LR）的训练数据需要不一样，这也很好理解，就是防止子模型因为“见过”这些训练数据而产生偏向于子模型的情况。实际的模型训练中，可以用training数据集作为子模型的训练数据，dev数据集作为最终融合模型的训练数据。 模型融合能够从多个角度更加全面地学习出训练数据中的模式，往往能比单个模型效果好一点（2~3个点左右）。 另外，通过观察LR模型给各个特征分配的权重大小和正负，我们可以看出对于训练数据而言，这些特征影响分类的重要程度（权重大小（绝对值）），以及特征影响最终分类目标的极性。特别的，我们可以通过观察那些手工特征的权重来验证这些特征的有效性和有效程度。 总结这篇文章主要从宏观上介绍了对于文本分类任务而言设计特征的思路，对于其他的NLP任务，也可以参考类似的方法。总而言之，特征工程的核心是尽量从多个角度和纬度来捕捉数据中的模式，并用数值特征来加以刻画。最近流行的深度学习模型可以end2end地学习数据中隐含的模式，免去了人工提取特征的麻烦，然而对于信息高度抽象的文本数据而言，深度学习模型能取得的效果有限，在实际的产品中，我们往往会加入一些传统的基于统计学习的自然语言技术，以及根据我们对业务和数据的理解而人工设计的特征，来最终实现一个比较优良的结果。]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇博客]]></title>
    <url>%2F2018%2F05%2F02%2F%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[FlAG立个flag，以后每周至少更新两篇博客，论文或者所学的东西都可以 让自己学到的东西有个痕迹]]></content>
  </entry>
  <entry>
    <title><![CDATA[personal-introduction]]></title>
    <url>%2F2017%2F07%2F29%2Fpersonal-introduction%2F</url>
    <content type="text"><![CDATA[高建伟 Jianwei Gao浙江大学, 计算机学院, 硕士研究生 研究方向：自然语言处理与深度学习 CurrentlyStanding on the shoulders of giants Specialized inLaws of motion, gravitation, minting coins, disliking Robert Hooke Research interestsCooling, power series, optics, alchemy, planetary motions, apples. Education1654-1660The King’s School, Grantham. June 1661 - nowTrinity College, Cambridge Sizar 1667 - deathTrinity College, Cambridge Fellow Awards2012President, Royal Society, London, UK Associate, French Academy of Science, Paris, France Publications Journals1669Newton Sir I, De analysi per æquationes numero terminorum infinitas. 1669Lectiones opticæ. etc. etc. etc. Patents2012Infinitesimal calculus for solutions to physics problems, SMBC patent 001 Occupation1600Royal Mint, London Warden Minted coins 1600Lucasian professor of Mathematics, Cambridge University]]></content>
      <tags>
        <tag>personal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[categories]]></title>
    <url>%2Fcategories%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[tags]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
